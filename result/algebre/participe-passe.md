628

**207**: Le cours est ***découpé*** en quatre parties : espaces vectoriels sur R ou C matrices et systèmes linéaires déterminant réduction des endomorphismes

**239**: Les calculs et les dessins ont été , pour la plupart , ***effectués*** grâce aux logiciels Wxmaxima et Python , sympy , matplotlib , outils d' une très grande qualité , gratuits et fonctionnant sur tout système ( Linux , Windows , Mac , Androïd )

**295**: Signalons aussi l' outil de géométrie plane Geogebra et l' excellent Ipe qui permet d' annoter en LATEX 2 les dessins ***produits*** directement ou à l' aide d' un autre outil

**496**: Liste des figures Ensembles usuels Notations usuelles Opérations sur les ensembles Signification Ensemble des entiers naturels Ensemble des entiers relatifs Ensemble des entiers naturels ou relatifs non nuls Ensemble des entiers relatifs compris entre p et q Ensemble des rationnels Ensemble des réels Ensemble des rationnels ou des réels strictement positifs Ensemble des complexes Ensemble des rationnels ou des réels ou des complexes non nuls Corps commutatif quelconque ( désigne souvent R ou C , en ce cas , c' est ***signalé*** en début de chapitre ) Ensemble des n - uplets ( si n 2 , on parle de couple et si n 3 , on parle de triplet ) un n - uplet sera parfois noté x au lieu de ( x1 ,

**532**: Liste des figures Ensembles usuels Notations usuelles Opérations sur les ensembles Signification Ensemble des entiers naturels Ensemble des entiers relatifs Ensemble des entiers naturels ou relatifs non nuls Ensemble des entiers relatifs compris entre p et q Ensemble des rationnels Ensemble des réels Ensemble des rationnels ou des réels strictement positifs Ensemble des complexes Ensemble des rationnels ou des réels ou des complexes non nuls Corps commutatif quelconque ( désigne souvent R ou C , en ce cas , c' est signalé en début de chapitre ) Ensemble des n - uplets ( si n 2 , on parle de couple et si n 3 , on parle de triplet ) un n - uplet sera parfois ***noté*** x au lieu de ( x1 ,

**625**: , En Produit cartésien de la famille d' ensembles ( Ei ) iI , où I est un ensemble quelQ conque les éléments de iI Ei sont ***notés*** ( xi ) iI où , pour tout i I , xi Ei Ensemble des Q familles d' éléments de l' ensemble E indexées par l' ensemble F ( correspond à iF Ei où tous les Ei sont égaux à E ) Classe d' équivalence de x pour la relation R Classe(x , R ) Quantificateurs Quantificateurs ( usage ) Quel que soit

**649**: , En Produit cartésien de la famille d' ensembles ( Ei ) iI , où I est un ensemble quelQ conque les éléments de iI Ei sont notés ( xi ) iI où , pour tout i I , xi Ei Ensemble des Q familles d' éléments de l' ensemble E ***indexées*** par l' ensemble F ( correspond à iF Ei où tous les Ei sont égaux à E ) Classe d' équivalence de x pour la relation R Classe(x , R ) Quantificateurs Quantificateurs ( usage ) Quel que soit

**738**: Signifie x E , y E L' écriture x , y E ne est pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions ***Introduit*** une nouvelle notation Introduit une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions définies sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues définies sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux définies sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , définies sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle engendrée par x Sous-espace vectoriel de l' espace vectoriel E engendré par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E est un K - espace vectoriel c' est l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients sont les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( correspond à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y a un 1 ( Attention : les dimensions de ces matrices ne sont pas précisées ... ) Transposée de A , où A Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**742**: Signifie x E , y E L' écriture x , y E ne est pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions Introduit une nouvelle notation ***Introduit*** une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions définies sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues définies sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux définies sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , définies sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle engendrée par x Sous-espace vectoriel de l' espace vectoriel E engendré par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E est un K - espace vectoriel c' est l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients sont les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( correspond à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y a un 1 ( Attention : les dimensions de ces matrices ne sont pas précisées ... ) Transposée de A , où A Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**760**: Signifie x E , y E L' écriture x , y E ne est pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions Introduit une nouvelle notation Introduit une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions ***définies*** sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues définies sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux définies sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , définies sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle engendrée par x Sous-espace vectoriel de l' espace vectoriel E engendré par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E est un K - espace vectoriel c' est l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients sont les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( correspond à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y a un 1 ( Attention : les dimensions de ces matrices ne sont pas précisées ... ) Transposée de A , où A Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**775**: Signifie x E , y E L' écriture x , y E ne est pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions Introduit une nouvelle notation Introduit une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions définies sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues ***définies*** sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux définies sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , définies sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle engendrée par x Sous-espace vectoriel de l' espace vectoriel E engendré par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E est un K - espace vectoriel c' est l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients sont les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( correspond à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y a un 1 ( Attention : les dimensions de ces matrices ne sont pas précisées ... ) Transposée de A , où A Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**790**: Signifie x E , y E L' écriture x , y E ne est pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions Introduit une nouvelle notation Introduit une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions définies sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues définies sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux ***définies*** sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , définies sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle engendrée par x Sous-espace vectoriel de l' espace vectoriel E engendré par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E est un K - espace vectoriel c' est l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients sont les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( correspond à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y a un 1 ( Attention : les dimensions de ces matrices ne sont pas précisées ... ) Transposée de A , où A Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**814**: Signifie x E , y E L' écriture x , y E ne est pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions Introduit une nouvelle notation Introduit une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions définies sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues définies sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux définies sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , ***définies*** sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle engendrée par x Sous-espace vectoriel de l' espace vectoriel E engendré par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E est un K - espace vectoriel c' est l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients sont les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( correspond à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y a un 1 ( Attention : les dimensions de ces matrices ne sont pas précisées ... ) Transposée de A , où A Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**829**: Signifie x E , y E L' écriture x , y E ne est pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions Introduit une nouvelle notation Introduit une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions définies sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues définies sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux définies sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , définies sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle ***engendrée*** par x Sous-espace vectoriel de l' espace vectoriel E engendré par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E est un K - espace vectoriel c' est l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients sont les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( correspond à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y a un 1 ( Attention : les dimensions de ces matrices ne sont pas précisées ... ) Transposée de A , où A Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**839**: Signifie x E , y E L' écriture x , y E ne est pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions Introduit une nouvelle notation Introduit une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions définies sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues définies sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux définies sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , définies sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle engendrée par x Sous-espace vectoriel de l' espace vectoriel E ***engendré*** par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E est un K - espace vectoriel c' est l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients sont les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( correspond à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y a un 1 ( Attention : les dimensions de ces matrices ne sont pas précisées ... ) Transposée de A , où A Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**1288**: Signifie x E , y E L' écriture x , y E ne est pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions Introduit une nouvelle notation Introduit une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions définies sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues définies sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux définies sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , définies sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle engendrée par x Sous-espace vectoriel de l' espace vectoriel E engendré par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E est un K - espace vectoriel c' est l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients sont les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( correspond à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y a un 1 ( Attention : les dimensions de ces matrices ne sont pas ***précisées*** ... ) Transposée de A , où A Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**1291**: Signifie x E , y E L' écriture x , y E ne est pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions Introduit une nouvelle notation Introduit une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions définies sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues définies sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux définies sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , définies sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle engendrée par x Sous-espace vectoriel de l' espace vectoriel E engendré par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E est un K - espace vectoriel c' est l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients sont les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( correspond à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y a un 1 ( Attention : les dimensions de ces matrices ne sont pas précisées ... ) ***Transposée*** de A , où A Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**1658**: , xn ) dans la base E Déterminant d' un endomorphisme u L ( E ) Cofacteur d' indices ( i , j ) de la matrice A Comatrice de A Déterminant de Vandermonde Ensemble des formes p - linéaires de E dans K Ensemble des formes p - linéaires symétriques de E dans K Ensemble des formes p - linéaires antisymétriques de E dans K Réduction des endomorphismes Sp(u ) ou Sp(A ) multu ( ) ou multA ( ) Espace propre de u L ( E ) ou A pour la valeur propre K Spectre de u L ( E ) ou A Polynôme caractéristique de u L ( E ) ou A Multiplicité de dans u ou A Algèbre ***engendrée*** par u ou A Signification Polynôme d' endomorphisme ou de matrice Idéal annulateur de u ou A Polynôme minimal de u ou A Matrice compagnon du polynôme P Norme subordonnée de u ou A Espace des endomorphismes continus de E Rayon spectral de u ou A Commutant de u ou A Espace caractéristique de u ou A Chapitre 1 Espaces vectoriels sur R ou C Dans ce chapitre , nous noterons K les corps R ou C. Cela signifie alors que le résultat énoncé est vrai dans R et dans C. Généralités Premières définitions Soit E un ensemble non vide , on dit que E est un K - espace vectoriel ( ou un espace vectoriel sur K ) si il vérifie les axiomes suivants : 1

**1789**: Il est ***muni*** d' une opération interne a , notée et appelée addition qui vérifie : ( a ) est associative : ( b ) possède un élément neutre noté 0E ( ou 0 lorsqu' il ne y a pas d' ambiguïté ) : ( c ) Tout élément de E possède un unique opposé : de plus , on note l' opération : ( d ) est commutative : 2

**1798**: Il est muni d' une opération interne a , notée et ***appelée*** addition qui vérifie : ( a ) est associative : ( b ) possède un élément neutre noté 0E ( ou 0 lorsqu' il ne y a pas d' ambiguïté ) : ( c ) Tout élément de E possède un unique opposé : de plus , on note l' opération : ( d ) est commutative : 2

**1816**: Il est muni d' une opération interne a , notée et appelée addition qui vérifie : ( a ) est associative : ( b ) possède un élément neutre ***noté*** 0E ( ou 0 lorsqu' il ne y a pas d' ambiguïté ) : ( c ) Tout élément de E possède un unique opposé : de plus , on note l' opération : ( d ) est commutative : 2

**1861**: Il est ***muni*** d' une opération externe b , notée

**1868**: Il est muni d' une opération externe b , ***notée***

**1871**: et ***appelée*** multiplication par un scalaire qui vérifie : ( a )

**1930**: est distributive par rapport à la multiplication de K : ( d ) L' unité 1 du corps est ***respectée*** : Les éléments de E s' appellent alors vecteurs et les éléments de K scalaires

**2223**: Combinaison linéaire d' un nombre ***fini*** de vecteurs Soit ( x1 ,

**2293**: , xn ) n vecteurs de E , on appelle combinaison linéaire de ces vecteurs toute expression de la Combinaison linéaire d' un nombre quelconque de vecteurs Plus généralement , si on a un nombre quelconque de vecteurs de E , ( xi ) iI , on appelle combinaison linéaire de ces vecteurs , toute combinaison linéaire d' une sous-famille ***finie*** ( xi1 ,

**2318**: Dans C ***considéré*** comme un R - espace vectoriel , tout nombre complexe est combinaison linéaire de 1 et i , mais aussi de 1 et exp(2 i 3 ) j , ou de i et j , ou de 1 , i et j , etc. 3

**2415**: Produit ***fini*** d' espaces vectoriels Soit E1 , ... , En des K - espaces vectoriels , l' ensemble E1 En est alors muni d' une structure de K - espace vectoriel définie par ( avec des notations évidentes ) : Produit quelconque d' espaces vectoriels Si E est un K - espace vectoriel et I un ensemble ( d' indices ) , alors , de même , E I est muni d' une structure de K - espace vectoriel définie par ( avec des notations évidentes ) : 1

**2437**: Produit fini d' espaces vectoriels Soit E1 , ... , En des K - espaces vectoriels , l' ensemble E1 En est alors ***muni*** d' une structure de K - espace vectoriel définie par ( avec des notations évidentes ) : Produit quelconque d' espaces vectoriels Si E est un K - espace vectoriel et I un ensemble ( d' indices ) , alors , de même , E I est muni d' une structure de K - espace vectoriel définie par ( avec des notations évidentes ) : 1

**2446**: Produit fini d' espaces vectoriels Soit E1 , ... , En des K - espaces vectoriels , l' ensemble E1 En est alors muni d' une structure de K - espace vectoriel ***définie*** par ( avec des notations évidentes ) : Produit quelconque d' espaces vectoriels Si E est un K - espace vectoriel et I un ensemble ( d' indices ) , alors , de même , E I est muni d' une structure de K - espace vectoriel définie par ( avec des notations évidentes ) : 1

**2485**: Produit fini d' espaces vectoriels Soit E1 , ... , En des K - espaces vectoriels , l' ensemble E1 En est alors muni d' une structure de K - espace vectoriel définie par ( avec des notations évidentes ) : Produit quelconque d' espaces vectoriels Si E est un K - espace vectoriel et I un ensemble ( d' indices ) , alors , de même , E I est ***muni*** d' une structure de K - espace vectoriel définie par ( avec des notations évidentes ) : 1

**2494**: Produit fini d' espaces vectoriels Soit E1 , ... , En des K - espaces vectoriels , l' ensemble E1 En est alors muni d' une structure de K - espace vectoriel définie par ( avec des notations évidentes ) : Produit quelconque d' espaces vectoriels Si E est un K - espace vectoriel et I un ensemble ( d' indices ) , alors , de même , E I est muni d' une structure de K - espace vectoriel ***définie*** par ( avec des notations évidentes ) : 1

**2535**: KN ( l' ensemble des suites de K ***indexées*** par N ) est un K - espace vectoriel

**2561**: 1.1.1 Démontrer que , dans tout K - espace vectoriel , les relations suivantes sont ***vérifiées*** : 1.1.2 Parmi les ensembles suivants , lesquels , munis des opérations usuelles , sont des R - espaces vectoriels ? Sous-espaces vectoriels Soit E un K - espace vectoriel et F E , on dit que F est un sous-espace vectoriel de E si : F est stable par ( c' est - à - dire que pour tout ( x , y ) F 2 , x y F ) F est stable par

**2682**: La droite vectorielle ***dirigée*** par x , notée K.x , est le sous-ensemble de E défini par Les droites vectorielles sont des sous-espaces vectoriels de E. 2

**2686**: La droite vectorielle dirigée par x , ***notée*** K.x , est le sous-ensemble de E défini par Les droites vectorielles sont des sous-espaces vectoriels de E. 2

**2694**: La droite vectorielle dirigée par x , notée K.x , est le sous-ensemble de E ***défini*** par Les droites vectorielles sont des sous-espaces vectoriels de E. 2

**2857**: Alors F est un sous-espace vectoriel de E si , et seulement si , F ***muni*** des mêmes opérations et

**2922**: Démonstration Supposons que F soit sous-espace vectoriel de E. Alors est une opération interne sur F F ( car F est stable par ) , qui est associative ( car elle l' est sur E F ) , possède un élément neutre 0E F , tout élément de F possède un unique ***opposé*** dans F ( si x F , x ( 1).x F car F est stable par

**2998**: ) , est distributive par rapport à l' addition de F ainsi que l' addition et la multiplication de K et l' unité du corps est ***respectée*** ( car ces propriétés sont vraies sur E F )

**3013**: Conclusion : F ***muni*** des opérations et

**3029**: Réciproquement , si F ***muni*** des opérations et

**3097**: Pour démontrer qu' un ensemble est un K - espace vectoriel , il est très souvent plus simple et plus rapide de démontrer que c' est un sous-espace vectoriel d' un K - espace vectoriel déjà ***connu***

**3533**: Soit E un K - espace vectoriel et soit A E , on appelle sous-espace vectoriel ***engendré*** par A le plus petit sous-espace vectoriel de E contenant A. On le note : Démonstration que la notion de sous-espace vectoriel engendré par une partie est bien définie L' existence d' un plus petit sous-espace vectoriel contenant A mérite une justification

**3556**: Soit E un K - espace vectoriel et soit A E , on appelle sous-espace vectoriel engendré par A le plus petit sous-espace vectoriel de E contenant A. On le note : Démonstration que la notion de sous-espace vectoriel ***engendré*** par une partie est bien définie L' existence d' un plus petit sous-espace vectoriel contenant A mérite une justification

**3562**: Soit E un K - espace vectoriel et soit A E , on appelle sous-espace vectoriel engendré par A le plus petit sous-espace vectoriel de E contenant A. On le note : Démonstration que la notion de sous-espace vectoriel engendré par une partie est bien ***définie*** L' existence d' un plus petit sous-espace vectoriel contenant A mérite une justification

**3690**: Si E 6 0E et si x E , x 6 0E , alors Vect(x ) est la droite vectorielle ***engendrée*** par x : Proposition 1.1 Si E est un K - espace vectoriel et si A E , A 6 , alors Vect(A ) est l' ensemble des combinaisons linéaires construites à partir des vecteurs de A : Vect(A ) A Vect(A ) A Démonstration ( ) L' ensemble des combinaisons linéaires de vecteurs de A est un sous-espace vectoriel de E qui contient A , il contient donc Vect(A )

**3721**: Si E 6 0E et si x E , x 6 0E , alors Vect(x ) est la droite vectorielle engendrée par x : Proposition 1.1 Si E est un K - espace vectoriel et si A E , A 6 , alors Vect(A ) est l' ensemble des combinaisons linéaires ***construites*** à partir des vecteurs de A : Vect(A ) A Vect(A ) A Démonstration ( ) L' ensemble des combinaisons linéaires de vecteurs de A est un sous-espace vectoriel de E qui contient A , il contient donc Vect(A )

**3767**: ( ) A est ***inclus*** dans Vect(A ) ( par définition ) qui est stable par combinaisons linéaires , donc Vect(A ) contient toutes les combinaisons linéaires de vecteurs de A. Soit E un K - espace vectoriel

**3925**: C' est la même différence qu' entre une liste et un ensemble en informatique ! a Voir le code 1.1 , de la présente a. L' ensemble ai , i I ***associé*** à la famille ( ai ) iI s' appelle le support de la famille

**3974**: ( 1 , i ) , ( 1 , j ) , ( i , j ) , ( 1 , i , j ) sont des familles génératrices de C ***vu*** comme un R - espace vectoriel

**4354**: Somme ***finie*** Soit E1 et E2 deux sous-espaces vectoriels de E , on appelle somme de E1 et E2 et on note E1 E2 le sous-espace vectoriel : Somme quelconque Plus généralement , si ( Ei ) iI est une famille de sous-espaces vectoriels de E , on appelle somme des Ei et Ei le sous-espace vectoriel : Il est faux de penser : 1

**4564**: En reprenant les notations de la définition 1.7 , page ci - contre , on a où xik iI Ei Eik pour tout k 1 , n. Le vecteur x s' ***écrit*** donc comme une combinaison linéaire d' éléments de iI Ei , donc x Vect ( ) Soit x Vect iI Ei

**4773**: Soit E1 et E2 deux sous-espaces vectoriels de E , on dit que E1 et E2 sont en somme directe si tout élément de E1 E2 s' ***écrit*** , de manière unique sous la forme x1 x2 , où x1 E1 et x2 E2

**4911**: On en ***déduit*** que E1 et E2 sont en somme directe

**5024**: On en ***déduit*** que E1 et E2 sont en somme Soit E un K - espace vectoriel , soit ( Ei ) iI une famille de sous-espaces vectoriels de E , on dit qu' ils sont en somme directe , si on a écriture unique de 0E , soit : distincts 2 à 2 On écrit alors Ei à la place de Ne pas oublier que l' on ne sait faire que des sommes finies de vecteurs ! ! Si I contient strictement plus de deux éléments , la condition iI Ei 0E ne suffit pas pour assurer que les Ei sont en somme directe , de même que les conditions Ei Ej 0E pour tout Pour s' en convaincre , on peut considérer les sous-espaces vectoriels E1 R.(1 , 0 ) , E2 R.(0 , 1 ) et 1

**5451**: Dans C ( ***vu*** comme un R - espace vectoriel ) , deux droites vectorielles engendrées par des complexes z1 et z2 non nuls tels que sont supplémentaires

**5463**: Dans C ( vu comme un R - espace vectoriel ) , deux droites vectorielles ***engendrées*** par des complexes z1 et z2 non nuls tels que sont supplémentaires

**5538**: Dans E Cpm ( 0 , 1 , R ) , la droite ***engendrée*** par la fonction x 7 1 est supplémentaire de Soit E un K - espace vectoriel

**5773**: Famille libre ( ***finie*** ) Soit ( x1 ,

**5820**: Famille ***liée*** ( finie ) Une famille qui ne est pas libre est dite famille liée et les vecteurs de cette famille sont dits linéairement dépendants

**5822**: Famille liée ( ***finie*** ) Une famille qui ne est pas libre est dite famille liée et les vecteurs de cette famille sont dits linéairement dépendants

**5832**: Famille liée ( finie ) Une famille qui ne est pas libre est ***dite*** famille liée et les vecteurs de cette famille sont dits linéairement dépendants

**5834**: Famille liée ( finie ) Une famille qui ne est pas libre est dite famille ***liée*** et les vecteurs de cette famille sont dits linéairement dépendants

**5858**: Famille libre Plus généralement , une famille ( xi ) iI est ***dite*** libre ( vecteurs indépendants ) , si ( écriture unique de 0E ) : distincts 2 à 2 Autrement dit , ( xi ) iI est libre si toute sous-famille finie est libre

**5878**: Famille libre Plus généralement , une famille ( xi ) iI est dite libre ( vecteurs indépendants ) , si ( écriture unique de 0E ) : distincts 2 à 2 Autrement ***dit*** , ( xi ) iI est libre si toute sous-famille finie est libre

**5889**: Famille libre Plus généralement , une famille ( xi ) iI est dite libre ( vecteurs indépendants ) , si ( écriture unique de 0E ) : distincts 2 à 2 Autrement dit , ( xi ) iI est libre si toute sous-famille ***finie*** est libre

**5929**: la famille ( xi ) iI est ***liée*** 2

**6084**: Dans C en tant que R - espace vectoriel les parties 1 , i , 1 , j et i , j sont libres , alors que la partie 1 , i , j est ***liée***

**6112**: Dans C en tant que C - espace vectoriel , les parties 1 , i , 1 , j et i , j sont ***liées***

**6173**: Alors .xik Vect(ei , i I ) ce qui ***contredit*** x 6 Vect(ei , i I )

**6213**: On en ***déduit*** que x ei , i I est une partie libre de E. Soit X ( xi ) iI une famille de vecteurs d' un K - espace vectoriel E. Alors : 1

**6260**: si il existe i0 I tel que xi0 0E , alors X est ***liée*** 2

**6285**: si il existe ( i , j ) I 2 tel que i 6 j et xi xj , alors X est ***liée*** 3

**6299**: si une sous-famille ( xi ) iJ où J I est ***liée*** , alors X est liée 4

**6304**: si une sous-famille ( xi ) iJ où J I est liée , alors X est ***liée*** 4

**6339**: Il existe une écriture de 0E non trivial , car 0E 1.xi0 , donc X est ***liée***

**6364**: Il existe une écriture de 0 K non trivial , car 0E xi xj 1.xi ( 1).xj , donc X est ***liée***

**6438**: Soit E un K - espace vectoriel , une famille ( xi ) iI est ***appelée*** base de E , si elle est à la fois famille libre et famille génératrice

**6490**: Si B ( ei ) iI est une base de E , alors a : distincts 2 à 2 Cela signifie que tout vecteur de E s' ***écrit*** de manière unique comme une combinaison linéaire d' éléments a. Notons que dans cette écriture , lorsque x 0E , on a n 0

**6521**: Démonstration Immédiat : x s' ***écrit*** comme une combinaison linéaire d' éléments de B car B est génératrice et cette combinaison linéaire est unique car B est libre

**6551**: La famille ( i ) iI ***définie*** par : est appelée coordonnées du vecteur x dans la base B. L' ensemble vide est une base de 0E

**6555**: La famille ( i ) iI définie par : est ***appelée*** coordonnées du vecteur x dans la base B. L' ensemble vide est une base de 0E

**6556**: La famille ( i ) iI définie par : est appelée ***coordonnées*** du vecteur x dans la base B. L' ensemble vide est une base de 0E

**6855**: 1.6.4 On considère dans R4 les cinq vecteurs suivants : Donner une équation du sous-espace vectoriel ***engendré*** par ces cinq vecteurs

**6877**: 1.6.5 Soit E F ( R , R ) et F le sous-espace vectoriel de E ***engendré*** par les fonctions : Déterminer une base de F

**7048**: Dimension ***finie*** Soit E un K - espace vectoriel

**7072**: On dit que E est de dimension finie si il possède une base de cardinal ***fini***

**7081**: Dans le cas contraire , il est ***dit*** de dimension infinie

**7123**: Si E est un C - espace vectoriel de dimension finie , alors E est un R - espace vectoriel de dimension ***finie***

**7291**: Supposons qu' on ait ***formé*** une famille génératrice de la forme donc on peut écrire liées )

**7302**: Supposons qu' on ait formé une famille génératrice de la forme donc on peut écrire ***liées*** )

**7382**: , n1 ) serait ***liée*** , ce qui contredit le fait que ( 1 ,

**7386**: , n1 ) serait liée , ce qui ***contredit*** le fait que ( 1 ,

**7466**: Ce cardinal commun est ***appelé*** dimension de E et est notée a : dimK E ou dim E si il ne y a pas ambiguïté sur le corps K a. On utiliser aussi parfois la notation dim(E ) ou dimK ( E )

**7472**: Ce cardinal commun est appelé dimension de E et est ***notée*** a : dimK E ou dim E si il ne y a pas ambiguïté sur le corps K a. On utiliser aussi parfois la notation dim(E ) ou dimK ( E )

**7577**: Cette définition est cohérente avec celle de droite vectorielle ***engendrée*** : si E est un K - espace vectoriel différent de 0E et si x E , x 6 0E , alors x est une base de K.x , la droite vectorielle engendrée par x , qui est bien de dimension 1

**7610**: Cette définition est cohérente avec celle de droite vectorielle engendrée : si E est un K - espace vectoriel différent de 0E et si x E , x 6 0E , alors x est une base de K.x , la droite vectorielle ***engendrée*** par x , qui est bien de dimension 1

**7690**: Quelques espaces de dimensions infinies : le R - espace vectoriel des fonctions polynomiales , mais aussi : Théorème 1.3 Base incomplète Soit E un K - espace vectoriel de dimension ***finie*** n , soit ( 1 ,

**7760**: Principe de construction de bases ( en dimension ***finie*** )

**7788**: Supposons ***construits*** les p premiers vecteurs de la base ( au départ p 0 ) , notés ( e1 ,

**7803**: Supposons construits les p premiers vecteurs de la base ( au départ p 0 ) , ***notés*** ( e1 ,

**7836**: , ep ) , cela ***fait*** un nouveau vecteur

**7898**: Les calculs sont ***explicités*** dans la session Wxmaxima 1.2 , de la présente page

**7986**: Pour cela , il suffit d' imposer z t 0 et ( x , y ) 6 ( 0 , 0 ) , car Par exemple , on peut prendre : Remarquons que nous avons utilisé une partie génératrice bien ***connue*** : la base canonique de R4

**8003**: Soit E un K - espace vectoriel de dimension ***finie***

**8023**: toute famille de plus de n 1 éléments est ***liée*** 2

**8099**: Si une famille de plus de n 1 éléments est libre , on pourrait la compléter en une base qui contiendrait au moins n 1 éléments , ce qui ***contredit*** dim E n. 2

**8254**: Soit E un K - espace vectoriel de dimension ***finie***

**8267**: Alors de toute partie génératrice on peut extraire une base ( ***finie*** )

**8309**: Il existe une base B de E qui est ***finie***

**8317**: Tous les vecteurs ( en nombre ***fini*** ) de B peuvent s' écrire comme une combinaison linéaire d' éléments de G donc E est en fait engendré par une sous-famille finie G 0 de G. Comme E 6 0E , G 0 contient un vecteur x 6 0E

**8337**: Tous les vecteurs ( en nombre fini ) de B peuvent s' écrire comme une combinaison linéaire d' éléments de G donc E est en fait ***engendré*** par une sous-famille finie G 0 de G. Comme E 6 0E , G 0 contient un vecteur x 6 0E

**8341**: Tous les vecteurs ( en nombre fini ) de B peuvent s' écrire comme une combinaison linéaire d' éléments de G donc E est en fait engendré par une sous-famille ***finie*** G 0 de G. Comme E 6 0E , G 0 contient un vecteur x 6 0E

**8411**: Alors E est de dimension finie si , et seulement si , il possède une partie génératrice ***finie*** ( c' est d' ailleurs une définition fréquente des espaces de dimension finie )

**8424**: Alors E est de dimension finie si , et seulement si , il possède une partie génératrice finie ( c' est d' ailleurs une définition fréquente des espaces de dimension ***finie*** )

**8439**: Démonstration C' est une conséquence immédiate de la définition de la dimension ***finie*** et de la proposition précédente

**8507**: Alors E est de dimension infinie si , et seulement si , il existe des parties libres de cardinal quelconque n N. Démonstration Cela revient à démontrer que E est de dimension finie si , et seulement si , il existe n N tel que toute famille de E à n éléments soit ***liée***

**8542**: Si E est de dimension finie , alors tout sous-espace vectoriel F de E est de dimension ***finie*** et , de plus : dim F dim E E F dim E dim F Démonstration Si F 0E , il ne y a rien à démontrer

**8640**: Soit L une famille libre d' éléments de F de cardinal p. Pour tout x F , la famille ( L , x ) est ***liée*** ( car sinon cela contredit la définition de p ) , donc x est combinaison linéaire d' éléments de L , donc L est une famille génératrice de F , c' est donc une base de F

**8645**: Soit L une famille libre d' éléments de F de cardinal p. Pour tout x F , la famille ( L , x ) est liée ( car sinon cela ***contredit*** la définition de p ) , donc x est combinaison linéaire d' éléments de L , donc L est une famille génératrice de F , c' est donc une base de F

**8842**: Par exemple , une base de F R.(1 , 1 ) ne est jamais une sous-famille de la base canonique Soit E un K - espace vectoriel de dimension ***finie***

**8845**: Base ***adaptée*** à une somme directe de deux sous-espaces vectoriels On suppose que E F G , on peut alors construire une base ( e1 ,

**8884**: , en ) de E telle que Une telle base est ***dite*** base adaptée à la somme directe

**8893**: Base ***adaptée*** à une somme directe finie De même , lorsque l' on a une décomposition en somme directe de E , on peut toujours construire une base adaptée à cette somme directe

**8898**: Base adaptée à une somme directe ***finie*** De même , lorsque l' on a une décomposition en somme directe de E , on peut toujours construire une base adaptée à cette somme directe

**8920**: Base adaptée à une somme directe finie De même , lorsque l' on a une décomposition en somme directe de E , on peut toujours construire une base ***adaptée*** à cette somme directe

**8973**: Si E est un K - espace vectoriel de dimension finie et si E F G , alors : dim E dim F dim G Plus généralement , si E admet une décomposition en somme directe a : Ek dim E a. Il y alors un nombre ***fini*** de termes dans la décomposition en somme directe , si on enlève les espaces réduits à 0E

**8988**: Si E est un K - espace vectoriel de dimension finie et si E F G , alors : dim E dim F dim G Plus généralement , si E admet une décomposition en somme directe a : Ek dim E a. Il y alors un nombre fini de termes dans la décomposition en somme directe , si on enlève les espaces ***réduits*** à 0E

**9032**: Si E1 et E2 sont des K - espaces vectoriels de dimensions finies , alors E1 E2 est de dimension ***finie*** et : Plus généralement , si E1 ,

**9062**: , Ep sont des K - espaces vectoriels de dimensions finies , alors E1 Ep est de dimension ***finie*** et : Démonstration Si E1 0E ou E2 0E , c' est immédiat

**9233**: Soit E un K - espace vectoriel de dimension ***finie***

**9305**: , en ) est une base de E , tout élément x E s' ***écrit*** de manière unique sous la forme 2

**9448**: Soit G un sous-espace vectoriel de E tel que : Démontrer que F2 G. 1.7.4 Soit E un K - espace vectoriel de dimension ***finie*** , soit F et G deux sous-espaces vectoriels de E tels que F G 0E

**9552**: , Vk des sous-espaces vectoriels de V tels que Démontrer que 1.7.7 Soit E un K - espace vectoriel de dimension ***finie*** n , soit F1 et F2 deux sous-espaces vectoriels de dimension p n. ( a ) Démontrer que l' on peut alors trouver un supplémentaire commun à F1 et F2

**9596**: ( b ) Généraliser lorsque le corps est R à un nombre ***fini*** de sous-espaces vectoriels de dimension p. ( c ) Puis , à une infinité dénombrable de tels sous-espaces vectoriels

**9650**: Applications linéaires Généralités Soit E et E 0 deux K - espaces vectoriels , une application f : E E 0 est ***dite*** application linéaire si elle est compatible avec les structures d' espaces vectoriels , c' est - à - dire a : L' ensemble des applications linéaires de E dans E 0 se note : LK ( E , E 0 ) ou L ( E , E 0 ) lorsqu' il ne y a pas ambiguïté sur le corps K 1

**9834**: Si E est un K - espace vectoriel , alors l' application identité , ***notée*** idE et définie par idE ( x ) x pour tout x E est une application linéaire de E dans lui-même ( c' est donc un endomorphisme de E )

**9837**: Si E est un K - espace vectoriel , alors l' application identité , notée idE et ***définie*** par idE ( x ) x pour tout x E est une application linéaire de E dans lui-même ( c' est donc un endomorphisme de E )

**9877**: Plus généralement , si K , l' homothétie de rapport , ***définie*** par x 7 .x , est un endomorphisme de E. En particulier , l' application nulle x 7 0E est un endomorphisme de E. 2

**9969**: L' application ***définie*** pour a b par : est un isomorphisme

**9983**: L' application ***définie*** par : est une forme linéaire Si E et E 0 sont deux K - espaces vectoriels , alors : L ( E , E 0 ) est un K - espace vectoriel Démonstration Montrons que c' est un sous-espace vectoriel de F ( E , E 0 )

**10252**: On en ***déduit*** que f 1 ( F 0 ) est stable par combinaison linéaire

**10517**: Images et noyaux Soit E et E 0 deux K - espaces vectoriels , et f L ( E , E 0 ) , alors : L' image de E par f est un sous-espace vectoriel de E 0 ***noté*** : L' image réciproque de 0E 0 par f est un sous-espace vectoriel de E appelé noyau de f et noté 1

**10533**: Images et noyaux Soit E et E 0 deux K - espaces vectoriels , et f L ( E , E 0 ) , alors : L' image de E par f est un sous-espace vectoriel de E 0 noté : L' image réciproque de 0E 0 par f est un sous-espace vectoriel de E ***appelé*** noyau de f et noté 1

**10538**: Images et noyaux Soit E et E 0 deux K - espaces vectoriels , et f L ( E , E 0 ) , alors : L' image de E par f est un sous-espace vectoriel de E 0 noté : L' image réciproque de 0E 0 par f est un sous-espace vectoriel de E appelé noyau de f et ***noté*** 1

**10946**: , ip ) une sous-famille ***finie*** quelconque de I. Soit ( 1 ,

**11072**: Puisque X est génératrice , il On a alors par linéarité de f ce qui démontre que y s' ***écrit*** comme une combinaison linéaire finie d' éléments de ( f ( xi ) ) iI

**11077**: Puisque X est génératrice , il On a alors par linéarité de f ce qui démontre que y s' écrit comme une combinaison linéaire ***finie*** d' éléments de ( f ( xi ) ) iI

**11540**: 1.9.2 Soit f un endomorphisme d' un K - espace vectoriel E , démontrer que : 1.9.3 Soit f et g deux endomorphismes d' un K - espace vectoriel E , démontrer que : Projecteurs et symétries Soit E un K - espace vectoriel , F et G deux sous-espaces vectoriels de E tels que E F G. On appelle projection de E sur F parallèlement à G l' endomorphisme de E ***défini*** par : x 7 xF où x xF xG avec xF F et xG G Puisque E F G , xF et xG sont uniques donc pF kG est bien définie

**11571**: 1.9.2 Soit f un endomorphisme d' un K - espace vectoriel E , démontrer que : 1.9.3 Soit f et g deux endomorphismes d' un K - espace vectoriel E , démontrer que : Projecteurs et symétries Soit E un K - espace vectoriel , F et G deux sous-espaces vectoriels de E tels que E F G. On appelle projection de E sur F parallèlement à G l' endomorphisme de E défini par : x 7 xF où x xF xG avec xF F et xG G Puisque E F G , xF et xG sont uniques donc pF kG est bien ***définie***

**11942**: Si p est un projecteur , on a alors : E Im p Ker p d' après l' exemple 1.18 , page 49 , donc p est la projection de E sur son image Im p parallèlement à son noyau Ker p. Soit E un K - espace vectoriel , F et G deux sous-espaces vectoriels de E tels que E F G. On peut définir de même la notion de symétrie de E par rapport à F , parallèlement à G. C' est l' automorphisme de E ***défini*** x xF xG 7 xF xG , où x xF xG avec xF F et xG G Soit E un K - espace vectoriel , F et G deux sous-espaces vectoriels de E tels que E F G. On a alors : sF kG sF kG idE De plus , on a sF kG 2.pF kG idE et pF kG

**12410**: 1.10.7 Soit f un endomorphisme d' un K - espace vectoriel E et p un projecteur de E , démontrer que : Cas particulier de la dimension ***finie*** Soit E et E 0 deux K - espaces vectoriels

**12662**: Soit E et E 0 deux K - espaces vectoriels et f L ( E , E 0 ) , alors : E 0 de dimension ***finie*** rang(f ) dim E 0 et rang(f ) dim E 0 f surjective E de dimension finie rang(f ) dim E et rang(f ) dim E f injective Démonstration Im f est un sous-espace vectoriel de E 0 qui est de dimension finie donc Im f est de dimension finie et rang f dim Im f dim E 0

**12679**: Soit E et E 0 deux K - espaces vectoriels et f L ( E , E 0 ) , alors : E 0 de dimension finie rang(f ) dim E 0 et rang(f ) dim E 0 f surjective E de dimension ***finie*** rang(f ) dim E et rang(f ) dim E f injective Démonstration Im f est un sous-espace vectoriel de E 0 qui est de dimension finie donc Im f est de dimension finie et rang f dim Im f dim E 0

**12705**: Soit E et E 0 deux K - espaces vectoriels et f L ( E , E 0 ) , alors : E 0 de dimension finie rang(f ) dim E 0 et rang(f ) dim E 0 f surjective E de dimension finie rang(f ) dim E et rang(f ) dim E f injective Démonstration Im f est un sous-espace vectoriel de E 0 qui est de dimension ***finie*** donc Im f est de dimension finie et rang f dim Im f dim E 0

**12866**: Si E et E 0 sont de dimensions finies , alors : dim E dim E 0 f L ( E , E 0 ) injective dim E dim E 0 f L ( E , E 0 ) bijective dim E dim E 0 f L ( E , E 0 ) surjective En particulier , tout K - espace vectoriel de dimension ***finie*** n est isomorphe à Kn

**13230**: Réciproquement , si il existe f L ( E , E 0 ) bijective , alors d' après ce qui précède n p et p n , donc Ces propriétés sont très importantes pour démontrer des égalités ou des inégalités de dimensions ! Proposition 1.6 Rang d' une composition Soit E , E 0 et E 00 trois K - espaces vectoriels de dimensions finies , u L ( E , E 0 ) et v L ( E 0 , E 00 ) , alors : rang(v u ) dim E 0 rang(v ) rang(u ) Démonstration On peut déjà remarquer que : donc , en introduisant des supplémentaires : l' inégalité ***demandée*** devient : Or , il existe une application naturelle qui va de F 0 dans F 00 , l' application : L' énoncé devient : La démonstration ne est alors qu' une vérification : soit x00 F 00 , alors x00 Im(v ) , donc , il existe Théorème 1.4 Théorème du rang Soit E et E 0 des K - espace vectoriels et f L ( E , E 0 )

**13346**: Si E est de dimension finie , alors : rang(f ) dim E dim Ker(f ) Théorème du rang Démonstration C' est une application immédiate du théorème de factorisation ( théorème 1.5 , page 62 ) dans le cas de la dimension ***finie***

**13431**: Soit E un K - espace vectoriel de dimension finie et f L ( E ) , alors : ( a ) En posant f 0 idE , on a : ( b ) De plus , ( c ) On peut alors poser : Proposition 1.7 Caractérisation des automorphismes en dimension ***finie*** Soit E un K - espace vectoriel de dimension finie et f L ( E ) , alors : f injective f surjective f bijective C' est faux en dimension infinie

**13651**: e0n ) une base de E 0 , il suffit de vérifier alors que : où si ( k , l ) 1 , n 1 , p , uk , l est ***définie*** par a : a. On rappelle la définition du symbole de Kronecker : On voit donc que si E et E 0 sont de dimensions finies , alors L ( E E 0 ) et L ( E 0 E ) sont isomorphes car ils ont même dimension ! Nous verrons que c' est faux en dimension infinie ! Il y a en fait deux sortes d' isomorphismes : 1

**14023**: ( b ) Soit v0 F , démontrer que v0 F v v0 , v F est un sous-espace vectoriel de L ( E ) On suppose dorénavant que E est de dimension ***finie*** n. ( c ) Calculer la dimension de v0 F. ( d ) Que peut -on dire du rang de v lorsque v F ? 1.11.7 Soit E un K - espace vectoriel de dimension finie et f L ( E ) , démontrer que : rang f k rang f k2 1.11.8 Soit E un K - espace vectoriel de dimension finie et f L ( E ) , démontrer que : 1.11.9 Soit E , E 0 , E 00 et E 000 quatre K - espaces vectoriels de dimensions finies , f L ( E , E 0 ) , g L ( E 0 , E 00 ) 1.11.10 Soit E un K - espace vectoriel de dimension finie , f L ( E ) GL ( E )

**14213**: Factorisation des applications linéaires Si F est un sous-espace vectoriel d' un K - espace vectoriel E , alors la restriction d' une application linéaire f L ( E , E 0 ) , ***notée*** fF et définie par : est une application linéaire de F dans E 0

**14216**: Factorisation des applications linéaires Si F est un sous-espace vectoriel d' un K - espace vectoriel E , alors la restriction d' une application linéaire f L ( E , E 0 ) , notée fF et ***définie*** par : est une application linéaire de F dans E 0

**14261**: De plus , est une application linéaire Démonstration Si f L ( E , E 0 ) et F 0 est un sous-espace vectoriel de E 0 tel que de f , ***notée*** f , alors la co-restriction et définie par : est une application linéaire de E dans F 0

**14268**: De plus , est une application linéaire Démonstration Si f L ( E , E 0 ) et F 0 est un sous-espace vectoriel de E 0 tel que de f , notée f , alors la co-restriction et ***définie*** par : est une application linéaire de E dans F 0

**14547**: Notation 1.2 Inclusion canonique Soit E un K - espace vectoriel , F un sous-espace vectoriel de E , on peut définir l' inclusion canonique de F dans E par : On a donc : iF E ( idE ) F Si G est un supplémentaire de F dans E ( E F G ) , on a : iF E idF Ce ne sont pas des réciproques ! Pour connaître une application linéaire ***définie*** sur un K - espace vectoriel dont on connaît une décomposition en somme directe , il faut et il suffit d' en connaître ses restrictions à chaque sous-espace vectoriel composant la décomposition

**14588**: Supposons ***connue*** f L ( E , E 0 ) , il est alors facile de connaître : 2

**15370**: On obtient un candidat au rôle de u. Il s' ***écrit*** : ( Synthèse ) Il suffit alors de vérifier qu' il convient a , si x E , alors : or w(x ) Im(v ) par hypothèse 2

**15503**: ) Si v est inversible , alors u v 1 w. Dans le cas contraire , nous allons essayer de nous y ramener : ( Analyse ) Si u existe , alors b w v u donc w Pour pouvoir restreindre v à F 0 ( supplémentaire de Ker(v ) ) , nous allons imposer une condition supplémentaire à u permettant la co-restriction à F 0 de u : e inversible ! donc , dans ce cas , on a la condition nécessaire : ( Synthèse ) Soit u L ( E , E 0 ) ***défini*** par : ou , si l' on préfère : Alors , si x E , on a Le candidat ne est parfois pas celui dont on a besoin ! La vérification est indispensable ! b. La co-restriction est possible d' après l' hypothèse

**15602**: Figure 1.5 Relèvement linéaire : position du problème Figure 1.6 Relèvement linéaire : construction des chemins Théorème 1.7 Extension linéaire Soit E , E 0 et E 00 trois K - espaces vectoriels , w L ( E , E 00 ) et u L ( E , E 0 ) , alors Démonstration ***Laissé*** en exercice

**15944**: Si E est de dimension finie , alors E ? est de dimension finie et dim E dim E ? En dimension ***finie*** , les deux espaces sont donc isomorphes

**15988**: Démonstration Si E est de dimension finie , c' est un cas particulier de la proposition 1.8 , page 58 : Si E ne est pas de dimension ***finie*** , voir l' exemple ci-dessous

**16148**: a. Ceci est aussi un exemple où L ( E , E 0 ) ne est pas isomorphe à L ( E 0 , E ) Si ( ei ) iI est une base de E , on peut définir la famille duale ***associée*** ( e?i ) iI ( E ? ) I par : Cette notation est très dangereuse ! En effet , si l' on change un des vecteurs ei , alors on change tous les vecteurs e?i

**16206**: Soit E un K - espace vectoriel et soit ( ei ) iI une base de E. La famille duale ***associée*** est une partie libre Démonstration Soit ( i1 ,

**16225**: , ip ) une sous-famille quelconque ***finie*** de I d' éléments distincts deux-à-deux et soit ( 1 ,

**16312**: , en ) est une base de E , alors la famille duale ***associée*** est une base de E ? ( dite base duale )

**16320**: , en ) est une base de E , alors la famille duale associée est une base de E ? ( ***dite*** base duale )

**16395**: Soit E un K - espace vectoriel et soit ( ei ) iI une base de E. Si E est de dimension infinie , alors la famille duale ***associée*** ne est jamais génératrice

**16409**: Démonstration Considérons la forme linéaire f E ? ***définie*** par f ( ei ) 1 pour tout i I. Si la famille duale associée à ( ei ) iI était génératrice , il existerait une sous-famille ( i1 ,

**16424**: Démonstration Considérons la forme linéaire f E ? définie par f ( ei ) 1 pour tout i I. Si la famille duale ***associée*** à ( ei ) iI était génératrice , il existerait une sous-famille ( i1 ,

**16446**: , ip ) ***finie*** de I tel que f Vect(fi1 ,

**16506**: , ip ( possible car E est de dimension infinie donc I est infini ) , on a Proposition 1.11 Base ante - duale Soit E un K - espace vectoriel de dimension ***finie*** n , ( 1 ,

**16532**: , n ) une base de E ? , alors il existe une unique Cette base est ***appelée*** base ante - duale de la base ( 1 ,

**16554**: Démonstration Soit l' application ***définie*** par : Cette application a les propriétés suivantes : est linéaire

**16618**: Comme un isomorphisme envoie une base sur une base , si l' on pose : la famille ***obtenue*** convient , et c' est clairement la seule

**16780**: Lorsque cette condition ne est pas ***vérifiée*** , exprimer 4 en fonction des trois autres

**16798**: En déduire une méthode de calcul ***approché*** d' une intégrale

**16829**: 1.13.3 Trouver les formes linéaires ***définies*** sur E C 0 ( R , R ) telles que : 1.13.4 Soit E un K - espace vectoriel de dimension finie , soit E1 , E2 , ... , Ep des sous-espaces vectoriels de E , donner une CNS pour que : Hyperplans Soit E un K - espace vectoriel , on appelle hyperplan de E , tout sous-espace vectoriel H tel que : L' écriture : s' appelle équation de l' hyperplan H. Il ne y a pas unicité de l' équation , car , si convient , alors , quelque soit K ,

**16852**: 1.13.3 Trouver les formes linéaires définies sur E C 0 ( R , R ) telles que : 1.13.4 Soit E un K - espace vectoriel de dimension ***finie*** , soit E1 , E2 , ... , Ep des sous-espaces vectoriels de E , donner une CNS pour que : Hyperplans Soit E un K - espace vectoriel , on appelle hyperplan de E , tout sous-espace vectoriel H tel que : L' écriture : s' appelle équation de l' hyperplan H. Il ne y a pas unicité de l' équation , car , si convient , alors , quelque soit K ,

**16973**: La dimension commune de tous les supplémentaires de F est ***appelée*** codimension de F et notée ( si E F G ) : codim F dim G Si E est de dimension finie , tous les sous-espaces vectoriels de E sont de codimension finie et si F est un sous-espace vectoriel de E , alors : codim F dim E dim F Cette notion ne est donc pas intéressante en dimension finie , elle nous sera surtout utile en dimension 1

**16978**: La dimension commune de tous les supplémentaires de F est appelée codimension de F et ***notée*** ( si E F G ) : codim F dim G Si E est de dimension finie , tous les sous-espaces vectoriels de E sont de codimension finie et si F est un sous-espace vectoriel de E , alors : codim F dim E dim F Cette notion ne est donc pas intéressante en dimension finie , elle nous sera surtout utile en dimension 1

**17067**: Dans K3 , tout plan est un hyperplan , dans K2 , ce sont les droites ( ces sous-espaces vectoriels sont usuellement ***décrits*** par une équation )

**17092**: Dans E C 0 ( R , R ) , si a R , le sous-espace vectoriel ***défini*** par : est un hyperplan d' équation : f ( a ) 0

**17205**: Proposition 1.12 Caractérisation des hyperplans Soit E un K - espace vectoriel , H un sous-espace vectoriel de E , alors : H hyperplan de E codim H 1 Démonstration ( ) Soit ( x ) 0 une équation de H , comme est non nulle , on peut trouver un vecteur a E , tel ( ) Si E H K.a , a E 0E , alors , on peut prendre comme forme linéaire ***associée*** à H : Si E est de dimension finie , les hyperplans de E sont les sous-espaces vectoriels de dimension dim E1

**17214**: Proposition 1.12 Caractérisation des hyperplans Soit E un K - espace vectoriel , H un sous-espace vectoriel de E , alors : H hyperplan de E codim H 1 Démonstration ( ) Soit ( x ) 0 une équation de H , comme est non nulle , on peut trouver un vecteur a E , tel ( ) Si E H K.a , a E 0E , alors , on peut prendre comme forme linéaire associée à H : Si E est de dimension ***finie*** , les hyperplans de E sont les sous-espaces vectoriels de dimension dim E1

**17242**: Dans R2 , une équation de la droite ( hyperplan ) ***engendrée*** par ( 1 , 2 ) est , par exemple : 2

**17266**: Dans R3 , une équation du plan ( hyperplan ) ***engendré*** par ( 1 , 1 , 0 ) et ( 1 , 0 , 2 ) est , par exemple : 3

**17304**: Dans C 0 ( R , R ) , un supplémentaire de la droite ***engendrée*** par x 7 x , pourrait être donné par une équation du type : Théorème 1.8 Faisceaux d' hyperplans Soit E un K - espace vectoriel , soit n N , soit ( 1 ,

**17312**: Dans C 0 ( R , R ) , un supplémentaire de la droite engendrée par x 7 x , pourrait être ***donné*** par une équation du type : Théorème 1.8 Faisceaux d' hyperplans Soit E un K - espace vectoriel , soit n N , soit ( 1 ,

**17465**: , p1 ) des formes linéaires de E et E ? telle que : Si p1 est nulle , c' est ***terminé***

**17587**: Notons H1 Ker(1 ) , H2 Ker(2 ) , si 1 et 2 sont indépendantes , les deux plans se coupent suivant la droite D. Soit K un plan contenant D ( comme sur le dessin ) , où K Ker ( ) , le théorème nous assure alors Le plan K a donc pour équation : Cette équation est ***définie*** à un coefficient de proportionnalité près , donc Figure 1.7 Hyperplans de R3 Le résultat de cette proposition est particulièrement intéressant en géométrie affine

**17716**: Soit D une droite affine et A un point , cherchons les plans tangents à la sphère de centre A de rayon 1 , contenant D. Par exemple : Soit D la droite ***définie*** par : 4 x y z 0 , 2 x 5 y 3 z 4 0

**17763**: On va chercher le plan ***demandé*** sous la forme : Pour trouver , il suffit d' écrire d((1 , 1 , 1 ) , P ) 1

**18004**: Il reste à montrer que : de dimension p1 donc , en utilisant la première question de l' exercice 1.4.2 , page 32 , on obtient , puisque a E2 C' est ainsi que l' on retrouve que dans l' espace , les droites sont ***définies*** par 2 équations

**18146**: Cependant , l' intersection d' hyperplans affines peut-être vide , aussi faut -il , avant toutes choses , s' assurer qu' elle ne l' est pas ! Puis , en s' appuyant sur un point ***trouvé*** de l' intersection , on est ramené au cas vectoriel

**18153**: Cependant , l' intersection d' hyperplans affines peut-être vide , aussi faut -il , avant toutes choses , s' assurer qu' elle ne l' est pas ! Puis , en s' appuyant sur un point trouvé de l' intersection , on est ***ramené*** au cas vectoriel

**18216**: , p , alors : Démonstration ***Laissé*** en exercice

**18282**: 1.14.3 Soit E un K - espace vectoriel de dimension ***finie*** , soit V un sous-espace vectoriel de E ?

**18491**: ( e ) Si E est de dimension finie , alors dim B codim B ( f ) En déduire que l' inclusion de la question ( d ) est une inégalité en dimension ***finie***

**18570**: On définit l' application ***transposée*** de u et on note : ( a ) Démontrer que t ( u v ) t u t v. ( b ) Démontrer que : ( c ) Démontrer que : Applications Systèmes linéaires Ce paragraphe , très simple , est fondamental ! Nous nous en servirons très souvent ! Soit E et E 0 deux K - espaces vectoriels , u L ( E , E 0 ) et e0 E 0 , on appelle système linéaire l' équation d' inconnue x E : L' ensemble : est appelé ensemble des solutions de ( S )

**18661**: On définit l' application transposée de u et on note : ( a ) Démontrer que t ( u v ) t u t v. ( b ) Démontrer que : ( c ) Démontrer que : Applications Systèmes linéaires Ce paragraphe , très simple , est fondamental ! Nous nous en servirons très souvent ! Soit E et E 0 deux K - espaces vectoriels , u L ( E , E 0 ) et e0 E 0 , on appelle système linéaire l' équation d' inconnue x E : L' ensemble : est ***appelé*** ensemble des solutions de ( S )

**18705**: Une condition nécessaire et suffisante pour que Sol(S ) 6 , ***appelée*** condition de compatibilité de ( S ) est : Démonstration Si Sol(S ) 6 , il existe x E tel que u(x ) e0 , en particulier e0 Im u. Si e0 Im u , il existe x E tel que e0 u(x ) , donc Sol(S ) 6

**18787**: Si e0 0E0 , le système est ***dit*** homogène

**18844**: Si e0 6 0E0 , le système : est ***dit*** système homogène associé de ( S )

**18847**: Si e0 6 0E0 , le système : est dit système homogène ***associé*** de ( S )

**18938**: Si e0 s' ***écrit*** comme une somme : Si pour tout k 1 , p , xk est une solution du système linéaire : alors une solution de ( S ) est Démonstration C' est immédiat par linéarité de u : donc x Sol(S )

**19140**: ( c ) Écrire le système homogène ***associé*** et le résoudre

**19208**: ( c ) Écrire le système homogène ***associé*** et le résoudre

**19234**: ( e ) Comparer aux solutions du système récurrent ***obtenu*** par discrétisation : Interpolation Proposition 1.14 Interpolation de Lagrange Soit f : I K , où I est un intervalle de R. Soit x1 xn des réels dans I , on appelle fonction polynomiale d' interpolation de Lagrange l' unique fonction polynomiale P de degré n , telle que Elle est égale à : Démonstration E f polynomiale de degré n Cet espace vectoriel est clairement de dimension n , de base La famille de E ? définie par : étant une base de E ? , on sait qu' il existe une base ante - duale ( Pk ) k1,n qui vérifie : Un calcul simple nous démontre que : On cherche ensuite une solution sous la forme : en évaluant sur les xj , on trouve l' unique solution de l' énoncé

**19312**: ( e ) Comparer aux solutions du système récurrent obtenu par discrétisation : Interpolation Proposition 1.14 Interpolation de Lagrange Soit f : I K , où I est un intervalle de R. Soit x1 xn des réels dans I , on appelle fonction polynomiale d' interpolation de Lagrange l' unique fonction polynomiale P de degré n , telle que Elle est égale à : Démonstration E f polynomiale de degré n Cet espace vectoriel est clairement de dimension n , de base La famille de E ? ***définie*** par : étant une base de E ? , on sait qu' il existe une base ante - duale ( Pk ) k1,n qui vérifie : Un calcul simple nous démontre que : On cherche ensuite une solution sous la forme : en évaluant sur les xj , on trouve l' unique solution de l' énoncé

**19516**: Pour p N , on pose Lp ( f ) la fonction polynomiale d' interpolation de Lagrange de f pour les points : On suppose que : Démontrer que : 1.16.4 Soit f : x 7 x , ***définie*** sur 1 , 1

**19634**: On appelle fonctions spline , des fonctions qui ont une classe ***fixée*** ( par exemple C 2 ) et qui sont polynomiales par morceaux

**19703**: , ep ) une base de E , alors : Autrement ***dit*** , en utilisant la dualité , pour tout k 1 , p , xk e?k ( x )

**19762**: , e0n ) des bases de E Autrement ***dit*** , en utilisant la dualité , pour tout ( i , j ) 1 , n 1 , p , ai , j ( e0i ) ? ( u(ej ) )

**19845**: Pour tout x E , on a donc : ai , j xj .e0i On appelle matrice à n lignes et p colonnes ( n , p ) ( N ) 2 , toute famille ( ai , j ) ( i , j)1,n1,p d' éléments d' un ensemble A ***représentée*** sous la forme d' un tableau à n lignes et p colonnes entouré par des crochets a : Les éléments ai , j de la matrice s' appellent coefficients de la matrice

**19858**: Pour tout x E , on a donc : ai , j xj .e0i On appelle matrice à n lignes et p colonnes ( n , p ) ( N ) 2 , toute famille ( ai , j ) ( i , j)1,n1,p d' éléments d' un ensemble A représentée sous la forme d' un tableau à n lignes et p colonnes ***entouré*** par des crochets a : Les éléments ai , j de la matrice s' appellent coefficients de la matrice

**19968**: Si E est un K - espace vectoriel de dimension ***finie*** p , si E ( e1 ,

**20157**: Réciproquement , si A ai , j ( i , j)1,n1,p Mn , p ( K ) , on peut lui associer canoniquement l' application linéaire f ***définie*** sur Kp ( de base canonique ( e1 ,

**20377**: Si E R2 et f est la symétrie par rapport à y x , parallèlement à y x et si E ( 1 , 0 ) , ( 0 , 1 ) , On définit les deux opérations suivantes sur Mn , p ( K ) : L' addition a : si A Mn , p ( K ) et si B Mn , p ( K ) , on définit A B Mn , p ( K ) par an , p bn , p La multiplication externe : si K et si A Mn , p ( K ) , on définit .A Mn , p ( K ) par Il faut que les dimensions des matrices A et B soient les mêmes ! L' élément neutre pour l' addition de Mn , p ( K ) est ***appelée*** la matrice nulle , notée 0n , p et définie par Cette matrice correspond à l' application linéaire nulle 0L ( E , E 0 )

**20382**: Si E R2 et f est la symétrie par rapport à y x , parallèlement à y x et si E ( 1 , 0 ) , ( 0 , 1 ) , On définit les deux opérations suivantes sur Mn , p ( K ) : L' addition a : si A Mn , p ( K ) et si B Mn , p ( K ) , on définit A B Mn , p ( K ) par an , p bn , p La multiplication externe : si K et si A Mn , p ( K ) , on définit .A Mn , p ( K ) par Il faut que les dimensions des matrices A et B soient les mêmes ! L' élément neutre pour l' addition de Mn , p ( K ) est appelée la matrice nulle , ***notée*** 0n , p et définie par Cette matrice correspond à l' application linéaire nulle 0L ( E , E 0 )

**20387**: Si E R2 et f est la symétrie par rapport à y x , parallèlement à y x et si E ( 1 , 0 ) , ( 0 , 1 ) , On définit les deux opérations suivantes sur Mn , p ( K ) : L' addition a : si A Mn , p ( K ) et si B Mn , p ( K ) , on définit A B Mn , p ( K ) par an , p bn , p La multiplication externe : si K et si A Mn , p ( K ) , on définit .A Mn , p ( K ) par Il faut que les dimensions des matrices A et B soient les mêmes ! L' élément neutre pour l' addition de Mn , p ( K ) est appelée la matrice nulle , notée 0n , p et ***définie*** par Cette matrice correspond à l' application linéaire nulle 0L ( E , E 0 )

**20421**: Lorsque n p , on note Proposition 2.1 L' ensemble Mn , p ( K ) ***muni*** des deux opérations ci - dessus est un K - espace vectoriel isomorphe à L ( E , E 0 ) pour tous K - espaces vectoriels E et E 0 , toute base E de E et toute base E 0 de E 0 , via l' isomorphisme En particulier : dim Mn , p ( K ) n p Démonstration Il est immédiat que Mn , p ( K ) est un K - espace vectoriel : c' est le même espace que Kn Kp muni des opérations usuelles ( il est donc de dimension finie n p )

**20509**: Lorsque n p , on note Proposition 2.1 L' ensemble Mn , p ( K ) muni des deux opérations ci - dessus est un K - espace vectoriel isomorphe à L ( E , E 0 ) pour tous K - espaces vectoriels E et E 0 , toute base E de E et toute base E 0 de E 0 , via l' isomorphisme En particulier : dim Mn , p ( K ) n p Démonstration Il est immédiat que Mn , p ( K ) est un K - espace vectoriel : c' est le même espace que Kn Kp ***muni*** des opérations usuelles ( il est donc de dimension finie n p )

**20519**: Lorsque n p , on note Proposition 2.1 L' ensemble Mn , p ( K ) muni des deux opérations ci - dessus est un K - espace vectoriel isomorphe à L ( E , E 0 ) pour tous K - espaces vectoriels E et E 0 , toute base E de E et toute base E 0 de E 0 , via l' isomorphisme En particulier : dim Mn , p ( K ) n p Démonstration Il est immédiat que Mn , p ( K ) est un K - espace vectoriel : c' est le même espace que Kn Kp muni des opérations usuelles ( il est donc de dimension ***finie*** n p )

**20554**: Autrement ***dit*** , pour tout i 1 , n et en notant E ( e1 ,

**20604**: On a donc u 0L ( E , E 0 ) et on en ***déduit*** que Ker 0L ( E , E 0 ) donc est injective

**20654**: En particulier , on retrouve le fait que et la propriété de linéarité En dimension ***finie*** , connaître une application linéaire revient à connaître sa matrice dans des bases données

**20722**: Il existe une base naturelle de Mn , p ( K ) , ***appelée*** base canonique , donnée par Autrement dit , le coefficient en ( i , j ) de Ek , est nul , sauf pour ( i , j ) ( k , ) en lequel il vaut 1

**20726**: Il existe une base naturelle de Mn , p ( K ) , appelée base canonique , ***donnée*** par Autrement dit , le coefficient en ( i , j ) de Ek , est nul , sauf pour ( i , j ) ( k , ) en lequel il vaut 1

**20729**: Il existe une base naturelle de Mn , p ( K ) , appelée base canonique , donnée par Autrement ***dit*** , le coefficient en ( i , j ) de Ek , est nul , sauf pour ( i , j ) ( k , ) en lequel il vaut 1

**20798**: Soit A Mn , p ( K ) et B Mp , q ( K ) , on définit le produit de A par B comme la matrice A B Mn , q ( K ) ***définie*** par Voir la figure 2.1 , page ci - contre a

**20871**: a. Tiré de http : www.texample.nettikzexamplesmatrix - multiplication Il faut bien faire attention à ce que les dimensions des matrices soient compatibles : lorsque l' on veut faire le produit A B , le nombre de colonnes de A doit être égal au nombre de lignes de B. Soit E et E 0 deux K - espaces vectoriels de dimension ***finie*** ( avec p dim E et n dim E 0 ) , soit E une base de E et E 0 une base de E 0 , soit f L ( E , E 0 ) et soit x E. Alors Autrement dit , le produit matriciel MatE , E 0 ( f ) MatE ( x ) traduit le calcul de f ( x )

**20914**: a. Tiré de http : www.texample.nettikzexamplesmatrix - multiplication Il faut bien faire attention à ce que les dimensions des matrices soient compatibles : lorsque l' on veut faire le produit A B , le nombre de colonnes de A doit être égal au nombre de lignes de B. Soit E et E 0 deux K - espaces vectoriels de dimension finie ( avec p dim E et n dim E 0 ) , soit E une base de E et E 0 une base de E 0 , soit f L ( E , E 0 ) et soit x E. Alors Autrement ***dit*** , le produit matriciel MatE , E 0 ( f ) MatE ( x ) traduit le calcul de f ( x )

**20930**: a. Tiré de http : www.texample.nettikzexamplesmatrix - multiplication Il faut bien faire attention à ce que les dimensions des matrices soient compatibles : lorsque l' on veut faire le produit A B , le nombre de colonnes de A doit être égal au nombre de lignes de B. Soit E et E 0 deux K - espaces vectoriels de dimension finie ( avec p dim E et n dim E 0 ) , soit E une base de E et E 0 une base de E 0 , soit f L ( E , E 0 ) et soit x E. Alors Autrement dit , le produit matriciel MatE , E 0 ( f ) MatE ( x ) ***traduit*** le calcul de f ( x )

**20983**: On a Autrement ***dit*** , ai , j xj .e0i j1 ai , j xj est le i - ième coefficient de f ( x ) dans la base E ( e1 ,

**21098**: Proposition 2.2 Correspondance entre composition et produit matriciel Soit E , E 0 et E 00 des K - espaces vectoriels de dimension finie ( p dim E , n dim E 0 et q dim E 00 ) , soit Autrement ***dit*** , le produit matriciel MatE 0 , E 00 ( g ) MatE , E 0 ( f ) traduit le calcul de g f

**21118**: Proposition 2.2 Correspondance entre composition et produit matriciel Soit E , E 0 et E 00 des K - espaces vectoriels de dimension finie ( p dim E , n dim E 0 et q dim E 00 ) , soit Autrement dit , le produit matriciel MatE 0 , E 00 ( g ) MatE , E 0 ( f ) ***traduit*** le calcul de g f

**21158**: Soit j 1 , p. La j - ième colonne de MatE , E 00 ( g f ) est ***donnée*** par en remarquant que MatE ( ej ) Mp,1 ( K ) a des zéros partout sauf en position j et en utilisant plusieurs fois la propriété 2.1 , page 98

**21431**: Lorsque p n , on appelle matrice identité d' ordre p et on note : La matrice identité Ip correspond l' endomorphisme idE , l' application identité de E ( avec p dim E ) , dans ne importe quelle base de E. Il est important de savoir calculer les produits de matrices et de savoir prouver que le résultat est correct ! Ainsi , si Ek , est de taille n p , et Er , s est de taille p q , alors : En effet : Les dimensions sont compatibles , on peut effectuer le produit et : Le produit est associatif : Le produit est distributif à gauche et à droite : Existence d' éléments neutres ( à gauche et à droite ) pour la multiplication : Existence d' éléments absorbants pour la multiplication : Démonstration Deux méthodes : on peut vérifier ces égalités en calculant tous ces produits en utilisant la définition du produit ( définition 2.4 , page 98 ) on peut également utiliser l' isomorphisme de la proposition 2.1 , page 96 , les égalités ci - dessus découlent alors des propriétés de la composition des applications linéaires ( la démonstration est ***laissée*** en exercice )

**21455**: Pour toute matrice carrée A Mn ( K ) , on pose Si A et B sont deux matrices ***carrées*** qui commutent ( c' est - à - dire A.B B .A ) , alors on a la formule du binôme : A0 In et pour tout k N , Ak1 Ak A L' hypothèse A B B A est indispensable

**21564**: 2.1.4 Soit ( a , b ) R2 et soit A Mn ( R ) ***définie*** par : ai , j a si i j Trouver toutes les matrices qui commutent avec A , c' est - à - dire déterminer l' ensemble 2.1.5 On suppose que a , b et c sont trois nombrs complexes tels que a2 b2 c2 1

**21655**: On pose : Démontrer que : 2.1.6 Soit A , B et C trois matrices de Mn ( R ) telles que : Démontrer que : Transposition Soit A un ensemble , ( n , p ) N 2 , alors l' application ***définie*** par : est appelée transposition

**21659**: On pose : Démontrer que : 2.1.6 Soit A , B et C trois matrices de Mn ( R ) telles que : Démontrer que : Transposition Soit A un ensemble , ( n , p ) N 2 , alors l' application définie par : est ***appelée*** transposition

**21667**: La matrice t M est ***appelée*** la transposée a de M

**21669**: La matrice t M est appelée la ***transposée*** a de M

**21781**: Il y deux manières de considérer les formes linéaires d' un K - espace vectoriel E de dimension finie : comme des applications linéaires de E dans K elles sont alors ***représentées*** par des matrices de M1,p ( K ) en fixant une base de E ( avec p dim E ) comme des vecteurs de E ? elles sont alors représentées dans une base de E ? par des matrices de Y a -t -il un lien entre ces deux représentations ? Soit E ( e1 ,

**21811**: Il y deux manières de considérer les formes linéaires d' un K - espace vectoriel E de dimension finie : comme des applications linéaires de E dans K elles sont alors représentées par des matrices de M1,p ( K ) en fixant une base de E ( avec p dim E ) comme des vecteurs de E ? elles sont alors ***représentées*** dans une base de E ? par des matrices de Y a -t -il un lien entre ces deux représentations ? Soit E ( e1 ,

**21895**: , ep ) une base de E et E ? , en prenant ( 1 ) comme base de K , on a : On a donc , pour tout x E : Notation 2.1 Dans la suite de ce cours , on conviendra que les matrices à 1 ligne et 1 colonne seront ***notées*** comme des scalaires

**21930**: Soit : On a donc , avec cet abus de notation , Soit E ? la base duale de E , notons : de sorte que : Finalement : Autrement ***dit*** , 2

**21976**: L' ensemble des matrices symétriques de Mp ( K ) est ***noté*** On dit que M Mp ( K ) est antisymétrique si elle vérifie t M M

**22005**: L' ensemble des matrices antisymétriques de Mp ( K ) est ***noté*** a. Les matrices M vérifiant t M M sont nécessairement carrées ! ( a ) Démontrer que Sp ( K ) et Ap ( K ) sont des sous-espaces vectoriels de Mp ( K )

**22016**: L' ensemble des matrices antisymétriques de Mp ( K ) est noté a. Les matrices M vérifiant t M M sont nécessairement ***carrées*** ! ( a ) Démontrer que Sp ( K ) et Ap ( K ) sont des sous-espaces vectoriels de Mp ( K )

**22094**: Matrices diagonales , matrices triangulaires On dit que A est diagonale si tous ses coefficients non-diagonaux sont nuls , c' est - à - dire : Autrement ***dit*** : On note Dp ( K ) l' ensemble des matrices diagonales de Mp ( K )

**22141**: On dit que A est triangulaire supérieure si tous ses coefficients au - dessous de sa diagonale sont nuls , c' est - à - dire : Autrement ***dit*** : p ( K ) l' ensemble des matrices triangulaires supérieures de Mp ( K )

**22185**: On dit que A est triangulaire inférieur si tous ses coefficients au-dessus de sa diagonale sont nuls , c' est - à - dire : Autrement ***dit*** : p ( K ) l' ensemble des matrices triangulaires inférieures de Mp ( K )

**22296**: On définit la trace de A , ***notée*** trace(A ) , comme la somme des éléments diagonaux de A : Il est indispensable que A soit une matrice carrée ! 1

**22540**: Si c' est le cas , on a Proposition 2.3 Lien entre automorphismes et matrices inversibles Soit E un K - espace vectoriel de dimension ***finie*** , soit E une base de E et soit f L ( E )

**22917**: , xr ) dans la base E et on note : MatE ( X ) MatE où ai , j est le i - ième coefficient de xj dans la base E : Autrement ***dit*** , par dualité , pour tout ( i , j ) 1 , n 1 , r , ai , j e?i ( xj )

**23076**: On appelle matrice de passage de E à B et on note : C' est donc la matrice de la nouvelle base B ***exprimée*** dans l' ancienne base E

**23220**: Si E est un K - espace vectoriel de dimension ***finie*** avec p dim E , si E est une base de E , alors une famille X ( x1 ,

**23319**: Proposition 2.4 Changement de base pour les vecteurs Soit E un K - espace vectoriel de dimension finie , E et B deux bases de E , x E. Alors : MatE ( x ) PE Autrement ***dit*** , en multipliant à gauche par PE , on obtient les anciennes coordonnées en fonction des nouvelles coordonnées

**23505**: Figure 2.2 Changement de base pour les applications linéaires Le noyau de A est le sous-espace vectoriel de Mp,1 ( K ) ***défini*** par : L' image de A est le sous-espace vectoriel de Mn,1 ( K ) défini par : Le rang de A , noté rang(A ) , est la dimension de Im(A ) : rang(A ) dim Im(A ) a. On notera aussi Ker A , Im A et rang A. Soit A Mn , p ( K )

**23521**: Figure 2.2 Changement de base pour les applications linéaires Le noyau de A est le sous-espace vectoriel de Mp,1 ( K ) défini par : L' image de A est le sous-espace vectoriel de Mn,1 ( K ) ***défini*** par : Le rang de A , noté rang(A ) , est la dimension de Im(A ) : rang(A ) dim Im(A ) a. On notera aussi Ker A , Im A et rang A. Soit A Mn , p ( K )

**23529**: Figure 2.2 Changement de base pour les applications linéaires Le noyau de A est le sous-espace vectoriel de Mp,1 ( K ) défini par : L' image de A est le sous-espace vectoriel de Mn,1 ( K ) défini par : Le rang de A , ***noté*** rang(A ) , est la dimension de Im(A ) : rang(A ) dim Im(A ) a. On notera aussi Ker A , Im A et rang A. Soit A Mn , p ( K )

**23580**: , Cp les colonnes de A ( ***vues*** comme des matrices colonnes de En particulier , rang A min(n , p ) car Im A est un sous-espace vectoriel de Mn,1 ( K ) engendré par les p colonnes de A. Soit E et E 0 deux K - espaces vectoriels de dimension finie , soit E une base de E , soit E 0 une base de E 0 1

**23607**: , Cp les colonnes de A ( vues comme des matrices colonnes de En particulier , rang A min(n , p ) car Im A est un sous-espace vectoriel de Mn,1 ( K ) ***engendré*** par les p colonnes de A. Soit E et E 0 deux K - espaces vectoriels de dimension finie , soit E une base de E , soit E 0 une base de E 0 1

**23626**: , Cp les colonnes de A ( vues comme des matrices colonnes de En particulier , rang A min(n , p ) car Im A est un sous-espace vectoriel de Mn,1 ( K ) engendré par les p colonnes de A. Soit E et E 0 deux K - espaces vectoriels de dimension ***finie*** , soit E une base de E , soit E 0 une base de E 0 1

**24085**: avoir même dimension est une relation d' équivalence sur l' ensemble des sous-espaces vectoriels d' un même espace vectoriel E de dimension ***finie***

**24289**: Plus généralement , si n N , n 2 , on peut s' intéresser aux ensembles : ( Ek ) k0,n1 est une partition de N Soit ( Ei ) iI une partition d' un ensemble E non vide et soit R la relation sur E ***définie*** par : ( autrement dit , x et y sont en relation lorsque x et y appartiennent à un même Ei )

**24294**: Plus généralement , si n N , n 2 , on peut s' intéresser aux ensembles : ( Ek ) k0,n1 est une partition de N Soit ( Ei ) iI une partition d' un ensemble E non vide et soit R la relation sur E définie par : ( autrement ***dit*** , x et y sont en relation lorsque x et y appartiennent à un même Ei )

**24385**: Alors R est une relation d' équivalence sur E. Démonstration C' est immédiat , une fois qu' on a remarqué que pour tout x E , il existe i I tel que x Ei car la partition ( Ei ) iI recouvre E. Soit R une relation d' équivalence sur un ensemble E non vide et soit x E. La classe d' équivalence de x modulo R est le sous-ensemble de E ***défini*** par : Classe(x , R ) y E , xRy Un élément d' une classe d' équivalence est appelé un représentant de cette classe d' équivalence

**24404**: Alors R est une relation d' équivalence sur E. Démonstration C' est immédiat , une fois qu' on a remarqué que pour tout x E , il existe i I tel que x Ei car la partition ( Ei ) iI recouvre E. Soit R une relation d' équivalence sur un ensemble E non vide et soit x E. La classe d' équivalence de x modulo R est le sous-ensemble de E défini par : Classe(x , R ) y E , xRy Un élément d' une classe d' équivalence est ***appelé*** un représentant de cette classe d' équivalence

**24599**: Alors les classes d' équivalence forment une partition de E. Plus formellement , si on définit la famille ( Ei ) iI des classes d' équivalence , ***donnée*** par I Classe(x , R ) , x E et pour tout i I , Ei i alors ( Ei ) iI est une partition de E. Démonstration On a vu que pour tout i I , Ei est non vide

**24674**: On en ***déduit*** que ( Ei ) iI recouvre E Soit ( i , j ) I 2 tel que i 6 j. Supposons que Ei Ej 6

**24757**: On a donc Ei Ej , ce qui ***contredit*** i 6 j donc Ei Ej

**24893**: On définit de même la relation sur P(E ) par : ( c ) C' est une relation d' équivalence , trouver une bijection de l' ensemble des classes d' équivalence avec un ensemble ***connu***

**24937**: 2.5.3 Soit E un ensemble non vide , on définit la relation sur P(E ) par : f F ( X , Y ) , injective Cette relation est-elle réflexive ? Symétrique ? Transitive ? 2.5.4 Soit E un ensemble non vide ***muni*** d' une relation R réflexive et transitive

**24977**: On définit les deux relations suivantes : xRy ou yRx Les relations S et T sont - elles réflexives ? Symétriques ? Transitives ? 2.5.5 Soit E un ensemble non vide ***muni*** d' une relation d' équivalence R , on pose : Classe(a , R ) Soit A une partie de E. ( a ) Démontrer que A s(A )

**25062**: Deux matrices M et N de Mn , p ( K ) sont ***dites*** équivalentes si : Si c' est le cas , on note Cela définit une relation sur Mn , p ( K ) appelée équivalence

**25085**: Deux matrices M et N de Mn , p ( K ) sont dites équivalentes si : Si c' est le cas , on note Cela définit une relation sur Mn , p ( K ) ***appelée*** équivalence

**25101**: Deux matrices M et N de Mp ( K ) sont ***dites*** semblables si : Si c' est le cas , on note Cela définit une relation sur Mp ( K ) appelée similitude

**25122**: Deux matrices M et N de Mp ( K ) sont dites semblables si : Si c' est le cas , on note Cela définit une relation sur Mp ( K ) ***appelée*** similitude

**25217**: Démonstration ***Laissée*** en exercice ( utiliser la formule de changement de base et les propriété des matrices de passage )

**25255**: Notation 2.2 Nous noterons Jn , p , r la matrice de Mn , p ( K ) ***définie*** par : Proposition 2.6 Caractérisation des matrices équivalentes Deux matrices de Mn , p ( K ) sont équivalentes si , et seulement si , elles ont même rang

**25332**: Supposons - les équivalentes , il existe P GLn ( K ) et Q GLp ( K ) telles que N P M Q. En notant uX l' application linéaire canoniquement ***associée*** à une matrice X , on a uN uP uM uQ La conservation du rang en découle , car uP et uQ sont inversibles ( voir la remarque 2.14 , page 115 )

**25722**: ( a ) Justifier que la trace de MatE ( f ) ne dépend pas de la base E de E ***choisie***

**25732**: On définit alors la trace de f , ***notée*** trace(f ) , comme la valeur de trace(MatE ( f ) ) dans ne importe quelle base E de E. ( b ) Soit p un projecteur de E. Démontrer que trace(p ) rang(p )

**25816**: 2.6.3 Démontrer que A Mn , p ( K ) , rang(t A ) rang(A ) 2.6.4 Démontrer que toute matrice de Mp ( K ) non inversible est équivalente à une matrice B telle qu' il existe k N tel que B k 0p ( matrice ***dite*** nilpotente )

**25855**: Systèmes linéaires Algorithme du pivot de Gauss Notation 2.3 Soit ( k , ) 1 , p , k 6 et K , on appelle matrice de transvection la matrice de Mp ( K ) ***définie*** par : Cette matrice est inversible et son inverse est : Démonstration Un calcul direct donne Notation 2.4 Soit k 1 , p , K , on appelle matrice de dilatation la matrice de Mp ( K ) définie par : à la k - ième place Cette matrice est inversible et son inverse est : Démonstration Un calcul immédiat démontre que Notation 2.5 Soit une permutation a de 1 , p , on appelle matrice de permutation la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : a. C' est - à - dire une bijection de de 1 , p dans 1 , p. Démonstration Si et 0 sont deux permutations de 1 , p alors on remarque que : On prend alors 0 1 en remarquant que si id1,p , alors P Ip

**25894**: Systèmes linéaires Algorithme du pivot de Gauss Notation 2.3 Soit ( k , ) 1 , p , k 6 et K , on appelle matrice de transvection la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : Démonstration Un calcul direct donne Notation 2.4 Soit k 1 , p , K , on appelle matrice de dilatation la matrice de Mp ( K ) ***définie*** par : à la k - ième place Cette matrice est inversible et son inverse est : Démonstration Un calcul immédiat démontre que Notation 2.5 Soit une permutation a de 1 , p , on appelle matrice de permutation la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : a. C' est - à - dire une bijection de de 1 , p dans 1 , p. Démonstration Si et 0 sont deux permutations de 1 , p alors on remarque que : On prend alors 0 1 en remarquant que si id1,p , alors P Ip

**25941**: Systèmes linéaires Algorithme du pivot de Gauss Notation 2.3 Soit ( k , ) 1 , p , k 6 et K , on appelle matrice de transvection la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : Démonstration Un calcul direct donne Notation 2.4 Soit k 1 , p , K , on appelle matrice de dilatation la matrice de Mp ( K ) définie par : à la k - ième place Cette matrice est inversible et son inverse est : Démonstration Un calcul immédiat démontre que Notation 2.5 Soit une permutation a de 1 , p , on appelle matrice de permutation la matrice de Mp ( K ) ***définie*** par : Cette matrice est inversible et son inverse est : a. C' est - à - dire une bijection de de 1 , p dans 1 , p. Démonstration Si et 0 sont deux permutations de 1 , p alors on remarque que : On prend alors 0 1 en remarquant que si id1,p , alors P Ip

**26031**: , Ln les lignes de A ( ***vues*** comme des matrices 1 p ) , et C1 ,

**26052**: , Cp les colonnes de A ( ***vues*** comme des matrices n 1 )

**26078**: Transvection Cela revient donc à faire la transformation ( ***dite*** opération élémentaire ) : ( on remplace la k - ième ligne par la k - ième ligne à laquelle on a ajouté fois la -ième ligne )

**26119**: Dilatation Cela revient donc à faire la transformation ( ***dite*** opération élémentaire ) : ( on remplace la k - ième ligne par fois la k - ième ligne )

**26152**: Permutation Cela revient donc à faire la transformation ( ***dite*** opération élémentaire ) : Soit A ai , j ( i , j)1,n1,p Mn , p ( K )

**26186**: , Ln les lignes de A ( ***vues*** comme des matrices 1 p ) , et C1 ,

**26207**: , Cp les colonnes de A ( ***vues*** comme des matrices n 1 )

**26233**: Transvection Cela revient donc à faire la transformation ( ***dite*** opération élémentaire ) : ( on remplace la -ième colonne par la -ième colonne à laquelle on a ajouté fois la k - ième colonne )

**26272**: Dilatation Cela revient donc à faire la transformation ( ***dite*** opération élémentaire ) : ( on remplace la k - ième colonne par fois la k - ième colonne )

**26305**: Permutation Cela revient donc à faire la transformation ( ***dite*** opération élémentaire ) : Lorsqu' on utilise des opérations élémentaires , c' est généralement pour faire apparaître le plus de 0 possibles dans la matrice

**26378**: on signale la ou les opérations élémentaires ***effectuées*** sous la matrice

**26424**: Soit la matrice : Théorème 2.1 du pivot généralisé de Gauss Soit A Mn , p ( K ) , de rang r , alors il existe des matrices de transvection - dilatation - permutation de Mn ( K ) , ***notées*** R1 ,

**26447**: , Rq et des matrices de transvection - dilatation - permutation de Mp ( K ) , ***notées*** S1 ,

**26655**: Sinon on recommence l' algorithme avec A0 à la place de A À la fin , on obtient une matrice Jn , p , r , ***obtenue*** par opérations élémentaires sur les lignes et les colonnes d' où le résultat

**26697**: Autrement ***dit*** , toute matrice de Mn , p ( K ) de rang r est équivalente à Jn , p , r

**26759**: Que se passe -t -il lorsque , par exemple , la première colonne est ***remplie*** de 0 ? On commence par permuter les colonnes ! Ici , nous allons écrire les matrices nous - mêmes ... Toute matrice de permutation peut s' exprimer comme produit de matrices de transvection - dilatation

**26869**: transposition k , p1 de 1 , p 1 ***définie*** par : On pose alors 0 k , p1

**26982**: On peut procéder de la manière suivante : Soit A GLp ( K ) , alors il existe des matrices de transvection - dilatation - permutation de Mp ( K ) , ***notées*** R1 ,

**27005**: , Rq et des matrices de transvection - dilatation - permutation de Mp ( K ) , ***notées*** S1 ,

**27017**: , Ss telles que : Autrement ***dit*** , quand A est inversible , on peut se contenter de travailler soit uniquement sur les lignes , soit uniquement sur les colonnes

**27070**: Démonstration En reprenant le résultat du théorème 2.1 , page 127 , il existe un produit de matrices de transvection - dilatationpermutation de Mn ( K ) , ***noté*** R et un produit de matrices de transvection - dilatation - permutation de Mp ( K ) , noté S tels que : car rang(A ) p ( car A est inversible ) et Jp , p , p Ip

**27089**: Démonstration En reprenant le résultat du théorème 2.1 , page 127 , il existe un produit de matrices de transvection - dilatationpermutation de Mn ( K ) , noté R et un produit de matrices de transvection - dilatation - permutation de Mp ( K ) , ***noté*** S tels que : car rang(A ) p ( car A est inversible ) et Jp , p , p Ip

**27600**: n ( K ) , soit 0 , démontrer qu' il existe une matrice P Dn ( K ) GLn ( K ) telle que : Systèmes linéaires On a déjà vu au chapitre précédent ( section 1.4.1 , page 78 ) qu' en toute généralité , un système linéaire est une équation de la forme : où u L ( E , E 0 ) et b E 0 sont ***fixés*** et x est l' inconnue ( E et E 0 sont des K - espaces vectoriels )

**27631**: Dans le cas de la dimension finie et une fois des bases ***fixées*** , ce système linéaire est équivalent à un système de n équations à n inconnues , que l' on peut écrire sous forme matricielle : où A est la matrice n p de u , X la matrice p 1 de x et B la matrice n 1 de b ( avec p dim E et Lorsque l' on a un système de n équations à p inconnues , la condition de compatibilité ( c' est - à - dire la condition pour que le système admette des solutions ) s' écrit ( voir la partie suivante sur les matrices - blocs ) : rang(A ) rang A B ) ce qui se vérifie facilement à l' aide d' une méthode de pivot sur les lignes , où l' on ne prend jamais le pivot sur la colonne constituée des éléments de B. La matrice A B Mn , p1 ( K ) s' appelle la matrice augmentée du système A X B. Un système de n équations à n inconnues : est dit de Cramer , lorsque A est inversible

**27723**: Dans le cas de la dimension finie et une fois des bases fixées , ce système linéaire est équivalent à un système de n équations à n inconnues , que l' on peut écrire sous forme matricielle : où A est la matrice n p de u , X la matrice p 1 de x et B la matrice n 1 de b ( avec p dim E et Lorsque l' on a un système de n équations à p inconnues , la condition de compatibilité ( c' est - à - dire la condition pour que le système admette des solutions ) s' ***écrit*** ( voir la partie suivante sur les matrices - blocs ) : rang(A ) rang A B ) ce qui se vérifie facilement à l' aide d' une méthode de pivot sur les lignes , où l' on ne prend jamais le pivot sur la colonne constituée des éléments de B. La matrice A B Mn , p1 ( K ) s' appelle la matrice augmentée du système A X B. Un système de n équations à n inconnues : est dit de Cramer , lorsque A est inversible

**27770**: Dans le cas de la dimension finie et une fois des bases fixées , ce système linéaire est équivalent à un système de n équations à n inconnues , que l' on peut écrire sous forme matricielle : où A est la matrice n p de u , X la matrice p 1 de x et B la matrice n 1 de b ( avec p dim E et Lorsque l' on a un système de n équations à p inconnues , la condition de compatibilité ( c' est - à - dire la condition pour que le système admette des solutions ) s' écrit ( voir la partie suivante sur les matrices - blocs ) : rang(A ) rang A B ) ce qui se vérifie facilement à l' aide d' une méthode de pivot sur les lignes , où l' on ne prend jamais le pivot sur la colonne ***constituée*** des éléments de B. La matrice A B Mn , p1 ( K ) s' appelle la matrice augmentée du système A X B. Un système de n équations à n inconnues : est dit de Cramer , lorsque A est inversible

**27789**: Dans le cas de la dimension finie et une fois des bases fixées , ce système linéaire est équivalent à un système de n équations à n inconnues , que l' on peut écrire sous forme matricielle : où A est la matrice n p de u , X la matrice p 1 de x et B la matrice n 1 de b ( avec p dim E et Lorsque l' on a un système de n équations à p inconnues , la condition de compatibilité ( c' est - à - dire la condition pour que le système admette des solutions ) s' écrit ( voir la partie suivante sur les matrices - blocs ) : rang(A ) rang A B ) ce qui se vérifie facilement à l' aide d' une méthode de pivot sur les lignes , où l' on ne prend jamais le pivot sur la colonne constituée des éléments de B. La matrice A B Mn , p1 ( K ) s' appelle la matrice ***augmentée*** du système A X B. Un système de n équations à n inconnues : est dit de Cramer , lorsque A est inversible

**27805**: Dans le cas de la dimension finie et une fois des bases fixées , ce système linéaire est équivalent à un système de n équations à n inconnues , que l' on peut écrire sous forme matricielle : où A est la matrice n p de u , X la matrice p 1 de x et B la matrice n 1 de b ( avec p dim E et Lorsque l' on a un système de n équations à p inconnues , la condition de compatibilité ( c' est - à - dire la condition pour que le système admette des solutions ) s' écrit ( voir la partie suivante sur les matrices - blocs ) : rang(A ) rang A B ) ce qui se vérifie facilement à l' aide d' une méthode de pivot sur les lignes , où l' on ne prend jamais le pivot sur la colonne constituée des éléments de B. La matrice A B Mn , p1 ( K ) s' appelle la matrice augmentée du système A X B. Un système de n équations à n inconnues : est ***dit*** de Cramer , lorsque A est inversible

**27828**: En ce cas , il y a existence et unicité de la solution , ***donnée*** par Pour résoudre un système de Cramer , on ne calcule jamais l' inverse de la matrice A , on utilise l' algorithme du pivot de Gauss ! Soit à résoudre le système : Appliquons l' algorithme du pivot de Gauss à la matrice augmentée A B : La deuxième ligne permet de discerner trois cas : 1

**27873**: En ce cas , il y a existence et unicité de la solution , donnée par Pour résoudre un système de Cramer , on ne calcule jamais l' inverse de la matrice A , on utilise l' algorithme du pivot de Gauss ! Soit à résoudre le système : Appliquons l' algorithme du pivot de Gauss à la matrice ***augmentée*** A B : La deuxième ligne permet de discerner trois cas : 1

**27960**: 6 1 , 6 , le système est de Cramer ( solution unique ) , le système réduit s' ***écrit*** : On voit que le cas 6 ne s' obtient pas par continuité du cas de Cramer

**27984**: Donc : l' ordinateur ne ***fait*** pas apparaître le cas ! ! ! Si on a vraiment besoin de calculer l' inverse d' une matrice A GLp ( K ) inversible , on peut appliquer l' algorithme du pivot de Gauss généralisée à la matrice augmentée A Ip , avec des opérations sur les lignes

**28024**: Donc : l' ordinateur ne fait pas apparaître le cas ! ! ! Si on a vraiment besoin de calculer l' inverse d' une matrice A GLp ( K ) inversible , on peut appliquer l' algorithme du pivot de Gauss généralisée à la matrice ***augmentée*** A Ip , avec des opérations sur les lignes

**28042**: On obtient à la fin une matrice ***augmentée*** de la forme En effet , si on note E la matrice correspondante aux opérations élémentaires sur les lignes lors de l' algorithme , on a E A Ip , d' où E Ip A1

**28080**: Autrement ***dit*** , en effectuant les mêmes opérations sur Ip , on obtient A1

**28373**: Attention à ne pas oublier que la multiplication entre matrices ne est pas commutative ! Démonstration Il s' agit de simples vérifications par calculs ( ***laissées*** en exercice )

**28413**: Proposition 2.7 Correspondance matrices - blocs et décomposition en somme directe Soit E et E 0 deux K - espaces vectoriels de dimensions finies , avec ( E1 , E2 ) une base de E ***adaptée*** à la somme directe E E1 E2 ( E10 , E20 ) une base de E 0 adaptée à la somme directe E 0 E10 E20

**28431**: Proposition 2.7 Correspondance matrices - blocs et décomposition en somme directe Soit E et E 0 deux K - espaces vectoriels de dimensions finies , avec ( E1 , E2 ) une base de E adaptée à la somme directe E E1 E2 ( E10 , E20 ) une base de E 0 ***adaptée*** à la somme directe E 0 E10 E20

**28908**: Pour toute A Mn ( K ) , la matrice de l' application : dans la base canonique ( judicieusement ***ordonnée*** ) est : 3

**28951**: En prenant le même ordre de la base canonique , pour toute A Mn ( K ) la matrice de de l' application : Proposition 2.8 Démonstration Il s' agit d' une simple vérification par calculs ( ***laissée*** en exercice )

**29332**: On a card Sp p ! Démonstration Pour construire une permutation Sp : On a p choix possibles pour ( 1 ) ( les p éléments de 1 , p ) Une fois ( 1 ) ***choisi*** , on a p 1 choix possibles pour ( 2 ) ( les p 1 éléments de 1 , p ( 1 ) ) Plus généralement , une fois ( 1 ) ,

**29373**: , ( i ) ***choisis*** , on a p i choix possibles pour ( i 1 ) ( les p i Une fois ( 1 ) ,

**29404**: , ( p 1 ) ***choisis*** , il ne reste plus qu' un seul choix pour ( p ) ( l' unique élément de Finalement , on a choix possibles pour construire , d' où le résultat

**29505**: Toute permutation 1 , p s' ***écrit*** à l' aide de transpositions , c' est - à - dire qu' il existe des transpositions de Sp , notée 1 ,

**29526**: Toute permutation 1 , p s' écrit à l' aide de transpositions , c' est - à - dire qu' il existe des transpositions de Sp , ***notée*** 1 ,

**29539**: , r , telles que Démonstration Déjà ***démontré*** lors de la démonstration de la proposition 2.15 , page 136 du chapitre 2

**29776**: Par exemple , si ( k , ) 1 , p2 , Les autres cas sont analogues ( ***laissés*** en exercice )

**29871**: Soit la permutation de 1 , 9 ***donnée*** par a : alors sa signature vaut : 1

**29986**: Bien retenir qu' il suffit de regarder ce qui se passe avec les transpositions ! Formes p - linéaires sur un espace vectoriel de dimension n Soit E un K - espace vectoriel de dimension ***finie*** , soit p N

**30187**: , fp sont des formes linéaires sur E , alors est une forme p - linéaire sur E. Soit E un K - espace vectoriel de dimension ***finie*** , soit p N , p 2 , et soit Lp ( E , K )

**30287**: Dans l' espace euclidien R3 , l' application produit mixte ***définie*** par : ( où h , i est le produit scalaire euclidien ) est 3-linéaire antisymétrique

**30394**: En particulier : est symétrique si , et seulement si , échanger deux vecteurs ne change pas le signe ( ci-dessous seules les i - èmes et j - ièmes variables sont ***explicitées*** ) : est antisymétrique si , et seulement si , échanger deux vecteurs change le signe ( ci-dessous seules les i - èmes et j - ièmes variables sont explicitées ) : Soit Lp ( E , K )

**30424**: En particulier : est symétrique si , et seulement si , échanger deux vecteurs ne change pas le signe ( ci-dessous seules les i - èmes et j - ièmes variables sont explicitées ) : est antisymétrique si , et seulement si , échanger deux vecteurs change le signe ( ci-dessous seules les i - èmes et j - ièmes variables sont ***explicitées*** ) : Soit Lp ( E , K )

**30497**: , xp ) E p et soit ( i , j ) 1 , p2 tel que i 6 j ( ci-dessous , seules les i - èmes et j - ièmes variables sont ***explicitées*** )

**30698**: , xp ) est ***liée*** , cela veut dire qu' il existe i 1 , p tel que xi soit une combinaison linéaire des Puisque est linéaire par rapport à sa i - ème variable : puisque xk apparaît deux fois dans (

**30775**: i - ème variable : Théorème 3.1 Dimension de l' espace des formes n - linéaires ***alternées*** Soit E un K - espace vectoriel de dimension finie avec n dim E 1

**30785**: i - ème variable : Théorème 3.1 Dimension de l' espace des formes n - linéaires alternées Soit E un K - espace vectoriel de dimension ***finie*** avec n dim E 1

**31150**: Déterminant d' une famille de vecteurs Soit E un K - espace vectoriel de dimension ***finie*** avec n dim E 1 , soit E ( e1 ,

**31214**: Considérons E K2 ***muni*** de la base canonique E ( e1 , e2 ) ainsi que deux vecteurs u ( a , b ) a.e1 b.e2 et v ( c , d ) c.e1 d.e2 de E. Le groupe symétrique S2 a 2 ! 2 éléments , l' identité id1,2 et la transposition Soit E un K - espace vectoriel de dimension finie avec n dim E 1 , soit E ( e1 ,

**31273**: Considérons E K2 muni de la base canonique E ( e1 , e2 ) ainsi que deux vecteurs u ( a , b ) a.e1 b.e2 et v ( c , d ) c.e1 d.e2 de E. Le groupe symétrique S2 a 2 ! 2 éléments , l' identité id1,2 et la transposition Soit E un K - espace vectoriel de dimension ***finie*** avec n dim E 1 , soit E ( e1 ,

**31371**: , p , j 6 i. Démonstration Les trois propriétés ont déjà été ***démontrées*** lors de la démonstration du théorème 3.1 , page 157 et les deux dernières sont exactement la propriété 3.6 , page 156

**31404**: Soit E un K - espace vectoriel de dimension ***finie*** avec n dim E 1 et soit E ( e1 ,

**31495**: Supposons que C soit une base de E. D' après le résultat précédent , nous avons Par contraposition , si la famille C ne est pas une base de E alors elle est ***liée*** donc detE ( c1 ,

**31546**: Déterminant d' une matrice carrée Soit a A ai , j ( i , j)1,n2 Mn ( K ) , on appelle déterminant de A et on note Autrement ***dit*** , c' est le déterminant de la famille des n colonnes de A dans la base canonique de Mn,1 ( K )

**31985**: Le déterminant de u , ***noté*** det u , est défini par det u det MatE ( u ) où E est une base quelconque de E Cette définition a bien un sens : les matrices de u dans deux bases de E différentes sont semblables , donc elles ont même déterminant ( remarque 3.9 , page précédente )

**31990**: Le déterminant de u , noté det u , est ***défini*** par det u det MatE ( u ) où E est une base quelconque de E Cette définition a bien un sens : les matrices de u dans deux bases de E différentes sont semblables , donc elles ont même déterminant ( remarque 3.9 , page précédente )

**32054**: det idE det In 1 Soit E un K - espace vectoriel de dimension ***finie*** , soit E une base de E et soit u L ( E )

**32269**: Ainsi , pour calculer un déterminant d' une matrice carrée d' ordre 5 , il faut effectuer Pire encore , 60 ! ' 1082 est supérieur au nombre d' atomes observables dans l' univers , alors que les problèmes de mathématiques ***appliquées*** et d' ingénierie moderne nécessitent de traiter des matrices qui ont des centaines de milliers voire des millions de lignes ... Il faut donc trouver des méthodes plus efficaces

**32768**: a. Exercice ***conseillé***

**33035**: On peut démontrer que le calcul du déterminant d' une matrice carrée de taille n par la méthode du pivot de Gauss nécessite un nombre d' opération de l' ordre de n3 , ce qui est bien meilleur que n!. Soit à calculer ( n 2 ) : En faisant les transvections successives ( qui conservent le déterminant ) : On fait apparaître deux lignes identiques , donc : Si a , b , c , d , e , f , g , h et i sont dans K , alors e f aei dhc gbf gec ahf dbi que l' on peut mémoriser par la règle de Sarrus : On remarque , qu' avant développement , Wxmaxima nous proposait une formule non ***développée*** , qu' on peut ré-écrire : Que se passe -t -il pour n plus grand ? On reconnaît l' expression : Soit A Mn ( K ) , soit ( k , ) 1 , n , on note a Ak , la matrice de Mn1 ( K ) obtenue à partir de A en supprimant sa k - ième ligne et sa -ième colonne

**33085**: On peut démontrer que le calcul du déterminant d' une matrice carrée de taille n par la méthode du pivot de Gauss nécessite un nombre d' opération de l' ordre de n3 , ce qui est bien meilleur que n!. Soit à calculer ( n 2 ) : En faisant les transvections successives ( qui conservent le déterminant ) : On fait apparaître deux lignes identiques , donc : Si a , b , c , d , e , f , g , h et i sont dans K , alors e f aei dhc gbf gec ahf dbi que l' on peut mémoriser par la règle de Sarrus : On remarque , qu' avant développement , Wxmaxima nous proposait une formule non développée , qu' on peut ré-écrire : Que se passe -t -il pour n plus grand ? On reconnaît l' expression : Soit A Mn ( K ) , soit ( k , ) 1 , n , on note a Ak , la matrice de Mn1 ( K ) ***obtenue*** à partir de A en supprimant sa k - ième ligne et sa -ième colonne

**33163**: on appelle comatrice de A la matrice de Mn ( K ) ***définie*** par : Cofacteur1,1 ( A ) Com(A ) Cofacteuri , j ( A)(i , j)1,n2 Cofacteurn,1 ( A ) a. Attention , cette notation peut également désigner le coefficient en ( k , ) de A. Cofacteurn,1 ( A ) Cofacteurn , n ( A ) ACom(A ) ) A Pour retrouver les signes dans la comatrice , il suffit d' alterner les signes lorsque qu' on se déplace de case en case dans la comatrice : Proposition 3.1 Développement selon une ligne ou une colonne 1

**33700**: On peut obtenir son terme général en utilisant le fait que 1 a et 2 a2 b c. On considère la fonction : C' est une fonction polynomiale en x. En effectuant les opérations élémentaires k 2 , n , Ck Ck C1 , puis en développant suivant la première colonne , on en ***déduit*** que cette fonction polynomiale est de degré inférieur ou égale à 1

**33716**: Elle s' ***écrit*** donc sous la forme : x

**33740**: Pour x a , le déterminant vaut Pour x b , le déterminant vaut On en ***déduit*** alors , puis la valeur du déterminant en évaluant en x 0

**33889**: Théorème 3.2 Propriété de la comatrice Soit A Mn ( K ) , alors : En particulier , si A est inversible : Démonstration Pour i j , le coefficient en ( i , i ) de A t Com(A ) vaut , en utilisant la formule du développement suivant la i - ième ligne : ai , k Cofacteuri , k ( A ) det A Pour i 6 j , posons B la matrice ***obtenue*** à partir de A en remplaçant la j - ième ligne par la i - ème ligne de A. Puisque B a deux lignes égales , det B 0

**34061**: Cette formule est inutile pour calculer un inverse ! En effet , elle mène à des calculs trop Pour calculer l' inverse d' une matrice A , on peut par exemple utiliser la méthode du pivot de Gauss sur la matrice ***augmentée*** A In ( voir la partie sur les systèmes linéaires du chapitre précédent ) , ou résoudre , toujours avec la méthode du pivot de Gauss , un système générique A X Y , où X et Y sont dans Mn,1 ( K ) , ce qui donne X A1 Y

**34173**: Si on note A1 i , j ( i , j)1,n2 , alors l' application : est de classe C ( en particulier continue ) , donc une petite variation sur les coefficients de A provoque une petite variation sur les coefficients de A1 : on peut donc faire un calcul ***approché*** de A1 ! Faisons quelques calculs en Wxmaxima : Dans Rn , nous avons une base de référence : la base canonique Cn

**34271**: On peut alors interpréter le déterminant de la manière suivante : v deux vecteurs de R2 , alors : où ( ) désigne l' aire de , le parallélogramme ***construit*** sur v

**34284**: On peut alors donner un sens à une aire ***orientée*** en considérant le déterminant sans la valeur absolue

**34312**: w trois vecteurs de R3 , alors : où ( ) désigne le volume de , le parallélépipède ***construit*** sur On avait trouvé une autre formule ( qui est la même ) car : Plus généralement , si E est un R - espace vectoriel de dimension finie et si E et E 0 deux bases de E , on dit que E 0 a la même orientation que E si detE ( E 0 ) 0

**34461**: On peut alors décréter arbitrairement qu' une base E est une base directe , on dit alors que E est ***orienté***

**34473**: Toutes les bases ayant la même orientation que E sont ***dites*** directes et les autres indirectes

**34486**: Dans Rn , il est bien ***entendu*** naturel de décréter que la base canonique est directe ( ce qui est cohérent avec la remarque précédente )

**34547**: Retour sur les systèmes linéaires On rappelle qu' un système linéaire de n équations à n inconnues de la forme d' inconnue X Mn,1 ( K ) avec A Mn ( K ) et B Mn,1 ( K ) est ***dit*** de Cramer lorsqu' il y a existence et unicité de la solution

**34562**: Autrement ***dit*** , c' est un système de Cramer si , et seulement si , A est inversible si , et seulement si , Proposition 3.3 Soit A GLn ( K ) de colonnes A1 ,

**34618**: Alors l' unique solution du système de Cramer est ***donnée*** par Démonstration donc pour tout k 1 , n , Lorsque k 6 j , le déterminant ci - dessus est nul puisque deux colonnes sont égales , donc : ce qui donne le résultat puisque det A 6 0

**34744**: Elle a cependant un intérêt théorique , elle permet d' obtenir que la solution X dépend de manière continue ( et même C ) des coefficients de A et B : une petite variation sur les coefficients de A ou de B provoque une petite variation sur les coefficients de X : on peut donc faire un calcul ***approché*** de X ! Démontrer que A est inversible et que : 3.1.2 Soit V un K - espace vectoriel de dimension finie et soit p1 N , p2 N , p2 n. On considère l' ensemble des formes ( p1 p2 ) -linéaires vérifiant pour tout ( x1 ,

**35017**: ( b ) Démontrer que A possède une décomposition LU si , et seulement si , tous ses mineurs principaux m1 , ... mn sont non nuls , ***définis*** par : 3.1.9 ( a ) Démontrer que si A , B , C et D sont dans Mn ( R ) et que A C C A , alors : ( b ) Démontrer que c' est faux , en général , lorsqu' il ne y a plus commutation

**35150**: Résoudre les équations suivantes d' inconnues X Mn ( R ) : 3.1.14 À quelle(s ) condition(s ) , connaissant les affixes des milieux des côtés d' un polygone ***fermé*** à n côtés , existe -t -il un tel polygone ? Préciser dans tous les cas le procédé de construction du ou des polygone(s ) solution(s )

**35186**: Que signifie géométriquement la condition de compatibilité ***obtenue*** ? 3.1.15 Déterminer les ensembles de quatre points du plan tels que la somme des distances d' un point au trois autres est constante

**35266**: On dit que K est une valeur propre de u si il existe x E 0E tel que On dit alors que x est un vecteur propre de u ***associé*** à

**35303**: Si Sp(u ) , on appelle espace propre de u ***associé*** à le sous-espace vectoriel de E noté Eu ( ) et défini par Par définition , un vecteur propre ne est jamais nul

**35310**: Si Sp(u ) , on appelle espace propre de u associé à le sous-espace vectoriel de E ***noté*** Eu ( ) et défini par Par définition , un vecteur propre ne est jamais nul

**35315**: Si Sp(u ) , on appelle espace propre de u associé à le sous-espace vectoriel de E noté Eu ( ) et ***défini*** par Par définition , un vecteur propre ne est jamais nul

**35396**: De même pour une matrice A Mn ( K ) , on appelle valeur propre et vecteur propre de A tout couple Le spectre Sp(A ) de A est l' ensemble des valeurs propres de A et pour Sp(A ) , EA ( ) Ker(A .In ) est l' espace propre ***associé***

**35506**: L' espace propre ***associé*** est de dimension 1

**35538**: Il ne y a pas d' autre valeur propre que celles ***trouvées***

**35677**: Par exemple , si n 2 et si a et b sont dans K , a 6 b , la matrice : possède les valeurs propres a ( n 1 ) b et a b ( on verra qu' il ne y en a pas d' autre ) et les espaces propres associés sont : Si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à 2 n et si k R , alors l' endomorphisme u L ( E ) ***défini*** par : Il faut parfois faire preuve de vision géométrique : si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à n et si u L ( E ) est défini par : alors alors les valeurs propres de u sont 1 et 1 et les vecteurs propres se calculent facilement dans une base adaptée

**35715**: Par exemple , si n 2 et si a et b sont dans K , a 6 b , la matrice : possède les valeurs propres a ( n 1 ) b et a b ( on verra qu' il ne y en a pas d' autre ) et les espaces propres associés sont : Si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à 2 n et si k R , alors l' endomorphisme u L ( E ) défini par : Il faut parfois faire preuve de vision géométrique : si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à n et si u L ( E ) est ***défini*** par : alors alors les valeurs propres de u sont 1 et 1 et les vecteurs propres se calculent facilement dans une base adaptée

**35739**: Par exemple , si n 2 et si a et b sont dans K , a 6 b , la matrice : possède les valeurs propres a ( n 1 ) b et a b ( on verra qu' il ne y en a pas d' autre ) et les espaces propres associés sont : Si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à 2 n et si k R , alors l' endomorphisme u L ( E ) défini par : Il faut parfois faire preuve de vision géométrique : si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à n et si u L ( E ) est défini par : alors alors les valeurs propres de u sont 1 et 1 et les vecteurs propres se calculent facilement dans une base ***adaptée***

**35752**: Polynôme caractéristique Soit E un K - espace vectoriel de dimension ***finie*** , soit u L ( E ) et soit K. Les propositions suivantes sont équivalentes : 3

**35875**: Soit E un K - espace vectoriel de dimension ***finie*** non nulle et soit u L ( E )

**35892**: Le polynôme caractéristique de u , ***noté*** u , correspond à la fonction polynomiale : Afin d' alléger les notations et les calculs , on note le polynôme caractéristique u sous la forme d' un polynôme formel ( voir le cours sur les polynômes ) : au lieu d' une fonction polynomiale , où X est l' indéterminée , une variable ayant les mêmes règles de calcul que celles d' une variable dans K. Ainsi , une fonction polynomiale sur K de la forme se représente par Autrement dit , X correspond à la fonction polynomiale x 7 x. On note KX le K - espace vectoriel des polynômes ( formels ) à coefficients dans K et Kn X le sousespace vectoriel de KX des polynômes de degré inférieur ou égal à n. On définit de la même manière le polynôme caractéristique A det(A X.In ) d' une matrice carrée A Mn ( K )

**35974**: Le polynôme caractéristique de u , noté u , correspond à la fonction polynomiale : Afin d' alléger les notations et les calculs , on note le polynôme caractéristique u sous la forme d' un polynôme formel ( voir le cours sur les polynômes ) : au lieu d' une fonction polynomiale , où X est l' indéterminée , une variable ayant les mêmes règles de calcul que celles d' une variable dans K. Ainsi , une fonction polynomiale sur K de la forme se représente par Autrement ***dit*** , X correspond à la fonction polynomiale x 7 x. On note KX le K - espace vectoriel des polynômes ( formels ) à coefficients dans K et Kn X le sousespace vectoriel de KX des polynômes de degré inférieur ou égal à n. On définit de la même manière le polynôme caractéristique A det(A X.In ) d' une matrice carrée A Mn ( K )

**36081**: Soit E un K - espace vectoriel de dimension ***finie*** n 1 et soit u L ( E )

**36180**: Posons : ai , j si i 6 j trace Atrace u puisque ( k),k a(k),k et ( ) , a ( ) , ne dépendent pas de X. On en ***déduit*** le résultat , en remarquant que le terme constant est u ( 0 ) det(u 0

**36211**: idE ) det u. Soit E un K - espace vectoriel de dimension ***finie*** non nulle , soit u L ( E ) et soit K. Alors est une valeur propre de u si , et seulement si , u ( ) 0

**36243**: Autrement ***dit*** : Démonstration Immédiat en utilisant la propriété 4.1 , page précédente et en remarquant que u

**36447**: Ainsi : i1 Ei , i1 Mk ( K ) sont telles que toutes les Ak ( k 1 , n ) ont même polynôme caractéristique alors que les espaces propres associés ont des dimensions allant de 1 à n. Soit E un K - espace vectoriel de dimension ***finie*** non nulle , soit u L ( E ) et soit Sp(u )

**36539**: Soit E un K - espace vectoriel de dimension ***finie*** non nulle et soit u L ( E )

**36586**: Par définition , il existe un vecteur propre ***associé*** à ( donc non nul ) , d' où Eu ( ) 6 0E et Posons d dim Eu ( )

**36684**: Diagonalisation Proposition 4.1 Soit E un K - espace vectoriel de dimension ***finie*** , soit u L ( E ) et soit 1 ,

**36913**: Si E est de dimension finie , u L ( E ) est diagonalisable , alors dans toute base E de E ***formé*** de vecteurs propres de u , la matrice de u est diagonale et ses éléments sont les valeurs propres 1 ,

**36977**: , k qui apparaissent avec leur multiplicité : multu ( 1 ) fois multu ( 2 ) fois On dit qu' une matrice A Mn ( K ) est diagonalisable si , et seulement si , son endomorphisme canoniquement ***associé*** l' est

**36982**: Autrement ***dit*** , une matrice est diagonalisable si elle est semblable à une matrice diagonale

**37081**: Soit E un K - espace vectoriel de dimension ***finie*** non nulle et soit u L ( E )

**37216**: On en ***déduit*** que E est une base de E formée de vecteurs propres de u , donc u est diagonalisable

**37224**: On en déduit que E est une base de E ***formée*** de vecteurs propres de u , donc u est diagonalisable

**37251**: Théorème 4.1 Caractérisation des endomorphismes diagonalisables Soit E un K - espace vectoriel de dimension ***finie*** non nulle et soit u L ( E )

**37274**: Alors u est diagonalisable si , et seulement si , u est ***scindé*** et pour tout Sp(u ) , dim Eu ( ) multu ( ) Par définition , un polynôme P KX non constant est scindé si il est de la forme avec a K et , pour tout i 1 , k , i K Les i ne sont pas nécessairement tous différents

**37298**: Alors u est diagonalisable si , et seulement si , u est scindé et pour tout Sp(u ) , dim Eu ( ) multu ( ) Par définition , un polynôme P KX non constant est ***scindé*** si il est de la forme avec a K et , pour tout i 1 , k , i K Les i ne sont pas nécessairement tous différents

**37341**: Dans une base E de E ***adaptée*** à la décomposition la matrice A MatE ( u ) de u dans la base E est diagonale : On a donc ( n dim E ) ce qui démontre que u est scindé

**37375**: Dans une base E de E adaptée à la décomposition la matrice A MatE ( u ) de u dans la base E est diagonale : On a donc ( n dim E ) ce qui démontre que u est ***scindé***

**37430**: De plus , il est de degré n ( propriété 4.2 , page 194 ) donc multu ( i ) Mais pour tout i 1 , k , 1 di multu ( i ) ( propriété 4.4 , page 195 ) , ce qui implique que , pour tout Supposons que u soit ***scindé*** et que , pour tout i 1 , k , di multu ( i )

**37584**: On retrouve le fait qu' il est indispensable de préciser le corps dans lequel on se place quand on s' intéresse à la diagonalisation d' une matrice : sur C tous les polynômes non constants sont ***scindés*** , ce qui est faux sur R. La matrice de l' exemple 4.1 , page 184 ne est pas diagonalisable ( sur R ou sur C )

**37727**: 4.1.2 Diagonaliser ( c' est - à - dire démontrer qu' elle est diagonalisable et l' écrire sous la forme P D P 1 avec P GL3 ( R ) et D M3 ( R ) diagonale ) la matrice suivante : 4.1.3 Démontrer que les matrices suivantes sont semblables : 4.1.4 Diagonaliser l' endomorphisme de Kn X ***défini*** par ( pour n N ) : 4.1.5 Soit A Mn ( K ) diagonalisable et B Mp ( K ) diagonalisable , démontrer que A B est diagonalisable et préciser les éléments propres de A B en fonction de ceux de A et de B. 4.1.6 Trouver une condition nécessaire et suffisante sur A Mn ( C ) pour que soit diagonalisable 4.1.7 Démontrer que toute matrice circulante , c' est - à - dire de la forme : est diagonalisable dans Mn ( C ) et donner ses éléments propres

**37894**: À quelle condition nécessaire et suffisante la matrice M suivante est-elle diagonalisable ? 4.1.11 Trouver une condition nécessaire et suffisante pour que la matrice suivante soit diagonalisable : 4.1.12 Soit E un K - espace vectoriel de dimension ***finie*** non nulle et u L ( E )

**37944**: Démontrer l' équivalence de : ( a ) u diagonalisable ( b ) tout sous-espace de E stable par u admet un supplémentaire stable par u. 4.1.13 Trouver les valeurs propres et vecteurs propres de l' endomorphisme de Rn X ***défini*** par : Trigonalisation Soit E un K - espace vectoriel de dimension finie n 1 et soit u L ( E )

**37957**: Démontrer l' équivalence de : ( a ) u diagonalisable ( b ) tout sous-espace de E stable par u admet un supplémentaire stable par u. 4.1.13 Trouver les valeurs propres et vecteurs propres de l' endomorphisme de Rn X défini par : Trigonalisation Soit E un K - espace vectoriel de dimension ***finie*** n 1 et soit u L ( E )

**37973**: On dit que u est ***dit*** trigonalisable si il existe un drapeau stable pour u , c' est - à - dire des sous-espaces vectoriels V1 ,

**38061**: , en ) de E ***adaptée*** au drapeau , c' est - à - dire telle La diagonale de cette matrice est alors constituée des valeurs propres de u ( avec leurs multiplicités )

**38079**: , en ) de E adaptée au drapeau , c' est - à - dire telle La diagonale de cette matrice est alors ***constituée*** des valeurs propres de u ( avec leurs multiplicités )

**38112**: On dit qu' une matrice A Mn ( K ) est trigonalisable si , et seulement si , son endomorphisme canoniquement ***associé*** l' est

**38117**: Autrement ***dit*** , une matrice est trigonalisable si elle est semblable à une matrice triangulaire supérieure

**38168**: Théorème 4.2 Caractérisation des endomorphismes trigonalisables Soit E un K - espace vectoriel de dimension ***finie*** non nulle et soit u L ( E )

**38191**: Alors u est trigonalisable si , et seulement si , u est ***scindé*** Démonstration Supposons que u soit trigonalisable

**38241**: On a alors : donc u est ***scindé***

**38290**: Supposons le résultat vrai au rang n. Soit u L ( E ) avec dim E n 1 tel que u soit ***scindé***

**38321**: En particulier , il admet au moins une racine donc il existe un vecteur propre e1 de u : u(e1 ) 1 .e1 avec 1 K la valeur propre ***associée*** à e1

**38339**: Comme e1 6 0E , on peut compléter ( e1 ) Puisque u est ***scindé*** , nécessairement N est aussi scindé

**38345**: Comme e1 6 0E , on peut compléter ( e1 ) Puisque u est scindé , nécessairement N est aussi ***scindé***

**38390**: , en1 ) qui est de dimension n , H la projection de H parallèlement à Vect(e1 ) et H u H L ( H ) qui a pour polynôme caractéristique N qui est ***scindé***

**38493**: En particulier , tous les endomorphismes d' un C - espace vectoriel de dimension ***finie*** non nulle sont trigonalisables ( car les polynômes non constants sont scindés sur C )

**38505**: En particulier , tous les endomorphismes d' un C - espace vectoriel de dimension finie non nulle sont trigonalisables ( car les polynômes non constants sont ***scindés*** sur C )

**38585**: On a vu que le vecteur e1 c1 c2 c3 est vecteur propre de u ***associé*** à la valeur propre 2

**38748**: Ce vecteur est ***exprimé*** dans la base ( e2 , e3 ) , c' est donc : où b1 est choisi pour conserver le vecteur propre de A et b3 est choisi pour compléter b2 en une base 4

**38765**: Ce vecteur est exprimé dans la base ( e2 , e3 ) , c' est donc : où b1 est ***choisi*** pour conserver le vecteur propre de A et b3 est choisi pour compléter b2 en une base 4

**38776**: Ce vecteur est exprimé dans la base ( e2 , e3 ) , c' est donc : où b1 est choisi pour conserver le vecteur propre de A et b3 est ***choisi*** pour compléter b2 en une base 4

**38804**: On a alors : Finalement : Vérifions en Wxmaxima : Soit E un K - espace vectoriel de dimension ***finie*** non nulle , soit u E et soit F 6 0E un sous-espace vectoriel de E stable par u ( c' est - à - dire u(F ) F )

**38911**: diagonalisable , il existe une base de vecteurs propres de E donc on peut écrire : avec , pour tout i 1 , k , xi Eu ( i ) On a donc : Autrement ***dit*** , on a un système linéaire d' inconnues x1 ,

**38967**: On a alors On a donc un système linéaire dont le déterminant de la matrice ***associée*** est un déterminant de Vandermonde ( proposition 3.2 , page 173 )

**39066**: On a donc démontré que tout élément de F s' ***écrit*** de manière unique comme une somme de vecteurs propres de u qui sont dans F , d' où : d' après le point 1

**39094**: On en ***déduit*** que v u F est diagonalisable

**39151**: Supposons F 6 E. Soit F une base de F que l' on complète en une base E de E. On a alors ( n dim E et p dim F ) : On en ***déduit*** que u v D

**39160**: Or u est ***scindé*** car u est trigonalisable donc nécessairement v est aussi scindé , ce qui démontre que v est trigonalisable

**39170**: Or u est scindé car u est trigonalisable donc nécessairement v est aussi ***scindé*** , ce qui démontre que v est trigonalisable

**39410**: Théorème 4.3 Critère de co-diagonalisation Soit E un K - espace vectoriel de dimension ***finie*** non nulle et soit u et v deux endomorphismes de E qui sont diagonalisables

**39565**: D' après la propriété 4.7 , de la présente page , Eu ( ) est stable par v. Comme v est diagonalisable , on en ***déduit*** que v Eu ( ) est diagonalisable ( propriété 4.6 , page 203 )

**39699**: Théorème 4.4 Critère de co-trigonalisation Soit E un K - espace vectoriel de dimension ***finie*** non nulle et soit u et v deux endomorphismes de E qui sont trigonalisables

**39914**: De plus , u ( X ) M et u est ***scindé*** ( car u est trigonalisable ) donc nécessairement M est scindé donc M est trigonalisable

**39925**: De plus , u ( X ) M et u est scindé ( car u est trigonalisable ) donc nécessairement M est ***scindé*** donc M est trigonalisable

**40136**: 4.2.2 Soit E est C - espace vectoriel de dimension ***finie*** non nulle et u et v sont deux endomorphismes de E qui vérifient : Démontrer que u et v possèdent au moins un vecteur propre commun

**40174**: 4.2.3 Soit E est C - espace vectoriel de dimension ***finie*** non nulle et u , v et w trois endomorphismes de E qui vérifient : Démontrer que u et v possèdent au moins rang(w ) valeurs propres communes ( en comptant les multiplicités )

**40220**: 4.2.4 Soit E un C - espace vectoriel de dimension ***finie*** non nulle et soit u et v deux endomorphismes de E tels que u v 0L ( E )

**40303**: Le système : s' appelle le système homogène ***associé***

**40317**: La résolution du système homogène ***associé*** : 2

**40342**: La recherche d' une solution particulière , soit évidente , soit par variation de la constante a : Nous sommes donc ***ramenés*** au calcul de An

**40483**: , p ) P 1 , Par exemple si : alors A est diagonalisable et on trouve : Dans le cas où A est trigonalisable ( ce qu' on peut toujours supposer en se plaçant dans C ) , on se contente de trigonaliser la matrice et on résout le système triangulaire ***associé*** : ce qui est facile car il est triangulaire

**40576**: La recherche des racines de l' équation caractéristique correspond au calcul du polynôme caractéristique de la matrice du système ***associé***

**40719**: La résolution du système homogène ***associé*** : 2

**40824**: , p ) , alors En posant Y ( t ) P 1 X(t ) , on est ***ramené*** au système différentiel : qui est un système diagonal

**40884**: Par exemple , en reprenant le système de la remarque précédente ( ***obtenu*** à partir de 00 ( t ) ( t ) 0 ) , on a Il ne est pas difficile de démontrer que A est diagonalisable sur C , ses valeurs propres sont i et En posant Y P 1 X , on a le système donc il existe ( c1 , c2 ) C2 tel que : Si on ajoute la condition initiale ( qui correspond à ( 0 ) 1 et 0 ( 0 ) 0 ) , on trouve c1 c2 21 donc en particulier : exp(i t ) exp(i t ) On retrouve donc la solution bien connue de 00 ( t ) ( t ) 0 , ( 0 ) 1 et 0 ( 0 ) 0

**40986**: Par exemple , en reprenant le système de la remarque précédente ( obtenu à partir de 00 ( t ) ( t ) 0 ) , on a Il ne est pas difficile de démontrer que A est diagonalisable sur C , ses valeurs propres sont i et En posant Y P 1 X , on a le système donc il existe ( c1 , c2 ) C2 tel que : Si on ajoute la condition initiale ( qui correspond à ( 0 ) 1 et 0 ( 0 ) 0 ) , on trouve c1 c2 21 donc en particulier : exp(i t ) exp(i t ) On retrouve donc la solution bien ***connue*** de 00 ( t ) ( t ) 0 , ( 0 ) 1 et 0 ( 0 ) 0

**41045**: On est donc ***ramené*** ( par la même démarche qu' à l' exemple précédent en posant Y ( t ) P 1 X(t ) ) à un système de la forme où T est une matrice triangulaire supérieure que l' on peut résoudre facilement et on revient à X grâce à la relation X(t ) P Y ( t )

**41158**: 4.4.5 Résoudre le système différentiel 4.4.6 Déterminer les a R tels que le système admette au moins une solution non nulle ***bornée*** au voisinage de

**41201**: 4.4.7 Quelle est la nature des courbes intégrales de X 0 A X avec : Espaces stables La recherche des espaces stables par un endomorphisme u L ( E ) d' un K - espace vectoriel de dimension ***finie*** en dimension finie repose sur quelques résultats : 1

**41204**: 4.4.7 Quelle est la nature des courbes intégrales de X 0 A X avec : Espaces stables La recherche des espaces stables par un endomorphisme u L ( E ) d' un K - espace vectoriel de dimension finie en dimension ***finie*** repose sur quelques résultats : 1

**41245**: En particulier , E1 a une base ***constituée*** de vecteurs propres de u. 2

**41268**: Si u est trigonalisable , alors il admet des espaces stables de toute dimension ( ***donné*** par le drapeau ) et de plus , si E1 est un sous-espace vectoriel stable par u , alors : 3

**41418**: ide ) H. I Ces résultats suffisent en général lorsque l' on est en dimension 3 , car il ne y a que les droites ***dirigées*** par un vecteur propre et les hyperplans qui peuvent être stables ( avec en plus , bien évidemment , 0E Soit la matrice a : Le polynôme caractéristique est bien X 3

**41530**: 4.5.2 Soit E Rn , Sn , on note T l' application de E dans E ***définie*** par Démontrer que T est dans GL ( E )

**41574**: Trouver les sous-espaces vectoriels F de E tels que , pour tout 4.5.3 Trouver tous les sous-espaces stables de la matrice : 4.5.4 Soit E un K - espace vectoriel de dimension ***finie*** n 1 et f L ( E ) telle que f n 0L ( E ) et f n1 6 0L ( E )

**41628**: Démontrer que les sous espaces stables par f sont les Ker(f k ) pour k 0 , n. 4.5.5 Soit E un C - espace vectoriel de dimension ***finie*** n 1 et u L(E )

**41648**: ( a ) Démontrer que si u ne admet qu' un nombre ***fini*** de sous-espaces vectoriels stables alors chaque sous-espace propre est de dimension 1

**41703**: 4.5.6 Soit E un K - espace vectoriel de dimension ***finie*** non nulle et u L ( E )

**41731**: Démontrer que u est diagonalisable si , et seulement si , le polynôme caractéristique u de u est ***scindé*** et que tout sous-espace vectoriel de E stable par u admet un supplémentaire stable

**41979**: On dit que P est un polynôme annulateur Soit E un K - espace vectoriel de dimension ***finie*** , soit u L ( E ) et soit P KX un polynôme annulateur de u. Alors : Autrement dit , les valeurs propres de sont racines de tout polynôme annulateur de P

**41999**: On dit que P est un polynôme annulateur Soit E un K - espace vectoriel de dimension finie , soit u L ( E ) et soit P KX un polynôme annulateur de u. Alors : Autrement ***dit*** , les valeurs propres de sont racines de tout polynôme annulateur de P

**42024**: Démonstration Soit Sp(u ) et soit x un vecteur propre ***associé*** : u(x ) .x

**42135**: Ainsi , connaître les racines d' un polynôme annulateur de u ne donne que des candidats potentiels pour les valeurs propres de u. Le lemme des noyaux Théorème 5.1 ***dit*** lemme des noyaux Soit E un K - espace vectoriel et u L ( E )

**42223**: On en ***déduit*** Soit x Ker ( P Q)(u )

**42264**: De même , x2 Ker P ( u ) donc On en ***déduit*** le résultat

**42317**: , Pk des polynômes premiers deux à deux , alors : Ker Pi ( u ) Théorème 5.2 Caractérisation des endomorphismes diagonalisables Soit E un K - espace vectoriel de dimension ***finie*** non nulle et u L ( E )

**42347**: il existe un polynôme annulateur de u ***scindé*** à racines simples a

**42410**: Soit Le polynôme P est ***scindé*** à racines simples

**42468**: On a donc P ( u ) 0L ( E ) , c' est - à - dire que P est un polynôme annulateur de u ***scindé*** à racines simples

**42483**: Supposons qu' il existe un polynôme P annulateur de u ***scindé*** à racines simples

**42582**: Si F 6 0E est un sous-espace vectoriel de E stable par u et si u est diagonalisable , alors u F est diagonalisable , car tout polynôme ***scindé*** à racines simples qui est annulateur de u est aussi annulateur de u F

**42697**: La polynôme X p 1 est annulateur de A et est ***scindé*** , à racines simples dans Mn ( C )

**42743**: Réciproquement , si A vérifie A2 In , alors le polynôme X 2 1 est annulateur de A , ***scindé*** , à racines simples ( dans R ou C ) , donc A est semblable à une matrice diagonale avec des 1 sur la diagonale

**42797**: Si M est diagonalisable , il existe un polynôme P non nul , ***scindé*** , à racines simples qui annule M

**42868**: L' idéal annulateur de u est ***défini*** par : Soit E un K - espace vectoriel et soit u L ( E )

**42903**: si E est de dimension ***finie*** non nulle , Iu 6 0KX

**42962**: c' est - à - dire que Soit E un K - espace vectoriel de dimension ***finie*** non nulle et soit u L ( E )

**42991**: Le polynôme minimal de u est l' unique polynôme unitaire de degré 1 u KX tel que Autrement ***dit*** , si P KX est annulateur de u , alors il existe Q KX tel que P u Q. L' existence et l' unicité du polynôme minimal sont assurées par le fait que KX est principal et que Iu est un idéal de KX non réduit à 0KX ( voir le cours sur les anneaux )

**43020**: Le polynôme minimal de u est l' unique polynôme unitaire de degré 1 u KX tel que Autrement dit , si P KX est annulateur de u , alors il existe Q KX tel que P u Q. L' existence et l' unicité du polynôme minimal sont ***assurées*** par le fait que KX est principal et que Iu est un idéal de KX non réduit à 0KX ( voir le cours sur les anneaux )

**43120**: On a nécessairement R 0KX ( sinon cela ***contredit*** la définition de d ) donc P Q u , autrement dit Iu u KX ( l' autre inclusion est immédiate puisque u Iu et que Iu est un idéal de KX )

**43132**: On a nécessairement R 0KX ( sinon cela contredit la définition de d ) donc P Q u , autrement ***dit*** Iu u KX ( l' autre inclusion est immédiate puisque u Iu et que Iu est un idéal de KX )

**43194**: Si E est une base de E ( en dimension ***finie*** ) et si A est la matrice de u dans la base E , alors : Soit E un K - espace vectoriel de dimension finie non nulle et soit u L ( E )

**43220**: Si E est une base de E ( en dimension finie ) et si A est la matrice de u dans la base E , alors : Soit E un K - espace vectoriel de dimension ***finie*** non nulle et soit u L ( E )

**43334**: On en ***déduit*** que u

**43365**: Proposition 5.1 Soit E un K - espace vectoriel de dimension ***finie*** non nulle et u L ( E )

**43388**: Alors : u est diagonalisable si , et seulement si , u est ***scindé*** à racines simples Démonstration Si u est scindé à racines simples , comme u est annulateur de u , le théorème 5.2 , page 229 démontre que u est diagonalisable

**43396**: Alors : u est diagonalisable si , et seulement si , u est scindé à racines simples Démonstration Si u est ***scindé*** à racines simples , comme u est annulateur de u , le théorème 5.2 , page 229 démontre que u est diagonalisable

**43440**: Alors : est ***scindé*** à racines simples et est annulateur de u car il existe une base une base E de E constituée de vecteurs propres de u donc pour tout e E , P ( u)(e ) 0E ( car e est associé à une valeur propre i ) d' où P ( u ) 0L ( E )

**43459**: Alors : est scindé à racines simples et est annulateur de u car il existe une base une base E de E ***constituée*** de vecteurs propres de u donc pour tout e E , P ( u)(e ) 0E ( car e est associé à une valeur propre i ) d' où P ( u ) 0L ( E )

**43480**: Alors : est scindé à racines simples et est annulateur de u car il existe une base une base E de E constituée de vecteurs propres de u donc pour tout e E , P ( u)(e ) 0E ( car e est ***associé*** à une valeur propre i ) d' où P ( u ) 0L ( E )

**43500**: On en ***déduit*** que u est scindé à racines simples car il divise P qui est lui-même scindé à racines simples

**43504**: On en déduit que u est ***scindé*** à racines simples car il divise P qui est lui-même scindé à racines simples

**43515**: On en déduit que u est scindé à racines simples car il divise P qui est lui-même ***scindé*** à racines simples

**43544**: Théorème de Cayley - Hamilton Soit P un polynôme unitaire : ak X k X p KX La matrice compagnon du polynôme P est ***définie*** par : Soit P KX un polynôme unitaire

**43609**: Alors : Démonstration Reprenons les notations de la définition 5.5 , page précédente et effectuons l' opération élémentaire : de sorte que ( on développe ensuite selon la première ligne et on reconnaît le déterminant d' une matrice triangulaire ) : Théorème 5.3 Cayley - Hamilton Soit E un K - espace vectoriel de dimension ***finie*** non nulle et u L ( E )

**43621**: Alors Autrement ***dit*** , le polynôme caractéristique u de u est annulateur de u. Démonstration Soit x E , x 6 0E ( possible car n dim E 1 )

**43678**: , up ( x ) ) soit ***liée*** , donc la famille ( x , u(x ) ,

**43861**: En particulier , puisque deg u dim E et u 6 0KX , on a : 1 deg u dim E On peut également démontrer ce théorème sans passer par la notion de matrice compagnon , en le démontrant d' abord dans le cas d' une matrice trigonalisable ( en utilisant notamment le fait que dans ce cas son polynôme caractéristique est ***scindé*** )

**43948**: Théorème 5.4 Soit E un K - espace vectoriel de dimension ***finie*** non nulle et u L ( E )

**43978**: il existe un polynôme annulateur de u ***scindé*** 3

**43983**: u est ***scindé*** 4

**43988**: u est ***scindé***

**44158**: 5.1.4 Soit E un K - espace vectoriel de dimension ***finie*** n 1 , x E et f L ( E )

**44245**: 5.1.5 Soit E un C - espace vectoriel de dimension ***finie*** non nulle et u L ( E ) , donner une condition nécessaire et suffisante pour que : u2 diagonalisable u diagonalisable Que devient ce résultat lorsque le corps est R ? 5.1.6 Soit A Mn ( R ) telle que A3 3 A 2 In 0n

**44340**: A est-elle diagonalisable ? Calculer Ap pour p Z. Topologie sur les endomorphismes Soit ( E , k k ) un espace vectoriel normé a , on peut munir Lc ( E ) ( l' espace des endomorphismes continus de E ) de la norme subordonnée ***donnée*** par : On a alors : De plus , est une norme d' algèbre : Si E est de dimension finie , tous les endomorphismes sont continues : Lc ( E ) L ( E ) ( c' est faux en dimension infinie )

**44448**: a. Dans cette partie , quand on parle d' un espace vectoriel normé ( E , k k ) , on se place toujours dans le corps K R ou C. Proposition 5.2 Densité des endomorphismes inversibles et diagonalisables Soit ( E , k k ) un espace vectoriel normé de dimension ***finie*** non nulle

**44544**: Son polynôme caractéristique u ne a qu' un nombre ***fini*** de racines ( car deg u dim E 1 ) donc il existe r 0 tel que u ne s' annule pas sur B(0 , r ) 0

**44575**: Autrement ***dit*** , pour tout B(0 , r ) 0 , u

**44873**: Alors : pour toute valeur propre de u , u Démonstration Si est une valeur propre de u , en notant x E 0E un vecteur propre ***associé*** , on a ku(x)k u kxk ku(x)k k.xk kxk d' où u en divisant par kxk 6 0E

**44906**: Le rayon spectral de A est ***défini*** par : D' après la propriété précédente , pour ne importe quelle norme d' algèbre sur Mn ( C ) , on a : Soit A Mn ( C ) et soit 0

**45121**: Dans beaucoup d' applications , on s' intéresse à des suites récurrentes de la forme : par exemple pour résoudre de manière approchée des équations différentielles ou des équations aux ***dérivées*** partielles

**45136**: Il est souvent crucial que la suite ( Uk ) kN soit ***bornée*** ( par exemple pour garantir la stabilité du schéma numérique considéré )

**45147**: Il est souvent crucial que la suite ( Uk ) kN soit bornée ( par exemple pour garantir la stabilité du schéma numérique ***considéré*** )

**45173**: La propriété précédente implique que : ( Ak ) kN est ***bornée*** , alors ( A ) 1 et si ( A ) 1 alors ( Ak ) kN est bornée Ainsi , le calcul du rayon spectral de A ( souvent calculé de manière approchée dans les applications ) nous permet de démontrer la stabilité d' une méthode numérique

**45192**: La propriété précédente implique que : ( Ak ) kN est bornée , alors ( A ) 1 et si ( A ) 1 alors ( Ak ) kN est ***bornée*** Ainsi , le calcul du rayon spectral de A ( souvent calculé de manière approchée dans les applications ) nous permet de démontrer la stabilité d' une méthode numérique

**45204**: La propriété précédente implique que : ( Ak ) kN est bornée , alors ( A ) 1 et si ( A ) 1 alors ( Ak ) kN est bornée Ainsi , le calcul du rayon spectral de A ( souvent ***calculé*** de manière approchée dans les applications ) nous permet de démontrer la stabilité d' une méthode numérique

**45482**: partielles qui sont des k0 ak .u est un polynôme en u car c' est la limite de la suite des sommes polynômes en u et Ku est ***fermé*** car de dimension finie

**45486**: partielles qui sont des k0 ak .u est un polynôme en u car c' est la limite de la suite des sommes polynômes en u et Ku est fermé car de dimension ***finie***

**45565**: L' exponentielle de u est ***défini*** exp(u ) eu Soit ( E , k k ) un espace vectoriel normé de dimension finie et soit ( u , v ) L ( E)2

**45568**: L' exponentielle de u est défini exp(u ) ***eu*** Soit ( E , k k ) un espace vectoriel normé de dimension finie et soit ( u , v ) L ( E)2

**45747**: L' exponentielle de matrices sert notamment pour étudier les systèmes linéaires différentiels à coefficients constants ( voir le chapitre 4 ) : La solution vérifiant la condition initiale X(t0 ) X0 est ***donnée*** par : exp(s

**45760**: A ) B(s ) ds Nous sommes donc ***ramenés*** au calcul de exp(t

**45903**: 5.2.1 Soit ( E , k k ) un espace vectoriel normé de dimension ***finie*** n 1

**46000**: 5.2.3 Soit ( E , k k ) un espace vectoriel normé de dimension ***finie*** n 1

**46070**: ( a ) Démontrer que p A Mn ( R ) , rang A p est ***fermé*** dans Mn ( R )

**46114**: 5.2.5 Soit ( E , k k ) un espace vectoriel normé de dimension ***finie***

**46147**: Démontrer que : 5.2.6 Démontrer que : A Mn ( C ) , det(exp(A ) ) exp(trace(A ) ) Décomposition de Dunford Soit E un K - espace vectoriel de dimension ***finie*** non nulle et u L ( E ) nilpotent a

**46355**: Soit E un K - espace vectoriel de dimension ***finie*** n 1 et u L ( E )

**46373**: Si le polynôme caractéristique u de u est ***scindé*** : où les k sont distincts , on appelle espace caractéristique associé à la valeur propre k : Soit E un K - espace vectoriel de dimension finie non nulle et u L ( E ) tel que u soit scindé : où les k sont distincts 1

**46385**: Si le polynôme caractéristique u de u est scindé : où les k sont distincts , on appelle espace caractéristique ***associé*** à la valeur propre k : Soit E un K - espace vectoriel de dimension finie non nulle et u L ( E ) tel que u soit scindé : où les k sont distincts 1

**46401**: Si le polynôme caractéristique u de u est scindé : où les k sont distincts , on appelle espace caractéristique associé à la valeur propre k : Soit E un K - espace vectoriel de dimension ***finie*** non nulle et u L ( E ) tel que u soit scindé : où les k sont distincts 1

**46414**: Si le polynôme caractéristique u de u est scindé : où les k sont distincts , on appelle espace caractéristique associé à la valeur propre k : Soit E un K - espace vectoriel de dimension finie non nulle et u L ( E ) tel que u soit ***scindé*** : où les k sont distincts 1

**46752**: En particulier , les projecteurs sur les espaces propres sont des polynômes en u. Il est toujours possible de se ramener au cas où u est ***scindé*** en se plaçant dans le corps des racines de u ( si K R , on se place dans K C )

**46784**: Le point 2 démontre que si u est ***scindé*** , alors dans une base E de E adaptée à la décomposition E Fu ( 1 ) Fu ( p ) , la matrice de u est diagonale par blocs : Théorème 5.5 Décomposition de Dunford Soit E un K - espace vectoriel de dimension finie non nulle et u L ( E ) tel que u soit scindé

**46793**: Le point 2 démontre que si u est scindé , alors dans une base E de E ***adaptée*** à la décomposition E Fu ( 1 ) Fu ( p ) , la matrice de u est diagonale par blocs : Théorème 5.5 Décomposition de Dunford Soit E un K - espace vectoriel de dimension finie non nulle et u L ( E ) tel que u soit scindé

**46830**: Le point 2 démontre que si u est scindé , alors dans une base E de E adaptée à la décomposition E Fu ( 1 ) Fu ( p ) , la matrice de u est diagonale par blocs : Théorème 5.5 Décomposition de Dunford Soit E un K - espace vectoriel de dimension ***finie*** non nulle et u L ( E ) tel que u soit scindé

**46843**: Le point 2 démontre que si u est scindé , alors dans une base E de E adaptée à la décomposition E Fu ( 1 ) Fu ( p ) , la matrice de u est diagonale par blocs : Théorème 5.5 Décomposition de Dunford Soit E un K - espace vectoriel de dimension finie non nulle et u L ( E ) tel que u soit ***scindé***

**47185**: La décomposition de Dunford ne est donc pas un outil ***adapté*** au calcul numérique approché

**47189**: La décomposition de Dunford ne est donc pas un outil adapté au calcul numérique ***approché***

**47356**: on prend une base E de E ***adaptée*** à la décomposition de E en espaces caractéristiques , pour obtenir la matrice A de u dans cette base sous la forme d' une matrice diagonale par blocs : 2

**47420**: I Mais ces méthodes sont fastidieuses ( et peu ***adaptées*** au calcul numérique approchée , voir la remarque 5.15 , page précédente )

**47474**: Le commutant de u est ***défini*** par : 1

**47535**: Soit E un K - espace vectoriel de dimension ***finie*** n 1 et u L ( E ) diagonalisable

**47604**: Alors : C ( u ) est isomorphe à En particulier : u a n valeurs propres distinctes si , et seulement si , Ku C ( u ) si , et seulement si , dim C ( u ) n Démonstration Considérons l' application linéaire de C ( u ) dans pk1 L Eu ( k ) ***définie*** par : On vérifie alors que est bien définie et bijective ( car les espaces propres Eu ( k ) sont stables par les éléments de C ( u ) et que E Eu ( 1 ) Eu ( p ) ) d' où le résultat

**47613**: Alors : C ( u ) est isomorphe à En particulier : u a n valeurs propres distinctes si , et seulement si , Ku C ( u ) si , et seulement si , dim C ( u ) n Démonstration Considérons l' application linéaire de C ( u ) dans pk1 L Eu ( k ) définie par : On vérifie alors que est bien ***définie*** et bijective ( car les espaces propres Eu ( k ) sont stables par les éléments de C ( u ) et que E Eu ( 1 ) Eu ( p ) ) d' où le résultat

**47814**: Proposition 5.3 Réduction de Jordan d' un endomorphisme nilpotent Soit E un K - espace vectoriel de dimension ***finie*** n 1 et soit u L ( E ) nilpotent

**47842**: Alors il existe une base E de E pour laquelle : où les blocs Bk , ***appelés*** blocs de Jordan , sont soit nuls , soit de la forme : Démonstration On passe par les invariants de similitude ( voir la dernière partie ) : si ( P1 ,

**47929**: On en ***déduit*** que les matrices compagnons C(Pi ) des Pi sont des transposées de blocs de Jordan

**47987**: Théorème 5.6 Réduction de Jordan Soit E un K - espace vectoriel de dimension ***finie*** n 1 et soit u L ( E ) tel que son polynôme caractéristique u soit scindé

**48004**: Théorème 5.6 Réduction de Jordan Soit E un K - espace vectoriel de dimension finie n 1 et soit u L ( E ) tel que son polynôme caractéristique u soit ***scindé***

**48022**: Alors il existe une base E de E pour laquelle : où les blocs Bk , ***appelés*** blocs de Jordan , sont soit diagonales , soit de la forme : Démonstration On reprend les notations de la définition 5.8 , page 245

**48109**: Lorsque les valeurs propres sont ***séparées*** , tout se passe bien : 2

**48123**: Lorsque les valeurs propres sont ***confondues*** , il y a de mauvaises surprises , ainsi : 5.3.1 Trouver le commutant de la matrice : 5.3.2 Soit E un C - espace vectoriel de dimension finie n 1 et soit u L(E )

**48152**: Lorsque les valeurs propres sont confondues , il y a de mauvaises surprises , ainsi : 5.3.1 Trouver le commutant de la matrice : 5.3.2 Soit E un C - espace vectoriel de dimension ***finie*** n 1 et soit u L(E )

**48338**: Démontrer qu' elle commute avec sa ***transposée*** si , et seulement si , elle est diagonale

**48531**: ( b ) En déduire que si E est de dimension ***finie*** alors dim E q. I On pose E1 Vect x , u(x ) ,

**48562**: , uq1 ( x ) et on suppose que E est de dimension ***finie*** n p. I Par ailleurs , soit E ? une forme linéaire telle que : On pose alors : I Finalement , on pose : ( c ) Justifier l' existence de

**48823**: On est alors ***ramené*** à la résolution de B p .In , ce qui est aisée

**48966**: On en ***déduit*** que B1 est diagonalisable dans C , égale à I2 ou semblable à Diag(j , j 2 ) avec j exp(2 i 3 ) , ce qui ramené dans R nous donne : Finalement , les B qui conviennent vérifient : Le calcul brutal ne est pas efficace ! Même si l' on pense à montrer que B C ( A )

**48994**: On en déduit que B1 est diagonalisable dans C , égale à I2 ou semblable à Diag(j , j 2 ) avec j exp(2 i 3 ) , ce qui ***ramené*** dans R nous donne : Finalement , les B qui conviennent vérifient : Le calcul brutal ne est pas efficace ! Même si l' on pense à montrer que B C ( A )

**49754**: Les matrices suivantes sont semblables : Soit E un K - espace vectoriel de dimension ***finie*** non nulle , u L ( E ) et x E. On définit : Ix l' idéal des polynômes annulateurs x , c' est - à - dire le noyau de : KX E définie par ( P ) x le polynôme minimal de x ( le polynôme unitaire engendrant l' idéal Ix )

**49789**: Les matrices suivantes sont semblables : Soit E un K - espace vectoriel de dimension finie non nulle , u L ( E ) et x E. On définit : Ix l' idéal des polynômes annulateurs x , c' est - à - dire le noyau de : KX E ***définie*** par ( P ) x le polynôme minimal de x ( le polynôme unitaire engendrant l' idéal Ix )

**49924**: Théorème 5.7 Théorème de Frobenius Soit E un K - espace vectoriel de dimension ***finie*** non nulle et u L ( E )

**49995**: En notant k deg u , une base de hxi est ***donnée*** par : On complète en une base ( e1 ,

**50028**: , e?n ) est la base duale ***associée*** , posons : Alors G est un sous-espace vectoriel de E stable par u. Démontrons que E hxi G. ce qui contredit y G. On a donc y 0E d' où hx G 0E

**50050**: , e?n ) est la base duale associée , posons : Alors G est un sous-espace vectoriel de E stable par u. Démontrons que E hxi G. ce qui ***contredit*** y G. On a donc y 0E d' où hx G 0E

**50230**: Finalement , Ku et Vect ( ) sont isomorphes donc la même dimension : dim Vect ( ) dim Ku deg u k. On a donc bien E hxi G avec G stable par u. Le polynôme minimal de uhxi est x u donc divise le polynôme minimal de uG On recommence alors la démonstration avec G à la place de E et uG G à la place de u. En un nombre ***fini*** r d' étapes , on obtient la décomposition voulue

**50313**: Soit E un K - espace vectoriel de dimension ***finie*** non nulle et u L ( E )

**50343**: , Pr où , pour tout i 1 , r , Pi xi ***donné*** par le théorème 5.7 , page précédente , sont appelées les invariants de similitude de u. La matrice de u dans la base E de E adaptée à la décomposition de E du théorème 5.7 , page précédente : est appelée forme de Frobenius

**50353**: , Pr où , pour tout i 1 , r , Pi xi donné par le théorème 5.7 , page précédente , sont ***appelées*** les invariants de similitude de u. La matrice de u dans la base E de E adaptée à la décomposition de E du théorème 5.7 , page précédente : est appelée forme de Frobenius

**50370**: , Pr où , pour tout i 1 , r , Pi xi donné par le théorème 5.7 , page précédente , sont appelées les invariants de similitude de u. La matrice de u dans la base E de E ***adaptée*** à la décomposition de E du théorème 5.7 , page précédente : est appelée forme de Frobenius

**50384**: , Pr où , pour tout i 1 , r , Pi xi donné par le théorème 5.7 , page précédente , sont appelées les invariants de similitude de u. La matrice de u dans la base E de E adaptée à la décomposition de E du théorème 5.7 , page précédente : est ***appelée*** forme de Frobenius

**50489**: Les théorèmes 5.7 , page précédente et 5.8 , de la présente page permettent de montrer de nombreux résultats théoriques , par exemple : la réduction de Jordan des endomorphismes nilpotents ( voir la proposition 5.3 , page 249 ) le fait que toute matrice est semblable à sa ***transposée*** si deux matrices A et B de Mn ( K ) sont semblables en tant que matrices de Mn ( L ) , où L est un surcorps de K , alors elles sont semblables sur Mn ( K ) ( en effet , par unicité , les invariants de similitude dans Mn ( K ) sont les invariants de similitudes dans Mn ( L ) )

**50571**: Si C(P ) est une matrice compagnon , sa forme de Jordan ne ***fait*** apparaître qu' un et un seul bloc pour chaque valeur propre ( est une racine de P et la taille du bloc est son ordre de multiplicité )

**50896**: Étape 2 Si il existe sur la première ligne ( ou la première colonne ) un élément m1,k ( ou mk,1 ) non multiple de m1,1 , on le remplace ( en utilisant une transvection ) par le reste de la division euclidienne de m1,k ( ou mk,1 ) par m1,1 et on revient à l' étape 1 en permutant les colonnes ( ou les lignes ) 1 et k. Etape 3 L' étape 2 a pour effet de diminuer strictement ( M ) , elle ne peut donc se réaliser qu' un nombre ***fini*** de fois

**51006**: Étape 5 Au bout d' un nombre ***fini*** d' étapes , on obtient une matrice de la forme où m1,1 divise tous les éléments de M1 et on recommence l' étape 1 à partir de la matrice M1 qui est de taille strictement plus petite

**51074**: On en ***déduit*** que sa forme de Frobenius est : La forme de Smith correspondante est donc les invariants de similitude de A sont ( X 1 , A ( X 1)2 )

**51108**: On en ***déduit*** que sa forme de Frobenius 5.5.1 Déterminer la forme de Frobenius de la matrice : 5.5.2 Démontrer que : sont semblables

**51219**: ( a ) On suppose que P est ***scindé*** dans K , à quelle condition C(P ) est-elle diagonalisable ? ( b ) On suppose que P est irréductible , montrer qu' en ce cas KC(P ) est un corps P admet au moins une racine ( laquelle ? )

**51288**: En déduire la construction d' un corps à 4 éléments et d' un corps à 8 Un exemple d' utilisation de la réduction sur un corps ***fini*** Dans tout ce chapitre , nous avons travaillé dans un corps commutatif quelconque , et plus nécessairement dans R ou C. Voici un petit exemple de synthèse montrant comment on peu utiliser les notions vues ici dans un corps fini

**51323**: En déduire la construction d' un corps à 4 éléments et d' un corps à 8 Un exemple d' utilisation de la réduction sur un corps fini Dans tout ce chapitre , nous avons travaillé dans un corps commutatif quelconque , et plus nécessairement dans R ou C. Voici un petit exemple de synthèse montrant comment on peu utiliser les notions ***vues*** ici dans un corps fini

**51328**: En déduire la construction d' un corps à 4 éléments et d' un corps à 8 Un exemple d' utilisation de la réduction sur un corps fini Dans tout ce chapitre , nous avons travaillé dans un corps commutatif quelconque , et plus nécessairement dans R ou C. Voici un petit exemple de synthèse montrant comment on peu utiliser les notions vues ici dans un corps ***fini***

**51334**: Cet exercice a été ***posé*** à un oral de concours dans les années 70 ... Soit ( a , b , u0 , u1 ) Z2 , on construit successivement les suites récurrentes : Nous allons démontrer que 1

**51421**: Démonstration de la périodicité La suite ( vn , vn1 ) nN est à valeurs dans 0 , 92 qui est ***fini***

**51565**: Dans Z2 Z La vectorialisation de la suite ( en notant a la classe de a dans Z2 Z ) donne On obtient alors que A est semblable à une matrice parmi ( Z2 Z ) impossible de période 1 ou 2 de période 1 si son polynôme caractéristique est ***scindé***

**51574**: Lorsque son polynôme caractéristique ne est pas ***scindé*** , de degré 2 , il est alors irréductible ( c' est X 2 X 1 ) donc , en posant C(A ) , et en se plaçant dans M2 ( Z2 Z ) , les racines de A sont alors et I2 , distinctes

**51685**: Dans Z5 Z ( Nous noterons toujours les classes sous la forme a ) Le même raisonnement nous ***conduit*** , lorsque le polynôme caractéristique est scindé aux matrices ( ( , ) Z5 Z2 ) impossible de période 1 , 5 ou 20 de période 1 , 2 ou 4 En effet , on sait d' après le petit théorème de Fermat que ce qui justifie la période 1 , 2 ou 4 pour la troisième forme de matrice

**51692**: Dans Z5 Z ( Nous noterons toujours les classes sous la forme a ) Le même raisonnement nous conduit , lorsque le polynôme caractéristique est ***scindé*** aux matrices ( ( , ) Z5 Z2 ) impossible de période 1 , 5 ou 20 de période 1 , 2 ou 4 En effet , on sait d' après le petit théorème de Fermat que ce qui justifie la période 1 , 2 ou 4 pour la troisième forme de matrice

**51836**: mod 5 et n Lorsque le polynôme caractéristique de A ne est plus ***scindé*** , il est irréductible ( car de degré 2 ) , en posant C(A ) , et en travaillant dans le corps K Z5 Z qui est de cardinal 25 , on a ( même démonstration que Fermat ) on a donc de période 3 , 6 , 8 , 12 ou 24

**51907**: En enlevant celle qu' on connaît ( 1 , 2 et 4 correspondant aux matrices précédemment ***vues*** ) Dans Z5 Z les périodes de la suite ( un ) nN sont , à partir d' un certain rang dans Finalement les périodes possibles sont , d' après le théorème chinois ( on fait le PPCM des périodes dans Z2 Z et dans Z5 Z ) Juste pour le plaisir , construisons une situation où la période est 60

**52058**: La vérification par le calcul donne , pour les 63 premiers termes ( avec u0 u1 1 ) Application linéaire , 46 Automorphisme , 46 ***adaptée*** à une somme directe , 43 ante - duale , 70 d' un espace vectoriel , 35 Codimension , 72 Coefficients d' une matrice , 90 Cofacteur d' une matrice carrée , 170 Comatrice , 170 Combinaison linéaire , 21 Commutant d' un endomorphisme , 248 Condition initiale ( d' un système linéaire ) , 207 Décomposition LU , 180 Dimension d' un espace vectoriel , 39 Drapeau stable , 199 Droite vectorielle , 39 Dual d' un espace vectoriel , 68 Décomposition de Dunford , 247 Décomposition en somme directe , 32 Déterminant d' un endomorphisme , 162 d' une famille de vecteurs , 158 d' une matrice carrée , 160 de Vandermonde , 173 Éléments propres , 184 Endomorphisme , 46 diagonalisable , 196 trigonalisable , 199 Équation d' un hyperplan , 71 Espace caractéristique , 245 Espace propre d' un endomorphisme , 183 d' une matrice , 183 Espace vectoriel , 19 de dimension finie , 38 Exponentielle d' endomorphisme , 240 Famille duale , 69 Fonctions spline , 85 Forme p - linéaire , 154 anti-symétrique , 155 symétrique , 155 Forme de Smith , 273 Forme linéaire , 46 , 68 Groupe symétrique , 151 Hyperplan d' un espace vectoriel , 71 Idéal annulateur , 230 d' une application linéaire , 49 d' une matrice , 114 Interpolation de Lagrange , 80 Isomorphisme , 46 Matrice , 89 anti-symétrique , 106 augmentée , 140 circulante , 198 compagnon , 232 d' un vecteur , 90 d' une application linéaire , 90 d' une famille de vecteurs , 109 de dilatation , 121 de passage , 110 de permutation , 121 de transvection , 121 diagonale , 106 diagonalisable , 196 identité , 100 inversible , 108 symétrique , 106 triangulaire , 106 trigonalisable , 200 semblables , 119 équivalentes , 119 Mineur d' une matrice carrée , 170 Multiplicité d' une valeur propre , 195 d' une application linéaire , 49 d' une matrice , 114 Orthogonal ( direct ) d' une partie , 77 ( indirect ) d' une partie , 77 Partie ou famille génératrice , 25 Partition d' un ensemble , 116 Permutation , 151 Plan vectoriel , 39 caractéristique , 193 d' endomorphisme , 227 de matrice , 227 minimal , 231 d' espaces vectoriels , 22 de deux matrices , 98 de Kronecker , 146 Projecteur , 53 Projection , 52 Trace d' une matrice , 107 Transposition , 152 Transposée d' une matrice , 103 Valeur propre d' un endomorphisme , 183 d' une matrice , 183 Vecteur propre d' un endomorphisme , 183 d' une matrice , 183 Vecteurs , 20 indépendants , 34 linéairement dépendants , 34 d' une application linéaire , 55 d' une matrice , 114 Rayon spectral , 239 d' équivalence , 116 réflexive , 116 symétrique , 116 transitive , 116 Scalaires , 20 Second membre ( d' un système linéaire ) , 207 Signature d' une permutation , 152 de sous-espaces vectoriels , 28 d' une famille d' espaces vectoriels , 31 de deux sous-espaces vectoriels , 30 Sous-espace vectoriel , 23 engendré par une partie , 25 stable , 52 d' un endomorphisme , 183 d' une matrice , 183 Supplémentaire d' un sous-espace vectoriel , 32 Support d' une famille , 26 Symbole de Kronecker , 58 Symétrie , 54 associé , 207 Système linéaire , 78 de Cramer , 140 différentiel , 215 homogène , 78 associé , 78 récurrent , 207 Caractérisation des applications linéaires injectives , 67 surjectives , 67 des automorphismes en dimension finie , 57 des classes de similitude , 270 des endomorphismes diagonalisables , 197 , 229 trigonalisables , 200 , 234 des hyperplans , 72 des matrices équivalentes , 120 Changement de base pour les applications linéaires , 111 pour les vecteurs , 111 Correspondance matrices - blocs et décomposition en somme directe , 144 de co-diagonalisation , 205 de co-trigonalisation , 205 Développement d' un déterminant , 171 Forme de Jordan d' un endomorphisme nilpotent , 249 trigonalisable , 249 Formule de Grassman , 44 Invariants de similitude , 270 Lemme des noyaux , 228 Mise en équation des sous-espaces de codimensions finies , 75 Pivot de Gauss , 127 Rang d' une composée , 56 d' extension linéaire , 66 de Cayley - Hamilton , 233 de factorisation des applications linéaires , 62 de Frobenius , 269 de l' échange , 38 de la base incomplète , 40 des faisceaux d' hyperplans , 73 du rang , 57 du relèvement linéaire , 65 Commandes Wxmaxima binomial , 208 charpoly , 234 columnswap , 279 eigenvalues , 184 , 190 identfor , 175 cspline , 86 length , 136 load(solverec ) , 211 polytocompanion , 270 , 271 randompermutation , 137 setify , 26 solverec , 211 kroneckerproduct , 147 Commandes Python matplotlib , 26 binomial , 208 Dictionnaire , 113 .factorlist , 235 BlockDiagMatrix , 272 BlockMatrix , 148 expand , 186 Function , 192 initsession , 26 interpolate , 82 , 85 interpolatingspline , 86 .charpoly , 235 .cofactormatrix , 176 .extract , 170 .allcoeffs , 271 rsolve , 213 integer , 192 Figure 5.1 Similitude Trajectoires du système :

**52216**: La vérification par le calcul donne , pour les 63 premiers termes ( avec u0 u1 1 ) Application linéaire , 46 Automorphisme , 46 adaptée à une somme directe , 43 ante - duale , 70 d' un espace vectoriel , 35 Codimension , 72 Coefficients d' une matrice , 90 Cofacteur d' une matrice carrée , 170 Comatrice , 170 Combinaison linéaire , 21 Commutant d' un endomorphisme , 248 Condition initiale ( d' un système linéaire ) , 207 Décomposition LU , 180 Dimension d' un espace vectoriel , 39 Drapeau stable , 199 Droite vectorielle , 39 Dual d' un espace vectoriel , 68 Décomposition de Dunford , 247 Décomposition en somme directe , 32 Déterminant d' un endomorphisme , 162 d' une famille de vecteurs , 158 d' une matrice carrée , 160 de Vandermonde , 173 Éléments propres , 184 Endomorphisme , 46 diagonalisable , 196 trigonalisable , 199 Équation d' un hyperplan , 71 Espace caractéristique , 245 Espace propre d' un endomorphisme , 183 d' une matrice , 183 Espace vectoriel , 19 de dimension ***finie*** , 38 Exponentielle d' endomorphisme , 240 Famille duale , 69 Fonctions spline , 85 Forme p - linéaire , 154 anti-symétrique , 155 symétrique , 155 Forme de Smith , 273 Forme linéaire , 46 , 68 Groupe symétrique , 151 Hyperplan d' un espace vectoriel , 71 Idéal annulateur , 230 d' une application linéaire , 49 d' une matrice , 114 Interpolation de Lagrange , 80 Isomorphisme , 46 Matrice , 89 anti-symétrique , 106 augmentée , 140 circulante , 198 compagnon , 232 d' un vecteur , 90 d' une application linéaire , 90 d' une famille de vecteurs , 109 de dilatation , 121 de passage , 110 de permutation , 121 de transvection , 121 diagonale , 106 diagonalisable , 196 identité , 100 inversible , 108 symétrique , 106 triangulaire , 106 trigonalisable , 200 semblables , 119 équivalentes , 119 Mineur d' une matrice carrée , 170 Multiplicité d' une valeur propre , 195 d' une application linéaire , 49 d' une matrice , 114 Orthogonal ( direct ) d' une partie , 77 ( indirect ) d' une partie , 77 Partie ou famille génératrice , 25 Partition d' un ensemble , 116 Permutation , 151 Plan vectoriel , 39 caractéristique , 193 d' endomorphisme , 227 de matrice , 227 minimal , 231 d' espaces vectoriels , 22 de deux matrices , 98 de Kronecker , 146 Projecteur , 53 Projection , 52 Trace d' une matrice , 107 Transposition , 152 Transposée d' une matrice , 103 Valeur propre d' un endomorphisme , 183 d' une matrice , 183 Vecteur propre d' un endomorphisme , 183 d' une matrice , 183 Vecteurs , 20 indépendants , 34 linéairement dépendants , 34 d' une application linéaire , 55 d' une matrice , 114 Rayon spectral , 239 d' équivalence , 116 réflexive , 116 symétrique , 116 transitive , 116 Scalaires , 20 Second membre ( d' un système linéaire ) , 207 Signature d' une permutation , 152 de sous-espaces vectoriels , 28 d' une famille d' espaces vectoriels , 31 de deux sous-espaces vectoriels , 30 Sous-espace vectoriel , 23 engendré par une partie , 25 stable , 52 d' un endomorphisme , 183 d' une matrice , 183 Supplémentaire d' un sous-espace vectoriel , 32 Support d' une famille , 26 Symbole de Kronecker , 58 Symétrie , 54 associé , 207 Système linéaire , 78 de Cramer , 140 différentiel , 215 homogène , 78 associé , 78 récurrent , 207 Caractérisation des applications linéaires injectives , 67 surjectives , 67 des automorphismes en dimension finie , 57 des classes de similitude , 270 des endomorphismes diagonalisables , 197 , 229 trigonalisables , 200 , 234 des hyperplans , 72 des matrices équivalentes , 120 Changement de base pour les applications linéaires , 111 pour les vecteurs , 111 Correspondance matrices - blocs et décomposition en somme directe , 144 de co-diagonalisation , 205 de co-trigonalisation , 205 Développement d' un déterminant , 171 Forme de Jordan d' un endomorphisme nilpotent , 249 trigonalisable , 249 Formule de Grassman , 44 Invariants de similitude , 270 Lemme des noyaux , 228 Mise en équation des sous-espaces de codimensions finies , 75 Pivot de Gauss , 127 Rang d' une composée , 56 d' extension linéaire , 66 de Cayley - Hamilton , 233 de factorisation des applications linéaires , 62 de Frobenius , 269 de l' échange , 38 de la base incomplète , 40 des faisceaux d' hyperplans , 73 du rang , 57 du relèvement linéaire , 65 Commandes Wxmaxima binomial , 208 charpoly , 234 columnswap , 279 eigenvalues , 184 , 190 identfor , 175 cspline , 86 length , 136 load(solverec ) , 211 polytocompanion , 270 , 271 randompermutation , 137 setify , 26 solverec , 211 kroneckerproduct , 147 Commandes Python matplotlib , 26 binomial , 208 Dictionnaire , 113 .factorlist , 235 BlockDiagMatrix , 272 BlockMatrix , 148 expand , 186 Function , 192 initsession , 26 interpolate , 82 , 85 interpolatingspline , 86 .charpoly , 235 .cofactormatrix , 176 .extract , 170 .allcoeffs , 271 rsolve , 213 integer , 192 Figure 5.1 Similitude Trajectoires du système :

**52295**: La vérification par le calcul donne , pour les 63 premiers termes ( avec u0 u1 1 ) Application linéaire , 46 Automorphisme , 46 adaptée à une somme directe , 43 ante - duale , 70 d' un espace vectoriel , 35 Codimension , 72 Coefficients d' une matrice , 90 Cofacteur d' une matrice carrée , 170 Comatrice , 170 Combinaison linéaire , 21 Commutant d' un endomorphisme , 248 Condition initiale ( d' un système linéaire ) , 207 Décomposition LU , 180 Dimension d' un espace vectoriel , 39 Drapeau stable , 199 Droite vectorielle , 39 Dual d' un espace vectoriel , 68 Décomposition de Dunford , 247 Décomposition en somme directe , 32 Déterminant d' un endomorphisme , 162 d' une famille de vecteurs , 158 d' une matrice carrée , 160 de Vandermonde , 173 Éléments propres , 184 Endomorphisme , 46 diagonalisable , 196 trigonalisable , 199 Équation d' un hyperplan , 71 Espace caractéristique , 245 Espace propre d' un endomorphisme , 183 d' une matrice , 183 Espace vectoriel , 19 de dimension finie , 38 Exponentielle d' endomorphisme , 240 Famille duale , 69 Fonctions spline , 85 Forme p - linéaire , 154 anti-symétrique , 155 symétrique , 155 Forme de Smith , 273 Forme linéaire , 46 , 68 Groupe symétrique , 151 Hyperplan d' un espace vectoriel , 71 Idéal annulateur , 230 d' une application linéaire , 49 d' une matrice , 114 Interpolation de Lagrange , 80 Isomorphisme , 46 Matrice , 89 anti-symétrique , 106 ***augmentée*** , 140 circulante , 198 compagnon , 232 d' un vecteur , 90 d' une application linéaire , 90 d' une famille de vecteurs , 109 de dilatation , 121 de passage , 110 de permutation , 121 de transvection , 121 diagonale , 106 diagonalisable , 196 identité , 100 inversible , 108 symétrique , 106 triangulaire , 106 trigonalisable , 200 semblables , 119 équivalentes , 119 Mineur d' une matrice carrée , 170 Multiplicité d' une valeur propre , 195 d' une application linéaire , 49 d' une matrice , 114 Orthogonal ( direct ) d' une partie , 77 ( indirect ) d' une partie , 77 Partie ou famille génératrice , 25 Partition d' un ensemble , 116 Permutation , 151 Plan vectoriel , 39 caractéristique , 193 d' endomorphisme , 227 de matrice , 227 minimal , 231 d' espaces vectoriels , 22 de deux matrices , 98 de Kronecker , 146 Projecteur , 53 Projection , 52 Trace d' une matrice , 107 Transposition , 152 Transposée d' une matrice , 103 Valeur propre d' un endomorphisme , 183 d' une matrice , 183 Vecteur propre d' un endomorphisme , 183 d' une matrice , 183 Vecteurs , 20 indépendants , 34 linéairement dépendants , 34 d' une application linéaire , 55 d' une matrice , 114 Rayon spectral , 239 d' équivalence , 116 réflexive , 116 symétrique , 116 transitive , 116 Scalaires , 20 Second membre ( d' un système linéaire ) , 207 Signature d' une permutation , 152 de sous-espaces vectoriels , 28 d' une famille d' espaces vectoriels , 31 de deux sous-espaces vectoriels , 30 Sous-espace vectoriel , 23 engendré par une partie , 25 stable , 52 d' un endomorphisme , 183 d' une matrice , 183 Supplémentaire d' un sous-espace vectoriel , 32 Support d' une famille , 26 Symbole de Kronecker , 58 Symétrie , 54 associé , 207 Système linéaire , 78 de Cramer , 140 différentiel , 215 homogène , 78 associé , 78 récurrent , 207 Caractérisation des applications linéaires injectives , 67 surjectives , 67 des automorphismes en dimension finie , 57 des classes de similitude , 270 des endomorphismes diagonalisables , 197 , 229 trigonalisables , 200 , 234 des hyperplans , 72 des matrices équivalentes , 120 Changement de base pour les applications linéaires , 111 pour les vecteurs , 111 Correspondance matrices - blocs et décomposition en somme directe , 144 de co-diagonalisation , 205 de co-trigonalisation , 205 Développement d' un déterminant , 171 Forme de Jordan d' un endomorphisme nilpotent , 249 trigonalisable , 249 Formule de Grassman , 44 Invariants de similitude , 270 Lemme des noyaux , 228 Mise en équation des sous-espaces de codimensions finies , 75 Pivot de Gauss , 127 Rang d' une composée , 56 d' extension linéaire , 66 de Cayley - Hamilton , 233 de factorisation des applications linéaires , 62 de Frobenius , 269 de l' échange , 38 de la base incomplète , 40 des faisceaux d' hyperplans , 73 du rang , 57 du relèvement linéaire , 65 Commandes Wxmaxima binomial , 208 charpoly , 234 columnswap , 279 eigenvalues , 184 , 190 identfor , 175 cspline , 86 length , 136 load(solverec ) , 211 polytocompanion , 270 , 271 randompermutation , 137 setify , 26 solverec , 211 kroneckerproduct , 147 Commandes Python matplotlib , 26 binomial , 208 Dictionnaire , 113 .factorlist , 235 BlockDiagMatrix , 272 BlockMatrix , 148 expand , 186 Function , 192 initsession , 26 interpolate , 82 , 85 interpolatingspline , 86 .charpoly , 235 .cofactormatrix , 176 .extract , 170 .allcoeffs , 271 rsolve , 213 integer , 192 Figure 5.1 Similitude Trajectoires du système :

**52469**: La vérification par le calcul donne , pour les 63 premiers termes ( avec u0 u1 1 ) Application linéaire , 46 Automorphisme , 46 adaptée à une somme directe , 43 ante - duale , 70 d' un espace vectoriel , 35 Codimension , 72 Coefficients d' une matrice , 90 Cofacteur d' une matrice carrée , 170 Comatrice , 170 Combinaison linéaire , 21 Commutant d' un endomorphisme , 248 Condition initiale ( d' un système linéaire ) , 207 Décomposition LU , 180 Dimension d' un espace vectoriel , 39 Drapeau stable , 199 Droite vectorielle , 39 Dual d' un espace vectoriel , 68 Décomposition de Dunford , 247 Décomposition en somme directe , 32 Déterminant d' un endomorphisme , 162 d' une famille de vecteurs , 158 d' une matrice carrée , 160 de Vandermonde , 173 Éléments propres , 184 Endomorphisme , 46 diagonalisable , 196 trigonalisable , 199 Équation d' un hyperplan , 71 Espace caractéristique , 245 Espace propre d' un endomorphisme , 183 d' une matrice , 183 Espace vectoriel , 19 de dimension finie , 38 Exponentielle d' endomorphisme , 240 Famille duale , 69 Fonctions spline , 85 Forme p - linéaire , 154 anti-symétrique , 155 symétrique , 155 Forme de Smith , 273 Forme linéaire , 46 , 68 Groupe symétrique , 151 Hyperplan d' un espace vectoriel , 71 Idéal annulateur , 230 d' une application linéaire , 49 d' une matrice , 114 Interpolation de Lagrange , 80 Isomorphisme , 46 Matrice , 89 anti-symétrique , 106 augmentée , 140 circulante , 198 compagnon , 232 d' un vecteur , 90 d' une application linéaire , 90 d' une famille de vecteurs , 109 de dilatation , 121 de passage , 110 de permutation , 121 de transvection , 121 diagonale , 106 diagonalisable , 196 identité , 100 inversible , 108 symétrique , 106 triangulaire , 106 trigonalisable , 200 semblables , 119 équivalentes , 119 Mineur d' une matrice carrée , 170 Multiplicité d' une valeur propre , 195 d' une application linéaire , 49 d' une matrice , 114 Orthogonal ( direct ) d' une partie , 77 ( indirect ) d' une partie , 77 Partie ou famille génératrice , 25 Partition d' un ensemble , 116 Permutation , 151 Plan vectoriel , 39 caractéristique , 193 d' endomorphisme , 227 de matrice , 227 minimal , 231 d' espaces vectoriels , 22 de deux matrices , 98 de Kronecker , 146 Projecteur , 53 Projection , 52 Trace d' une matrice , 107 Transposition , 152 ***Transposée*** d' une matrice , 103 Valeur propre d' un endomorphisme , 183 d' une matrice , 183 Vecteur propre d' un endomorphisme , 183 d' une matrice , 183 Vecteurs , 20 indépendants , 34 linéairement dépendants , 34 d' une application linéaire , 55 d' une matrice , 114 Rayon spectral , 239 d' équivalence , 116 réflexive , 116 symétrique , 116 transitive , 116 Scalaires , 20 Second membre ( d' un système linéaire ) , 207 Signature d' une permutation , 152 de sous-espaces vectoriels , 28 d' une famille d' espaces vectoriels , 31 de deux sous-espaces vectoriels , 30 Sous-espace vectoriel , 23 engendré par une partie , 25 stable , 52 d' un endomorphisme , 183 d' une matrice , 183 Supplémentaire d' un sous-espace vectoriel , 32 Support d' une famille , 26 Symbole de Kronecker , 58 Symétrie , 54 associé , 207 Système linéaire , 78 de Cramer , 140 différentiel , 215 homogène , 78 associé , 78 récurrent , 207 Caractérisation des applications linéaires injectives , 67 surjectives , 67 des automorphismes en dimension finie , 57 des classes de similitude , 270 des endomorphismes diagonalisables , 197 , 229 trigonalisables , 200 , 234 des hyperplans , 72 des matrices équivalentes , 120 Changement de base pour les applications linéaires , 111 pour les vecteurs , 111 Correspondance matrices - blocs et décomposition en somme directe , 144 de co-diagonalisation , 205 de co-trigonalisation , 205 Développement d' un déterminant , 171 Forme de Jordan d' un endomorphisme nilpotent , 249 trigonalisable , 249 Formule de Grassman , 44 Invariants de similitude , 270 Lemme des noyaux , 228 Mise en équation des sous-espaces de codimensions finies , 75 Pivot de Gauss , 127 Rang d' une composée , 56 d' extension linéaire , 66 de Cayley - Hamilton , 233 de factorisation des applications linéaires , 62 de Frobenius , 269 de l' échange , 38 de la base incomplète , 40 des faisceaux d' hyperplans , 73 du rang , 57 du relèvement linéaire , 65 Commandes Wxmaxima binomial , 208 charpoly , 234 columnswap , 279 eigenvalues , 184 , 190 identfor , 175 cspline , 86 length , 136 load(solverec ) , 211 polytocompanion , 270 , 271 randompermutation , 137 setify , 26 solverec , 211 kroneckerproduct , 147 Commandes Python matplotlib , 26 binomial , 208 Dictionnaire , 113 .factorlist , 235 BlockDiagMatrix , 272 BlockMatrix , 148 expand , 186 Function , 192 initsession , 26 interpolate , 82 , 85 interpolatingspline , 86 .charpoly , 235 .cofactormatrix , 176 .extract , 170 .allcoeffs , 271 rsolve , 213 integer , 192 Figure 5.1 Similitude Trajectoires du système :

**52579**: La vérification par le calcul donne , pour les 63 premiers termes ( avec u0 u1 1 ) Application linéaire , 46 Automorphisme , 46 adaptée à une somme directe , 43 ante - duale , 70 d' un espace vectoriel , 35 Codimension , 72 Coefficients d' une matrice , 90 Cofacteur d' une matrice carrée , 170 Comatrice , 170 Combinaison linéaire , 21 Commutant d' un endomorphisme , 248 Condition initiale ( d' un système linéaire ) , 207 Décomposition LU , 180 Dimension d' un espace vectoriel , 39 Drapeau stable , 199 Droite vectorielle , 39 Dual d' un espace vectoriel , 68 Décomposition de Dunford , 247 Décomposition en somme directe , 32 Déterminant d' un endomorphisme , 162 d' une famille de vecteurs , 158 d' une matrice carrée , 160 de Vandermonde , 173 Éléments propres , 184 Endomorphisme , 46 diagonalisable , 196 trigonalisable , 199 Équation d' un hyperplan , 71 Espace caractéristique , 245 Espace propre d' un endomorphisme , 183 d' une matrice , 183 Espace vectoriel , 19 de dimension finie , 38 Exponentielle d' endomorphisme , 240 Famille duale , 69 Fonctions spline , 85 Forme p - linéaire , 154 anti-symétrique , 155 symétrique , 155 Forme de Smith , 273 Forme linéaire , 46 , 68 Groupe symétrique , 151 Hyperplan d' un espace vectoriel , 71 Idéal annulateur , 230 d' une application linéaire , 49 d' une matrice , 114 Interpolation de Lagrange , 80 Isomorphisme , 46 Matrice , 89 anti-symétrique , 106 augmentée , 140 circulante , 198 compagnon , 232 d' un vecteur , 90 d' une application linéaire , 90 d' une famille de vecteurs , 109 de dilatation , 121 de passage , 110 de permutation , 121 de transvection , 121 diagonale , 106 diagonalisable , 196 identité , 100 inversible , 108 symétrique , 106 triangulaire , 106 trigonalisable , 200 semblables , 119 équivalentes , 119 Mineur d' une matrice carrée , 170 Multiplicité d' une valeur propre , 195 d' une application linéaire , 49 d' une matrice , 114 Orthogonal ( direct ) d' une partie , 77 ( indirect ) d' une partie , 77 Partie ou famille génératrice , 25 Partition d' un ensemble , 116 Permutation , 151 Plan vectoriel , 39 caractéristique , 193 d' endomorphisme , 227 de matrice , 227 minimal , 231 d' espaces vectoriels , 22 de deux matrices , 98 de Kronecker , 146 Projecteur , 53 Projection , 52 Trace d' une matrice , 107 Transposition , 152 Transposée d' une matrice , 103 Valeur propre d' un endomorphisme , 183 d' une matrice , 183 Vecteur propre d' un endomorphisme , 183 d' une matrice , 183 Vecteurs , 20 indépendants , 34 linéairement dépendants , 34 d' une application linéaire , 55 d' une matrice , 114 Rayon spectral , 239 d' équivalence , 116 réflexive , 116 symétrique , 116 transitive , 116 Scalaires , 20 Second membre ( d' un système linéaire ) , 207 Signature d' une permutation , 152 de sous-espaces vectoriels , 28 d' une famille d' espaces vectoriels , 31 de deux sous-espaces vectoriels , 30 Sous-espace vectoriel , 23 ***engendré*** par une partie , 25 stable , 52 d' un endomorphisme , 183 d' une matrice , 183 Supplémentaire d' un sous-espace vectoriel , 32 Support d' une famille , 26 Symbole de Kronecker , 58 Symétrie , 54 associé , 207 Système linéaire , 78 de Cramer , 140 différentiel , 215 homogène , 78 associé , 78 récurrent , 207 Caractérisation des applications linéaires injectives , 67 surjectives , 67 des automorphismes en dimension finie , 57 des classes de similitude , 270 des endomorphismes diagonalisables , 197 , 229 trigonalisables , 200 , 234 des hyperplans , 72 des matrices équivalentes , 120 Changement de base pour les applications linéaires , 111 pour les vecteurs , 111 Correspondance matrices - blocs et décomposition en somme directe , 144 de co-diagonalisation , 205 de co-trigonalisation , 205 Développement d' un déterminant , 171 Forme de Jordan d' un endomorphisme nilpotent , 249 trigonalisable , 249 Formule de Grassman , 44 Invariants de similitude , 270 Lemme des noyaux , 228 Mise en équation des sous-espaces de codimensions finies , 75 Pivot de Gauss , 127 Rang d' une composée , 56 d' extension linéaire , 66 de Cayley - Hamilton , 233 de factorisation des applications linéaires , 62 de Frobenius , 269 de l' échange , 38 de la base incomplète , 40 des faisceaux d' hyperplans , 73 du rang , 57 du relèvement linéaire , 65 Commandes Wxmaxima binomial , 208 charpoly , 234 columnswap , 279 eigenvalues , 184 , 190 identfor , 175 cspline , 86 length , 136 load(solverec ) , 211 polytocompanion , 270 , 271 randompermutation , 137 setify , 26 solverec , 211 kroneckerproduct , 147 Commandes Python matplotlib , 26 binomial , 208 Dictionnaire , 113 .factorlist , 235 BlockDiagMatrix , 272 BlockMatrix , 148 expand , 186 Function , 192 initsession , 26 interpolate , 82 , 85 interpolatingspline , 86 .charpoly , 235 .cofactormatrix , 176 .extract , 170 .allcoeffs , 271 rsolve , 213 integer , 192 Figure 5.1 Similitude Trajectoires du système :

**52619**: La vérification par le calcul donne , pour les 63 premiers termes ( avec u0 u1 1 ) Application linéaire , 46 Automorphisme , 46 adaptée à une somme directe , 43 ante - duale , 70 d' un espace vectoriel , 35 Codimension , 72 Coefficients d' une matrice , 90 Cofacteur d' une matrice carrée , 170 Comatrice , 170 Combinaison linéaire , 21 Commutant d' un endomorphisme , 248 Condition initiale ( d' un système linéaire ) , 207 Décomposition LU , 180 Dimension d' un espace vectoriel , 39 Drapeau stable , 199 Droite vectorielle , 39 Dual d' un espace vectoriel , 68 Décomposition de Dunford , 247 Décomposition en somme directe , 32 Déterminant d' un endomorphisme , 162 d' une famille de vecteurs , 158 d' une matrice carrée , 160 de Vandermonde , 173 Éléments propres , 184 Endomorphisme , 46 diagonalisable , 196 trigonalisable , 199 Équation d' un hyperplan , 71 Espace caractéristique , 245 Espace propre d' un endomorphisme , 183 d' une matrice , 183 Espace vectoriel , 19 de dimension finie , 38 Exponentielle d' endomorphisme , 240 Famille duale , 69 Fonctions spline , 85 Forme p - linéaire , 154 anti-symétrique , 155 symétrique , 155 Forme de Smith , 273 Forme linéaire , 46 , 68 Groupe symétrique , 151 Hyperplan d' un espace vectoriel , 71 Idéal annulateur , 230 d' une application linéaire , 49 d' une matrice , 114 Interpolation de Lagrange , 80 Isomorphisme , 46 Matrice , 89 anti-symétrique , 106 augmentée , 140 circulante , 198 compagnon , 232 d' un vecteur , 90 d' une application linéaire , 90 d' une famille de vecteurs , 109 de dilatation , 121 de passage , 110 de permutation , 121 de transvection , 121 diagonale , 106 diagonalisable , 196 identité , 100 inversible , 108 symétrique , 106 triangulaire , 106 trigonalisable , 200 semblables , 119 équivalentes , 119 Mineur d' une matrice carrée , 170 Multiplicité d' une valeur propre , 195 d' une application linéaire , 49 d' une matrice , 114 Orthogonal ( direct ) d' une partie , 77 ( indirect ) d' une partie , 77 Partie ou famille génératrice , 25 Partition d' un ensemble , 116 Permutation , 151 Plan vectoriel , 39 caractéristique , 193 d' endomorphisme , 227 de matrice , 227 minimal , 231 d' espaces vectoriels , 22 de deux matrices , 98 de Kronecker , 146 Projecteur , 53 Projection , 52 Trace d' une matrice , 107 Transposition , 152 Transposée d' une matrice , 103 Valeur propre d' un endomorphisme , 183 d' une matrice , 183 Vecteur propre d' un endomorphisme , 183 d' une matrice , 183 Vecteurs , 20 indépendants , 34 linéairement dépendants , 34 d' une application linéaire , 55 d' une matrice , 114 Rayon spectral , 239 d' équivalence , 116 réflexive , 116 symétrique , 116 transitive , 116 Scalaires , 20 Second membre ( d' un système linéaire ) , 207 Signature d' une permutation , 152 de sous-espaces vectoriels , 28 d' une famille d' espaces vectoriels , 31 de deux sous-espaces vectoriels , 30 Sous-espace vectoriel , 23 engendré par une partie , 25 stable , 52 d' un endomorphisme , 183 d' une matrice , 183 Supplémentaire d' un sous-espace vectoriel , 32 Support d' une famille , 26 Symbole de Kronecker , 58 Symétrie , 54 ***associé*** , 207 Système linéaire , 78 de Cramer , 140 différentiel , 215 homogène , 78 associé , 78 récurrent , 207 Caractérisation des applications linéaires injectives , 67 surjectives , 67 des automorphismes en dimension finie , 57 des classes de similitude , 270 des endomorphismes diagonalisables , 197 , 229 trigonalisables , 200 , 234 des hyperplans , 72 des matrices équivalentes , 120 Changement de base pour les applications linéaires , 111 pour les vecteurs , 111 Correspondance matrices - blocs et décomposition en somme directe , 144 de co-diagonalisation , 205 de co-trigonalisation , 205 Développement d' un déterminant , 171 Forme de Jordan d' un endomorphisme nilpotent , 249 trigonalisable , 249 Formule de Grassman , 44 Invariants de similitude , 270 Lemme des noyaux , 228 Mise en équation des sous-espaces de codimensions finies , 75 Pivot de Gauss , 127 Rang d' une composée , 56 d' extension linéaire , 66 de Cayley - Hamilton , 233 de factorisation des applications linéaires , 62 de Frobenius , 269 de l' échange , 38 de la base incomplète , 40 des faisceaux d' hyperplans , 73 du rang , 57 du relèvement linéaire , 65 Commandes Wxmaxima binomial , 208 charpoly , 234 columnswap , 279 eigenvalues , 184 , 190 identfor , 175 cspline , 86 length , 136 load(solverec ) , 211 polytocompanion , 270 , 271 randompermutation , 137 setify , 26 solverec , 211 kroneckerproduct , 147 Commandes Python matplotlib , 26 binomial , 208 Dictionnaire , 113 .factorlist , 235 BlockDiagMatrix , 272 BlockMatrix , 148 expand , 186 Function , 192 initsession , 26 interpolate , 82 , 85 interpolatingspline , 86 .charpoly , 235 .cofactormatrix , 176 .extract , 170 .allcoeffs , 271 rsolve , 213 integer , 192 Figure 5.1 Similitude Trajectoires du système :

**52636**: La vérification par le calcul donne , pour les 63 premiers termes ( avec u0 u1 1 ) Application linéaire , 46 Automorphisme , 46 adaptée à une somme directe , 43 ante - duale , 70 d' un espace vectoriel , 35 Codimension , 72 Coefficients d' une matrice , 90 Cofacteur d' une matrice carrée , 170 Comatrice , 170 Combinaison linéaire , 21 Commutant d' un endomorphisme , 248 Condition initiale ( d' un système linéaire ) , 207 Décomposition LU , 180 Dimension d' un espace vectoriel , 39 Drapeau stable , 199 Droite vectorielle , 39 Dual d' un espace vectoriel , 68 Décomposition de Dunford , 247 Décomposition en somme directe , 32 Déterminant d' un endomorphisme , 162 d' une famille de vecteurs , 158 d' une matrice carrée , 160 de Vandermonde , 173 Éléments propres , 184 Endomorphisme , 46 diagonalisable , 196 trigonalisable , 199 Équation d' un hyperplan , 71 Espace caractéristique , 245 Espace propre d' un endomorphisme , 183 d' une matrice , 183 Espace vectoriel , 19 de dimension finie , 38 Exponentielle d' endomorphisme , 240 Famille duale , 69 Fonctions spline , 85 Forme p - linéaire , 154 anti-symétrique , 155 symétrique , 155 Forme de Smith , 273 Forme linéaire , 46 , 68 Groupe symétrique , 151 Hyperplan d' un espace vectoriel , 71 Idéal annulateur , 230 d' une application linéaire , 49 d' une matrice , 114 Interpolation de Lagrange , 80 Isomorphisme , 46 Matrice , 89 anti-symétrique , 106 augmentée , 140 circulante , 198 compagnon , 232 d' un vecteur , 90 d' une application linéaire , 90 d' une famille de vecteurs , 109 de dilatation , 121 de passage , 110 de permutation , 121 de transvection , 121 diagonale , 106 diagonalisable , 196 identité , 100 inversible , 108 symétrique , 106 triangulaire , 106 trigonalisable , 200 semblables , 119 équivalentes , 119 Mineur d' une matrice carrée , 170 Multiplicité d' une valeur propre , 195 d' une application linéaire , 49 d' une matrice , 114 Orthogonal ( direct ) d' une partie , 77 ( indirect ) d' une partie , 77 Partie ou famille génératrice , 25 Partition d' un ensemble , 116 Permutation , 151 Plan vectoriel , 39 caractéristique , 193 d' endomorphisme , 227 de matrice , 227 minimal , 231 d' espaces vectoriels , 22 de deux matrices , 98 de Kronecker , 146 Projecteur , 53 Projection , 52 Trace d' une matrice , 107 Transposition , 152 Transposée d' une matrice , 103 Valeur propre d' un endomorphisme , 183 d' une matrice , 183 Vecteur propre d' un endomorphisme , 183 d' une matrice , 183 Vecteurs , 20 indépendants , 34 linéairement dépendants , 34 d' une application linéaire , 55 d' une matrice , 114 Rayon spectral , 239 d' équivalence , 116 réflexive , 116 symétrique , 116 transitive , 116 Scalaires , 20 Second membre ( d' un système linéaire ) , 207 Signature d' une permutation , 152 de sous-espaces vectoriels , 28 d' une famille d' espaces vectoriels , 31 de deux sous-espaces vectoriels , 30 Sous-espace vectoriel , 23 engendré par une partie , 25 stable , 52 d' un endomorphisme , 183 d' une matrice , 183 Supplémentaire d' un sous-espace vectoriel , 32 Support d' une famille , 26 Symbole de Kronecker , 58 Symétrie , 54 associé , 207 Système linéaire , 78 de Cramer , 140 différentiel , 215 homogène , 78 ***associé*** , 78 récurrent , 207 Caractérisation des applications linéaires injectives , 67 surjectives , 67 des automorphismes en dimension finie , 57 des classes de similitude , 270 des endomorphismes diagonalisables , 197 , 229 trigonalisables , 200 , 234 des hyperplans , 72 des matrices équivalentes , 120 Changement de base pour les applications linéaires , 111 pour les vecteurs , 111 Correspondance matrices - blocs et décomposition en somme directe , 144 de co-diagonalisation , 205 de co-trigonalisation , 205 Développement d' un déterminant , 171 Forme de Jordan d' un endomorphisme nilpotent , 249 trigonalisable , 249 Formule de Grassman , 44 Invariants de similitude , 270 Lemme des noyaux , 228 Mise en équation des sous-espaces de codimensions finies , 75 Pivot de Gauss , 127 Rang d' une composée , 56 d' extension linéaire , 66 de Cayley - Hamilton , 233 de factorisation des applications linéaires , 62 de Frobenius , 269 de l' échange , 38 de la base incomplète , 40 des faisceaux d' hyperplans , 73 du rang , 57 du relèvement linéaire , 65 Commandes Wxmaxima binomial , 208 charpoly , 234 columnswap , 279 eigenvalues , 184 , 190 identfor , 175 cspline , 86 length , 136 load(solverec ) , 211 polytocompanion , 270 , 271 randompermutation , 137 setify , 26 solverec , 211 kroneckerproduct , 147 Commandes Python matplotlib , 26 binomial , 208 Dictionnaire , 113 .factorlist , 235 BlockDiagMatrix , 272 BlockMatrix , 148 expand , 186 Function , 192 initsession , 26 interpolate , 82 , 85 interpolatingspline , 86 .charpoly , 235 .cofactormatrix , 176 .extract , 170 .allcoeffs , 271 rsolve , 213 integer , 192 Figure 5.1 Similitude Trajectoires du système :