3144

**16**: Algèbre linéaire Alain Chillès ( ) , , , Valentin Vinoles , Adrien Joseph Remerciements Nous ***tenons*** à remercier chaudement tous les professeurs qui nous ont aidés à écrire ce livre , notamment en corrigeant les inévitables fautes

**68**: Préambule à la première version Je ***tiens*** avant toutes choses à remercier ici le professeur pour son aide précieuse lors de la relecture de ce polycopié

**90**: Je ***tiens*** aussi à remercier mon ami Franz Ridde , professeur en MPSI au lycée du Parc de Lyon , qui m' a fourni un grand nombre d' exercices

**122**: Ce livre ne ***est*** pas le cours

**143**: Il servira de support au cours , de guide et permettra , à ceux qui le ***souhaitent*** , d' approfondir quelques sujets

**153**: Il ne s' ***agit*** en aucun cas d' apprendre par cur son contenu

**171**: D' ailleurs , l' apprentissage par cur ***est*** , en général , une mauvaise technique d' apprentissage pour les mathématiques , qui proposent peu de résultats , peu de notions , mais demandent une compréhension profonde de ces notions

**186**: D' ailleurs , l' apprentissage par cur est , en général , une mauvaise technique d' apprentissage pour les mathématiques , qui ***proposent*** peu de résultats , peu de notions , mais demandent une compréhension profonde de ces notions

**196**: D' ailleurs , l' apprentissage par cur est , en général , une mauvaise technique d' apprentissage pour les mathématiques , qui proposent peu de résultats , peu de notions , mais ***demandent*** une compréhension profonde de ces notions

**206**: Le cours ***est*** découpé en quatre parties : espaces vectoriels sur R ou C matrices et systèmes linéaires déterminant réduction des endomorphismes

**274**: ***Signalons*** aussi l' outil de géométrie plane Geogebra et l' excellent Ipe qui permet d' annoter en LATEX 2 les dessins produits directement ou à l' aide d' un autre outil

**287**: Signalons aussi l' outil de géométrie plane Geogebra et l' excellent Ipe qui ***permet*** d' annoter en LATEX 2 les dessins produits directement ou à l' aide d' un autre outil

**484**: Liste des figures Ensembles usuels Notations usuelles Opérations sur les ensembles Signification Ensemble des entiers naturels Ensemble des entiers relatifs Ensemble des entiers naturels ou relatifs non nuls Ensemble des entiers relatifs compris entre p et q Ensemble des rationnels Ensemble des réels Ensemble des rationnels ou des réels strictement positifs Ensemble des complexes Ensemble des rationnels ou des réels ou des complexes non nuls Corps commutatif quelconque ( ***désigne*** souvent R ou C , en ce cas , c' est signalé en début de chapitre ) Ensemble des n - uplets ( si n 2 , on parle de couple et si n 3 , on parle de triplet ) un n - uplet sera parfois noté x au lieu de ( x1 ,

**495**: Liste des figures Ensembles usuels Notations usuelles Opérations sur les ensembles Signification Ensemble des entiers naturels Ensemble des entiers relatifs Ensemble des entiers naturels ou relatifs non nuls Ensemble des entiers relatifs compris entre p et q Ensemble des rationnels Ensemble des réels Ensemble des rationnels ou des réels strictement positifs Ensemble des complexes Ensemble des rationnels ou des réels ou des complexes non nuls Corps commutatif quelconque ( désigne souvent R ou C , en ce cas , c' ***est*** signalé en début de chapitre ) Ensemble des n - uplets ( si n 2 , on parle de couple et si n 3 , on parle de triplet ) un n - uplet sera parfois noté x au lieu de ( x1 ,

**513**: Liste des figures Ensembles usuels Notations usuelles Opérations sur les ensembles Signification Ensemble des entiers naturels Ensemble des entiers relatifs Ensemble des entiers naturels ou relatifs non nuls Ensemble des entiers relatifs compris entre p et q Ensemble des rationnels Ensemble des réels Ensemble des rationnels ou des réels strictement positifs Ensemble des complexes Ensemble des rationnels ou des réels ou des complexes non nuls Corps commutatif quelconque ( désigne souvent R ou C , en ce cas , c' est signalé en début de chapitre ) Ensemble des n - uplets ( si n 2 , on ***parle*** de couple et si n 3 , on parle de triplet ) un n - uplet sera parfois noté x au lieu de ( x1 ,

**522**: Liste des figures Ensembles usuels Notations usuelles Opérations sur les ensembles Signification Ensemble des entiers naturels Ensemble des entiers relatifs Ensemble des entiers naturels ou relatifs non nuls Ensemble des entiers relatifs compris entre p et q Ensemble des rationnels Ensemble des réels Ensemble des rationnels ou des réels strictement positifs Ensemble des complexes Ensemble des rationnels ou des réels ou des complexes non nuls Corps commutatif quelconque ( désigne souvent R ou C , en ce cas , c' est signalé en début de chapitre ) Ensemble des n - uplets ( si n 2 , on parle de couple et si n 3 , on ***parle*** de triplet ) un n - uplet sera parfois noté x au lieu de ( x1 ,

**562**: , xn ) , où , pour tout Ensemble des permutations de l' ensemble E Symbole de Kronecker ( ***vaut*** 1 , si i j et 0 sinon ) Groupe des permutations de 1 , n Signature de la permutation Sn Produit cartésien d' ensembles Produit cartésien des n ensembles E1 ,

**614**: , En Produit cartésien de la famille d' ensembles ( Ei ) iI , où I ***est*** un ensemble quelQ conque les éléments de iI Ei sont notés ( xi ) iI où , pour tout i I , xi Ei Ensemble des Q familles d' éléments de l' ensemble E indexées par l' ensemble F ( correspond à iF Ei où tous les Ei sont égaux à E ) Classe d' équivalence de x pour la relation R Classe(x , R ) Quantificateurs Quantificateurs ( usage ) Quel que soit

**624**: , En Produit cartésien de la famille d' ensembles ( Ei ) iI , où I est un ensemble quelQ conque les éléments de iI Ei ***sont*** notés ( xi ) iI où , pour tout i I , xi Ei Ensemble des Q familles d' éléments de l' ensemble E indexées par l' ensemble F ( correspond à iF Ei où tous les Ei sont égaux à E ) Classe d' équivalence de x pour la relation R Classe(x , R ) Quantificateurs Quantificateurs ( usage ) Quel que soit

**655**: , En Produit cartésien de la famille d' ensembles ( Ei ) iI , où I est un ensemble quelQ conque les éléments de iI Ei sont notés ( xi ) iI où , pour tout i I , xi Ei Ensemble des Q familles d' éléments de l' ensemble E indexées par l' ensemble F ( ***correspond*** à iF Ei où tous les Ei sont égaux à E ) Classe d' équivalence de x pour la relation R Classe(x , R ) Quantificateurs Quantificateurs ( usage ) Quel que soit

**663**: , En Produit cartésien de la famille d' ensembles ( Ei ) iI , où I est un ensemble quelQ conque les éléments de iI Ei sont notés ( xi ) iI où , pour tout i I , xi Ei Ensemble des Q familles d' éléments de l' ensemble E indexées par l' ensemble F ( correspond à iF Ei où tous les Ei ***sont*** égaux à E ) Classe d' équivalence de x pour la relation R Classe(x , R ) Quantificateurs Quantificateurs ( usage ) Quel que soit

**698**: ( ou Pour tout ) Il ***existe*** un unique

**706**: Il ne ***existe*** pas

**711**: ***Signifie*** x E , y E L' écriture x , y E ne est pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions Introduit une nouvelle notation Introduit une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions définies sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues définies sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux définies sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , définies sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle engendrée par x Sous-espace vectoriel de l' espace vectoriel E engendré par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E est un K - espace vectoriel c' est l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients sont les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( correspond à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y a un 1 ( Attention : les dimensions de ces matrices ne sont pas précisées ... ) Transposée de A , où A Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**724**: Signifie x E , y E L' écriture x , y E ne ***est*** pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions Introduit une nouvelle notation Introduit une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions définies sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues définies sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux définies sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , définies sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle engendrée par x Sous-espace vectoriel de l' espace vectoriel E engendré par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E est un K - espace vectoriel c' est l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients sont les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( correspond à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y a un 1 ( Attention : les dimensions de ces matrices ne sont pas précisées ... ) Transposée de A , où A Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**738**: Signifie x E , y E L' écriture x , y E ne est pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions ***Introduit*** une nouvelle notation Introduit une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions définies sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues définies sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux définies sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , définies sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle engendrée par x Sous-espace vectoriel de l' espace vectoriel E engendré par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E est un K - espace vectoriel c' est l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients sont les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( correspond à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y a un 1 ( Attention : les dimensions de ces matrices ne sont pas précisées ... ) Transposée de A , où A Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**742**: Signifie x E , y E L' écriture x , y E ne est pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions Introduit une nouvelle notation ***Introduit*** une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions définies sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues définies sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux définies sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , définies sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle engendrée par x Sous-espace vectoriel de l' espace vectoriel E engendré par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E est un K - espace vectoriel c' est l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients sont les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( correspond à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y a un 1 ( Attention : les dimensions de ces matrices ne sont pas précisées ... ) Transposée de A , où A Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**866**: Signifie x E , y E L' écriture x , y E ne est pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions Introduit une nouvelle notation Introduit une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions définies sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues définies sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux définies sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , définies sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle engendrée par x Sous-espace vectoriel de l' espace vectoriel E engendré par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI ***est*** une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E est un K - espace vectoriel c' est l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients sont les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( correspond à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y a un 1 ( Attention : les dimensions de ces matrices ne sont pas précisées ... ) Transposée de A , où A Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**881**: Signifie x E , y E L' écriture x , y E ne est pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions Introduit une nouvelle notation Introduit une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions définies sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues définies sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux définies sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , définies sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle engendrée par x Sous-espace vectoriel de l' espace vectoriel E engendré par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I ***est*** un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E est un K - espace vectoriel c' est l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients sont les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( correspond à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y a un 1 ( Attention : les dimensions de ces matrices ne sont pas précisées ... ) Transposée de A , où A Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**908**: Signifie x E , y E L' écriture x , y E ne est pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions Introduit une nouvelle notation Introduit une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions définies sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues définies sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux définies sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , définies sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle engendrée par x Sous-espace vectoriel de l' espace vectoriel E engendré par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI ***est*** une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E est un K - espace vectoriel c' est l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients sont les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( correspond à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y a un 1 ( Attention : les dimensions de ces matrices ne sont pas précisées ... ) Transposée de A , où A Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**923**: Signifie x E , y E L' écriture x , y E ne est pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions Introduit une nouvelle notation Introduit une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions définies sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues définies sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux définies sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , définies sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle engendrée par x Sous-espace vectoriel de l' espace vectoriel E engendré par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I ***est*** un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E est un K - espace vectoriel c' est l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients sont les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( correspond à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y a un 1 ( Attention : les dimensions de ces matrices ne sont pas précisées ... ) Transposée de A , où A Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**980**: Signifie x E , y E L' écriture x , y E ne est pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions Introduit une nouvelle notation Introduit une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions définies sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues définies sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux définies sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , définies sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle engendrée par x Sous-espace vectoriel de l' espace vectoriel E engendré par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E ***est*** un K - espace vectoriel c' est l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients sont les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( correspond à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y a un 1 ( Attention : les dimensions de ces matrices ne sont pas précisées ... ) Transposée de A , où A Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**987**: Signifie x E , y E L' écriture x , y E ne est pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions Introduit une nouvelle notation Introduit une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions définies sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues définies sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux définies sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , définies sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle engendrée par x Sous-espace vectoriel de l' espace vectoriel E engendré par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E est un K - espace vectoriel c' ***est*** l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients sont les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( correspond à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y a un 1 ( Attention : les dimensions de ces matrices ne sont pas précisées ... ) Transposée de A , où A Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**1135**: Signifie x E , y E L' écriture x , y E ne est pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions Introduit une nouvelle notation Introduit une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions définies sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues définies sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux définies sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , définies sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle engendrée par x Sous-espace vectoriel de l' espace vectoriel E engendré par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E est un K - espace vectoriel c' est l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients ***sont*** les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( correspond à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y a un 1 ( Attention : les dimensions de ces matrices ne sont pas précisées ... ) Transposée de A , où A Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**1250**: Signifie x E , y E L' écriture x , y E ne est pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions Introduit une nouvelle notation Introduit une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions définies sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues définies sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux définies sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , définies sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle engendrée par x Sous-espace vectoriel de l' espace vectoriel E engendré par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E est un K - espace vectoriel c' est l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients sont les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( ***correspond*** à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y a un 1 ( Attention : les dimensions de ces matrices ne sont pas précisées ... ) Transposée de A , où A Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**1274**: Signifie x E , y E L' écriture x , y E ne est pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions Introduit une nouvelle notation Introduit une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions définies sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues définies sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux définies sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , définies sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle engendrée par x Sous-espace vectoriel de l' espace vectoriel E engendré par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E est un K - espace vectoriel c' est l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients sont les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( correspond à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y ***a*** un 1 ( Attention : les dimensions de ces matrices ne sont pas précisées ... ) Transposée de A , où A Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**1286**: Signifie x E , y E L' écriture x , y E ne est pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions Introduit une nouvelle notation Introduit une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions définies sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues définies sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux définies sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , définies sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle engendrée par x Sous-espace vectoriel de l' espace vectoriel E engendré par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E est un K - espace vectoriel c' est l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients sont les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( correspond à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y a un 1 ( Attention : les dimensions de ces matrices ne ***sont*** pas précisées ... ) Transposée de A , où A Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**1296**: Signifie x E , y E L' écriture x , y E ne est pas correcte ! Opérations Signification Multiplication externe Multiplication des matrices Notations de définitions Introduit une nouvelle notation Introduit une nouvelle définition Espaces vectoriels dim E ou dim(E ) ou dimK E ou Ensemble des fonctions définies sur l' ensemble X à valeurs dans l' ensemble Y Ensemble des fonctions continues définies sur l' intervalle I R à valeurs dans Ensemble des fonctions continues par morceaux définies sur l' intervalle I R à valeurs dans K R ou C Ensemble des fonctions de classe C k où k N , définies sur l' intervalle I R à valeurs dans K R ou C Droite vectorielle engendrée par x Sous-espace vectoriel de l' espace vectoriel E engendré par la partie A Somme des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel E Somme des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque Somme directe des sous-espaces vectoriels E1 et E2 d' un même espace vectoriel Somme directe des Ei , où ( Ei ) iI est une famille de sous-espaces vectoriels d' un même espace vectoriel E et où I est un ensemble quelconque La dimension du K - espace vectoriel E Ensemble des applications linéaires de l' espace vectoriel E dans l' espace vectoriel L ( E , E ) , ensemble des endomorphismes de l' espace vectoriel E Ensemble des automorphismes de l' espace vectoriel E L ( E , K ) , où E est un K - espace vectoriel c' est l' ensemble des formes linéaires sur l' espace vectoriel E ( noter le ? et non pas ) Noyau d' une application linéaire f L ( E , F ) Image d' une application linéaire f L ( E , F ) Application linéaire identité de l' espace vectoriel E Restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel Co-restriction d' une application linéaire f L ( E , F ) à un sous-espace vectoriel F 0 de F contenant Im(f ) Projection sur F parallèlement à G , où E F G Symétrie par rapport à F parallèlement à G Rang de l' application linéaire f Famille duale de la base ( ei ) iI d' un espace vectoriel E Dimension d' un supplémentaire de F Ensemble des solutions du système linéaire S Matrice n p dont les coefficients sont les ( ai , j ) ( i , j)1,n1,p Im(A ) , Ker(A ) , rang(A ) Déterminants Signification Nous utiliserons systématiquement les crochets pour noter les matrices ! Ensemble des matrices à coefficients dans l' ensemble A ayant n lignes et p colonnes Matrice identité de Mp ( K ) Matrice nulle de Mn , p ( K ) Matrice du vecteur x E dans la base E de l' espace vectoriel E Matrice de l' application linéaire f L ( E , E 0 ) dans les bases E de E et E 0 de Matrice de l' endomorphisme f L ( E ) dans la base E de E ( correspond à Matrice ne contenant que des 0 sauf sur la k - ième ligne et la l - ième colonne où il y a un 1 ( Attention : les dimensions de ces matrices ne sont pas précisées ... ) Transposée de A , où ***A*** Mn , p ( K ) Ensemble des matrices symétriques de Mp ( K ) Ensemble des matrices antisymétriques de Mp ( K ) Matrice diagonale Ensemble des matrices diagonales de Mp ( K ) Matrice du système de vecteurs X ( x1 ,

**1398**: , xp ) E p ( p N ) dans la base E de l' espace vectoriel E MatE ( B ) , matrice de passage de E à B Matrice de Mn , p ( K ) ne contenant que des 0 , sauf lorsque i j 1 , r où il y ***a*** Matrice de transvection dans Mp ( K ) ( où p max(k , l ) ) , égale à Ip .Ek , l Matrice de dilatation dans Mp ( K ) ( où p n ) , égale à Ip ( 1).Ek , k Matrice de permutation Groupe des matrices inversibles d' ordre n à coefficients dans K Ensemble des matrices triangulaires supérieures d' ordre n à coefficients dans K Ensemble des matrices triangulaires inférieures d' ordre n à coefficients dans K Trace de la matrice A Image , noyau et rang d' une matrice A Mn , p ( K ) Produit de Kronecker des matrices A et B A semblable à B A équivalente à B Cofacteuri , j ( A ) Déterminant de la matrice Déterminant des vecteurs ( x1 ,

**1737**: , xn ) dans la base E Déterminant d' un endomorphisme u L ( E ) Cofacteur d' indices ( i , j ) de la matrice A Comatrice de A Déterminant de Vandermonde Ensemble des formes p - linéaires de E dans K Ensemble des formes p - linéaires symétriques de E dans K Ensemble des formes p - linéaires antisymétriques de E dans K Réduction des endomorphismes Sp(u ) ou Sp(A ) multu ( ) ou multA ( ) Espace propre de u L ( E ) ou A pour la valeur propre K Spectre de u L ( E ) ou A Polynôme caractéristique de u L ( E ) ou A Multiplicité de dans u ou A Algèbre engendrée par u ou A Signification Polynôme d' endomorphisme ou de matrice Idéal annulateur de u ou A Polynôme minimal de u ou A Matrice compagnon du polynôme P Norme subordonnée de u ou A Espace des endomorphismes continus de E Rayon spectral de u ou A Commutant de u ou A Espace caractéristique de u ou A Chapitre 1 Espaces vectoriels sur R ou C Dans ce chapitre , nous noterons K les corps R ou C. Cela ***signifie*** alors que le résultat énoncé est vrai dans R et dans C. Généralités Premières définitions Soit E un ensemble non vide , on dit que E est un K - espace vectoriel ( ou un espace vectoriel sur K ) si il vérifie les axiomes suivants : 1

**1743**: , xn ) dans la base E Déterminant d' un endomorphisme u L ( E ) Cofacteur d' indices ( i , j ) de la matrice A Comatrice de A Déterminant de Vandermonde Ensemble des formes p - linéaires de E dans K Ensemble des formes p - linéaires symétriques de E dans K Ensemble des formes p - linéaires antisymétriques de E dans K Réduction des endomorphismes Sp(u ) ou Sp(A ) multu ( ) ou multA ( ) Espace propre de u L ( E ) ou A pour la valeur propre K Spectre de u L ( E ) ou A Polynôme caractéristique de u L ( E ) ou A Multiplicité de dans u ou A Algèbre engendrée par u ou A Signification Polynôme d' endomorphisme ou de matrice Idéal annulateur de u ou A Polynôme minimal de u ou A Matrice compagnon du polynôme P Norme subordonnée de u ou A Espace des endomorphismes continus de E Rayon spectral de u ou A Commutant de u ou A Espace caractéristique de u ou A Chapitre 1 Espaces vectoriels sur R ou C Dans ce chapitre , nous noterons K les corps R ou C. Cela signifie alors que le résultat énoncé ***est*** vrai dans R et dans C. Généralités Premières définitions Soit E un ensemble non vide , on dit que E est un K - espace vectoriel ( ou un espace vectoriel sur K ) si il vérifie les axiomes suivants : 1

**1761**: , xn ) dans la base E Déterminant d' un endomorphisme u L ( E ) Cofacteur d' indices ( i , j ) de la matrice A Comatrice de A Déterminant de Vandermonde Ensemble des formes p - linéaires de E dans K Ensemble des formes p - linéaires symétriques de E dans K Ensemble des formes p - linéaires antisymétriques de E dans K Réduction des endomorphismes Sp(u ) ou Sp(A ) multu ( ) ou multA ( ) Espace propre de u L ( E ) ou A pour la valeur propre K Spectre de u L ( E ) ou A Polynôme caractéristique de u L ( E ) ou A Multiplicité de dans u ou A Algèbre engendrée par u ou A Signification Polynôme d' endomorphisme ou de matrice Idéal annulateur de u ou A Polynôme minimal de u ou A Matrice compagnon du polynôme P Norme subordonnée de u ou A Espace des endomorphismes continus de E Rayon spectral de u ou A Commutant de u ou A Espace caractéristique de u ou A Chapitre 1 Espaces vectoriels sur R ou C Dans ce chapitre , nous noterons K les corps R ou C. Cela signifie alors que le résultat énoncé est vrai dans R et dans C. Généralités Premières définitions Soit E un ensemble non vide , on ***dit*** que E est un K - espace vectoriel ( ou un espace vectoriel sur K ) si il vérifie les axiomes suivants : 1

**1764**: , xn ) dans la base E Déterminant d' un endomorphisme u L ( E ) Cofacteur d' indices ( i , j ) de la matrice A Comatrice de A Déterminant de Vandermonde Ensemble des formes p - linéaires de E dans K Ensemble des formes p - linéaires symétriques de E dans K Ensemble des formes p - linéaires antisymétriques de E dans K Réduction des endomorphismes Sp(u ) ou Sp(A ) multu ( ) ou multA ( ) Espace propre de u L ( E ) ou A pour la valeur propre K Spectre de u L ( E ) ou A Polynôme caractéristique de u L ( E ) ou A Multiplicité de dans u ou A Algèbre engendrée par u ou A Signification Polynôme d' endomorphisme ou de matrice Idéal annulateur de u ou A Polynôme minimal de u ou A Matrice compagnon du polynôme P Norme subordonnée de u ou A Espace des endomorphismes continus de E Rayon spectral de u ou A Commutant de u ou A Espace caractéristique de u ou A Chapitre 1 Espaces vectoriels sur R ou C Dans ce chapitre , nous noterons K les corps R ou C. Cela signifie alors que le résultat énoncé est vrai dans R et dans C. Généralités Premières définitions Soit E un ensemble non vide , on dit que E ***est*** un K - espace vectoriel ( ou un espace vectoriel sur K ) si il vérifie les axiomes suivants : 1

**1780**: , xn ) dans la base E Déterminant d' un endomorphisme u L ( E ) Cofacteur d' indices ( i , j ) de la matrice A Comatrice de A Déterminant de Vandermonde Ensemble des formes p - linéaires de E dans K Ensemble des formes p - linéaires symétriques de E dans K Ensemble des formes p - linéaires antisymétriques de E dans K Réduction des endomorphismes Sp(u ) ou Sp(A ) multu ( ) ou multA ( ) Espace propre de u L ( E ) ou A pour la valeur propre K Spectre de u L ( E ) ou A Polynôme caractéristique de u L ( E ) ou A Multiplicité de dans u ou A Algèbre engendrée par u ou A Signification Polynôme d' endomorphisme ou de matrice Idéal annulateur de u ou A Polynôme minimal de u ou A Matrice compagnon du polynôme P Norme subordonnée de u ou A Espace des endomorphismes continus de E Rayon spectral de u ou A Commutant de u ou A Espace caractéristique de u ou A Chapitre 1 Espaces vectoriels sur R ou C Dans ce chapitre , nous noterons K les corps R ou C. Cela signifie alors que le résultat énoncé est vrai dans R et dans C. Généralités Premières définitions Soit E un ensemble non vide , on dit que E est un K - espace vectoriel ( ou un espace vectoriel sur K ) si il ***vérifie*** les axiomes suivants : 1

**1788**: Il ***est*** muni d' une opération interne a , notée et appelée addition qui vérifie : ( a ) est associative : ( b ) possède un élément neutre noté 0E ( ou 0 lorsqu' il ne y a pas d' ambiguïté ) : ( c ) Tout élément de E possède un unique opposé : de plus , on note l' opération : ( d ) est commutative : 2

**1801**: Il est muni d' une opération interne a , notée et appelée addition qui ***vérifie*** : ( a ) est associative : ( b ) possède un élément neutre noté 0E ( ou 0 lorsqu' il ne y a pas d' ambiguïté ) : ( c ) Tout élément de E possède un unique opposé : de plus , on note l' opération : ( d ) est commutative : 2

**1804**: Il est muni d' une opération interne a , notée et appelée addition qui vérifie : ( ***a*** ) est associative : ( b ) possède un élément neutre noté 0E ( ou 0 lorsqu' il ne y a pas d' ambiguïté ) : ( c ) Tout élément de E possède un unique opposé : de plus , on note l' opération : ( d ) est commutative : 2

**1806**: Il est muni d' une opération interne a , notée et appelée addition qui vérifie : ( a ) ***est*** associative : ( b ) possède un élément neutre noté 0E ( ou 0 lorsqu' il ne y a pas d' ambiguïté ) : ( c ) Tout élément de E possède un unique opposé : de plus , on note l' opération : ( d ) est commutative : 2

**1812**: Il est muni d' une opération interne a , notée et appelée addition qui vérifie : ( a ) est associative : ( b ) ***possède*** un élément neutre noté 0E ( ou 0 lorsqu' il ne y a pas d' ambiguïté ) : ( c ) Tout élément de E possède un unique opposé : de plus , on note l' opération : ( d ) est commutative : 2

**1825**: Il est muni d' une opération interne a , notée et appelée addition qui vérifie : ( a ) est associative : ( b ) possède un élément neutre noté 0E ( ou 0 lorsqu' il ne y ***a*** pas d' ambiguïté ) : ( c ) Tout élément de E possède un unique opposé : de plus , on note l' opération : ( d ) est commutative : 2

**1838**: Il est muni d' une opération interne a , notée et appelée addition qui vérifie : ( a ) est associative : ( b ) possède un élément neutre noté 0E ( ou 0 lorsqu' il ne y a pas d' ambiguïté ) : ( c ) Tout élément de E ***possède*** un unique opposé : de plus , on note l' opération : ( d ) est commutative : 2

**1847**: Il est muni d' une opération interne a , notée et appelée addition qui vérifie : ( a ) est associative : ( b ) possède un élément neutre noté 0E ( ou 0 lorsqu' il ne y a pas d' ambiguïté ) : ( c ) Tout élément de E possède un unique opposé : de plus , on ***note*** l' opération : ( d ) est commutative : 2

**1854**: Il est muni d' une opération interne a , notée et appelée addition qui vérifie : ( a ) est associative : ( b ) possède un élément neutre noté 0E ( ou 0 lorsqu' il ne y a pas d' ambiguïté ) : ( c ) Tout élément de E possède un unique opposé : de plus , on note l' opération : ( d ) ***est*** commutative : 2

**1860**: Il ***est*** muni d' une opération externe b , notée

**1877**: et appelée multiplication par un scalaire qui ***vérifie*** : ( a )

**1880**: et appelée multiplication par un scalaire qui vérifie : ( ***a*** )

**1883**: ***est*** distributive par rapport à l' addition de E : ( b )

**1897**: ***est*** distributive par rapport à l' addition de K : ( c )

**1911**: ***est*** distributive par rapport à la multiplication de K : ( d ) L' unité 1 du corps est respectée : Les éléments de E s' appellent alors vecteurs et les éléments de K scalaires

**1929**: est distributive par rapport à la multiplication de K : ( d ) L' unité 1 du corps ***est*** respectée : Les éléments de E s' appellent alors vecteurs et les éléments de K scalaires

**1937**: est distributive par rapport à la multiplication de K : ( d ) L' unité 1 du corps est respectée : Les éléments de E s' ***appellent*** alors vecteurs et les éléments de K scalaires

**1947**: ***a.*** Cela signifie que si x et y sont dans E , alors x y est dans E. b. Cela signifie que si est un scalaire ( il est dans K ) et si x est dans E , alors .x est dans E. Que veut -on faire ? Garder l' essentiel , et éliminer les coordonnées

**1949**: a. Cela ***signifie*** que si x et y sont dans E , alors x y est dans E. b. Cela signifie que si est un scalaire ( il est dans K ) et si x est dans E , alors .x est dans E. Que veut -on faire ? Garder l' essentiel , et éliminer les coordonnées

**1955**: a. Cela signifie que si x et y ***sont*** dans E , alors x y est dans E. b. Cela signifie que si est un scalaire ( il est dans K ) et si x est dans E , alors .x est dans E. Que veut -on faire ? Garder l' essentiel , et éliminer les coordonnées

**1962**: a. Cela signifie que si x et y sont dans E , alors x y ***est*** dans E. b. Cela signifie que si est un scalaire ( il est dans K ) et si x est dans E , alors .x est dans E. Que veut -on faire ? Garder l' essentiel , et éliminer les coordonnées

**1967**: a. Cela signifie que si x et y sont dans E , alors x y est dans E. b. Cela ***signifie*** que si est un scalaire ( il est dans K ) et si x est dans E , alors .x est dans E. Que veut -on faire ? Garder l' essentiel , et éliminer les coordonnées

**1970**: a. Cela signifie que si x et y sont dans E , alors x y est dans E. b. Cela signifie que si ***est*** un scalaire ( il est dans K ) et si x est dans E , alors .x est dans E. Que veut -on faire ? Garder l' essentiel , et éliminer les coordonnées

**1975**: a. Cela signifie que si x et y sont dans E , alors x y est dans E. b. Cela signifie que si est un scalaire ( il ***est*** dans K ) et si x est dans E , alors .x est dans E. Que veut -on faire ? Garder l' essentiel , et éliminer les coordonnées

**1982**: a. Cela signifie que si x et y sont dans E , alors x y est dans E. b. Cela signifie que si est un scalaire ( il est dans K ) et si x ***est*** dans E , alors .x est dans E. Que veut -on faire ? Garder l' essentiel , et éliminer les coordonnées

**1988**: a. Cela signifie que si x et y sont dans E , alors x y est dans E. b. Cela signifie que si est un scalaire ( il est dans K ) et si x est dans E , alors .x ***est*** dans E. Que veut -on faire ? Garder l' essentiel , et éliminer les coordonnées

**1992**: a. Cela signifie que si x et y sont dans E , alors x y est dans E. b. Cela signifie que si est un scalaire ( il est dans K ) et si x est dans E , alors .x est dans E. Que ***veut*** -on faire ? Garder l' essentiel , et éliminer les coordonnées

**2051**: Le corps K ***est*** un K - espace vectoriel

**2062**: Si I ***est*** un intervalle de R et si k N , alors les ensembles suivants sont des K - espaces vectoriels : f : I K , de classe C k 3

**2076**: Si I est un intervalle de R et si k N , alors les ensembles suivants ***sont*** des K - espaces vectoriels : f : I K , de classe C k 3

**2100**: De même , l' ensemble suivant ***est*** un K - espace vectoriel : ( I , K ) f : I K , continue par morceaux 4

**2117**: De même , l' ensemble suivant est un K - espace vectoriel : ( I , K ) f : I K , ***continue*** par morceaux 4

**2131**: L' ensemble des fonctions polynomiales à coefficients dans K ***est*** un K - espace vectoriel de même que les fonctions rationnelles à coefficients dans K. 5

**2154**: Tout C - espace vectoriel ***est*** un R - espace vectoriel ( la réciproque est fausse )

**2163**: Tout C - espace vectoriel est un R - espace vectoriel ( la réciproque ***est*** fausse )

**2167**: ***Figure*** 1.2 Produit d' un vecteur par un scalaire 6

**2180**: Si X ***est*** un ensemble et si E est un K - espace vectoriel , alors F ( X , E ) f : X E est un K - espace vectoriel Soit E un K - espace vectoriel

**2186**: Si X est un ensemble et si E ***est*** un K - espace vectoriel , alors F ( X , E ) f : X E est un K - espace vectoriel Soit E un K - espace vectoriel

**2204**: Si X est un ensemble et si E est un K - espace vectoriel , alors F ( X , E ) f : X E ***est*** un K - espace vectoriel Soit E un K - espace vectoriel

**2242**: , xn ) n vecteurs de E , on ***appelle*** combinaison linéaire de ces vecteurs toute expression de la Combinaison linéaire d' un nombre quelconque de vecteurs Plus généralement , si on a un nombre quelconque de vecteurs de E , ( xi ) iI , on appelle combinaison linéaire de ces vecteurs , toute combinaison linéaire d' une sous-famille finie ( xi1 ,

**2265**: , xn ) n vecteurs de E , on appelle combinaison linéaire de ces vecteurs toute expression de la Combinaison linéaire d' un nombre quelconque de vecteurs Plus généralement , si on ***a*** un nombre quelconque de vecteurs de E , ( xi ) iI , on appelle combinaison linéaire de ces vecteurs , toute combinaison linéaire d' une sous-famille finie ( xi1 ,

**2280**: , xn ) n vecteurs de E , on appelle combinaison linéaire de ces vecteurs toute expression de la Combinaison linéaire d' un nombre quelconque de vecteurs Plus généralement , si on a un nombre quelconque de vecteurs de E , ( xi ) iI , on ***appelle*** combinaison linéaire de ces vecteurs , toute combinaison linéaire d' une sous-famille finie ( xi1 ,

**2310**: Dans R2 , tout vecteur ***est*** combinaison linéaire de 2

**2329**: Dans C considéré comme un R - espace vectoriel , tout nombre complexe ***est*** combinaison linéaire de 1 et i , mais aussi de 1 et exp(2 i 3 ) j , ou de i et j , ou de 1 , i et j , etc. 3

**2378**: Dans le K - espace vectoriel des fonctions polynomiales , toute fonction polynomiale ***est*** combinaison linéaire de la famille : On ne sait faire que des sommes finies de vecteurs ! D' où la présence du n. Lorsque n 0 , on obtient ( par convention ) 0E

**2387**: Dans le K - espace vectoriel des fonctions polynomiales , toute fonction polynomiale est combinaison linéaire de la famille : On ne ***sait*** faire que des sommes finies de vecteurs ! D' où la présence du n. Lorsque n 0 , on obtient ( par convention ) 0E

**2407**: Dans le K - espace vectoriel des fonctions polynomiales , toute fonction polynomiale est combinaison linéaire de la famille : On ne sait faire que des sommes finies de vecteurs ! D' où la présence du n. Lorsque n 0 , on ***obtient*** ( par convention ) 0E

**2435**: Produit fini d' espaces vectoriels Soit E1 , ... , En des K - espaces vectoriels , l' ensemble E1 En ***est*** alors muni d' une structure de K - espace vectoriel définie par ( avec des notations évidentes ) : Produit quelconque d' espaces vectoriels Si E est un K - espace vectoriel et I un ensemble ( d' indices ) , alors , de même , E I est muni d' une structure de K - espace vectoriel définie par ( avec des notations évidentes ) : 1

**2462**: Produit fini d' espaces vectoriels Soit E1 , ... , En des K - espaces vectoriels , l' ensemble E1 En est alors muni d' une structure de K - espace vectoriel définie par ( avec des notations évidentes ) : Produit quelconque d' espaces vectoriels Si E ***est*** un K - espace vectoriel et I un ensemble ( d' indices ) , alors , de même , E I est muni d' une structure de K - espace vectoriel définie par ( avec des notations évidentes ) : 1

**2484**: Produit fini d' espaces vectoriels Soit E1 , ... , En des K - espaces vectoriels , l' ensemble E1 En est alors muni d' une structure de K - espace vectoriel définie par ( avec des notations évidentes ) : Produit quelconque d' espaces vectoriels Si E est un K - espace vectoriel et I un ensemble ( d' indices ) , alors , de même , E I ***est*** muni d' une structure de K - espace vectoriel définie par ( avec des notations évidentes ) : 1

**2518**: Kn ( l' ensemble des n - uplets à coefficients dans K ) ***est*** un K - espace vectoriel

**2539**: KN ( l' ensemble des suites de K indexées par N ) ***est*** un K - espace vectoriel

**2560**: 1.1.1 Démontrer que , dans tout K - espace vectoriel , les relations suivantes ***sont*** vérifiées : 1.1.2 Parmi les ensembles suivants , lesquels , munis des opérations usuelles , sont des R - espaces vectoriels ? Sous-espaces vectoriels Soit E un K - espace vectoriel et F E , on dit que F est un sous-espace vectoriel de E si : F est stable par ( c' est - à - dire que pour tout ( x , y ) F 2 , x y F ) F est stable par

**2576**: 1.1.1 Démontrer que , dans tout K - espace vectoriel , les relations suivantes sont vérifiées : 1.1.2 Parmi les ensembles suivants , lesquels , munis des opérations usuelles , ***sont*** des R - espaces vectoriels ? Sous-espaces vectoriels Soit E un K - espace vectoriel et F E , on dit que F est un sous-espace vectoriel de E si : F est stable par ( c' est - à - dire que pour tout ( x , y ) F 2 , x y F ) F est stable par

**2597**: 1.1.1 Démontrer que , dans tout K - espace vectoriel , les relations suivantes sont vérifiées : 1.1.2 Parmi les ensembles suivants , lesquels , munis des opérations usuelles , sont des R - espaces vectoriels ? Sous-espaces vectoriels Soit E un K - espace vectoriel et F E , on ***dit*** que F est un sous-espace vectoriel de E si : F est stable par ( c' est - à - dire que pour tout ( x , y ) F 2 , x y F ) F est stable par

**2600**: 1.1.1 Démontrer que , dans tout K - espace vectoriel , les relations suivantes sont vérifiées : 1.1.2 Parmi les ensembles suivants , lesquels , munis des opérations usuelles , sont des R - espaces vectoriels ? Sous-espaces vectoriels Soit E un K - espace vectoriel et F E , on dit que F ***est*** un sous-espace vectoriel de E si : F est stable par ( c' est - à - dire que pour tout ( x , y ) F 2 , x y F ) F est stable par

**2609**: 1.1.1 Démontrer que , dans tout K - espace vectoriel , les relations suivantes sont vérifiées : 1.1.2 Parmi les ensembles suivants , lesquels , munis des opérations usuelles , sont des R - espaces vectoriels ? Sous-espaces vectoriels Soit E un K - espace vectoriel et F E , on dit que F est un sous-espace vectoriel de E si : F ***est*** stable par ( c' est - à - dire que pour tout ( x , y ) F 2 , x y F ) F est stable par

**2635**: 1.1.1 Démontrer que , dans tout K - espace vectoriel , les relations suivantes sont vérifiées : 1.1.2 Parmi les ensembles suivants , lesquels , munis des opérations usuelles , sont des R - espaces vectoriels ? Sous-espaces vectoriels Soit E un K - espace vectoriel et F E , on dit que F est un sous-espace vectoriel de E si : F est stable par ( c' est - à - dire que pour tout ( x , y ) F 2 , x y F ) F ***est*** stable par

**2689**: La droite vectorielle dirigée par x , notée K.x , ***est*** le sous-ensemble de E défini par Les droites vectorielles sont des sous-espaces vectoriels de E. 2

**2699**: La droite vectorielle dirigée par x , notée K.x , est le sous-ensemble de E défini par Les droites vectorielles ***sont*** des sous-espaces vectoriels de E. 2

**2716**: L' ensemble des fonctions polynomiales de R dans K ***est*** un sous-espace vectoriel de C 0 ( R , K )

**2750**: De même , pour I intervalle de R et p N , C p1 ( I , K ) ***est*** un sous-espace vectoriel de C p ( I , K )

**2774**: De plus , C ( I , K ) ***est*** un sous-espace vectoriel de C p ( I , K )

**2793**: Tout K - espace vectoriel E ***admet*** toujours les sous-espaces vectoriels ( dits triviaux ) E et 0E

**2808**: Démonstration On ***démontre*** que E et 0E vérifient les trois axiomes de la définition 1.4 , de la présente page

**2813**: Démonstration On démontre que E et 0E ***vérifient*** les trois axiomes de la définition 1.4 , de la présente page

**2844**: Alors F ***est*** un sous-espace vectoriel de E si , et seulement si , F muni des mêmes opérations et

**2863**: ***est*** un K - espace vectoriel

**2880**: Démonstration Supposons que F soit sous-espace vectoriel de E. Alors ***est*** une opération interne sur F F ( car F est stable par ) , qui est associative ( car elle l' est sur E F ) , possède un élément neutre 0E F , tout élément de F possède un unique opposé dans F ( si x F , x ( 1).x F car F est stable par

**2890**: Démonstration Supposons que F soit sous-espace vectoriel de E. Alors est une opération interne sur F F ( car F ***est*** stable par ) , qui est associative ( car elle l' est sur E F ) , possède un élément neutre 0E F , tout élément de F possède un unique opposé dans F ( si x F , x ( 1).x F car F est stable par

**2896**: Démonstration Supposons que F soit sous-espace vectoriel de E. Alors est une opération interne sur F F ( car F est stable par ) , qui ***est*** associative ( car elle l' est sur E F ) , possède un élément neutre 0E F , tout élément de F possède un unique opposé dans F ( si x F , x ( 1).x F car F est stable par

**2902**: Démonstration Supposons que F soit sous-espace vectoriel de E. Alors est une opération interne sur F F ( car F est stable par ) , qui est associative ( car elle l' ***est*** sur E F ) , possède un élément neutre 0E F , tout élément de F possède un unique opposé dans F ( si x F , x ( 1).x F car F est stable par

**2908**: Démonstration Supposons que F soit sous-espace vectoriel de E. Alors est une opération interne sur F F ( car F est stable par ) , qui est associative ( car elle l' est sur E F ) , ***possède*** un élément neutre 0E F , tout élément de F possède un unique opposé dans F ( si x F , x ( 1).x F car F est stable par

**2919**: Démonstration Supposons que F soit sous-espace vectoriel de E. Alors est une opération interne sur F F ( car F est stable par ) , qui est associative ( car elle l' est sur E F ) , possède un élément neutre 0E F , tout élément de F ***possède*** un unique opposé dans F ( si x F , x ( 1).x F car F est stable par

**2936**: Démonstration Supposons que F soit sous-espace vectoriel de E. Alors est une opération interne sur F F ( car F est stable par ) , qui est associative ( car elle l' est sur E F ) , possède un élément neutre 0E F , tout élément de F possède un unique opposé dans F ( si x F , x ( 1).x F car F ***est*** stable par

**2942**: ) et ***est*** commutative ( car elle l' est sur E F )

**2948**: ) et est commutative ( car elle l' ***est*** sur E F )

**2958**: ***est*** une opération externe sur K F ( car F est stable par

**2968**: est une opération externe sur K F ( car F ***est*** stable par

**2974**: ) , ***est*** distributive par rapport à l' addition de F ainsi que l' addition et la multiplication de K et l' unité du corps est respectée ( car ces propriétés sont vraies sur E F )

**2997**: ) , est distributive par rapport à l' addition de F ainsi que l' addition et la multiplication de K et l' unité du corps ***est*** respectée ( car ces propriétés sont vraies sur E F )

**3003**: ) , est distributive par rapport à l' addition de F ainsi que l' addition et la multiplication de K et l' unité du corps est respectée ( car ces propriétés ***sont*** vraies sur E F )

**3018**: ***est*** un K - espace vectoriel

**3034**: ***est*** un K - espace vectoriel , alors il est immédiat que F vérifie les trois axiomes de la définition 1.4 , de la présente page

**3043**: est un K - espace vectoriel , alors il ***est*** immédiat que F vérifie les trois axiomes de la définition 1.4 , de la présente page

**3047**: est un K - espace vectoriel , alors il est immédiat que F ***vérifie*** les trois axiomes de la définition 1.4 , de la présente page

**3066**: Pour démontrer qu' un ensemble ***est*** un K - espace vectoriel , il est très souvent plus simple et plus rapide de démontrer que c' est un sous-espace vectoriel d' un K - espace vectoriel déjà connu

**3074**: Pour démontrer qu' un ensemble est un K - espace vectoriel , il ***est*** très souvent plus simple et plus rapide de démontrer que c' est un sous-espace vectoriel d' un K - espace vectoriel déjà connu

**3086**: Pour démontrer qu' un ensemble est un K - espace vectoriel , il est très souvent plus simple et plus rapide de démontrer que c' ***est*** un sous-espace vectoriel d' un K - espace vectoriel déjà connu

**3116**: Alors F ***est*** un sous-espace vectoriel de E si , et seulement si , F est stable par combinaisons linéaires

**3129**: Alors F est un sous-espace vectoriel de E si , et seulement si , F ***est*** stable par combinaisons linéaires

**3159**: , xn des vecteurs de F ( qui ***existent*** car F 6 ) et soit ( 1 ,

**3189**: Alors k .xk F pour tout k 1 , n car F ***est*** stable par

**3202**: et par une récurrence immédiate sur n , on ***obtient*** donc F est stable par combinaisons linéaires

**3205**: et par une récurrence immédiate sur n , on obtient donc F ***est*** stable par combinaisons linéaires

**3213**: Réciproquement , ***supposons*** que F soit stable par combinaisons linéaires

**3223**: Il ***est*** alors immédiat que F est stable par et par

**3228**: Il est alors immédiat que F ***est*** stable par et par

**3250**: De plus , puisque F 6 , il ***existe*** x F

**3255**: On ***a*** alors Finalement , F est un sous-espace vectoriel de E. Soit E un K - espace vectoriel

**3260**: On a alors Finalement , F ***est*** un sous-espace vectoriel de E. Soit E un K - espace vectoriel

**3282**: Stabilité par intersection finie Si F1 et F2 ***sont*** deux sous-espaces vectoriels de E , alors F1 F2 est un sous-espace vectoriel de E. Stabilité par intersection quelconque Plus généralement , si ( Fi ) iI est une famille de sous-espaces vectoriels de E , alors Fi est un sous-espace vectoriel de E Démonstration On a 0E F car 0E Fi pour tout i I Soit ( x , y ) F 2

**3292**: Stabilité par intersection finie Si F1 et F2 sont deux sous-espaces vectoriels de E , alors F1 F2 ***est*** un sous-espace vectoriel de E. Stabilité par intersection quelconque Plus généralement , si ( Fi ) iI est une famille de sous-espaces vectoriels de E , alors Fi est un sous-espace vectoriel de E Démonstration On a 0E F car 0E Fi pour tout i I Soit ( x , y ) F 2

**3310**: Stabilité par intersection finie Si F1 et F2 sont deux sous-espaces vectoriels de E , alors F1 F2 est un sous-espace vectoriel de E. Stabilité par intersection quelconque Plus généralement , si ( Fi ) iI ***est*** une famille de sous-espaces vectoriels de E , alors Fi est un sous-espace vectoriel de E Démonstration On a 0E F car 0E Fi pour tout i I Soit ( x , y ) F 2

**3321**: Stabilité par intersection finie Si F1 et F2 sont deux sous-espaces vectoriels de E , alors F1 F2 est un sous-espace vectoriel de E. Stabilité par intersection quelconque Plus généralement , si ( Fi ) iI est une famille de sous-espaces vectoriels de E , alors Fi ***est*** un sous-espace vectoriel de E Démonstration On a 0E F car 0E Fi pour tout i I Soit ( x , y ) F 2

**3329**: Stabilité par intersection finie Si F1 et F2 sont deux sous-espaces vectoriels de E , alors F1 F2 est un sous-espace vectoriel de E. Stabilité par intersection quelconque Plus généralement , si ( Fi ) iI est une famille de sous-espaces vectoriels de E , alors Fi est un sous-espace vectoriel de E Démonstration On ***a*** 0E F car 0E Fi pour tout i I Soit ( x , y ) F 2

**3356**: En particulier , pour tout i I on ***a*** x Fi et y Fi donc x y Fi pour tout i I ( car les Fi sont stables par )

**3374**: En particulier , pour tout i I on a x Fi et y Fi donc x y Fi pour tout i I ( car les Fi ***sont*** stables par )

**3380**: On ***a*** donc x y F

**3400**: Soit x F et soit K. En particulier , pour tout i I on ***a*** x Fi donc .x Fi pour tout i I ( car les Fi sont stables par

**3414**: Soit x F et soit K. En particulier , pour tout i I on a x Fi donc .x Fi pour tout i I ( car les Fi ***sont*** stables par

**3421**: On ***a*** donc .x F

**3429**: Finalement , F ***est*** un sous-espace vectoriel de E. Si F1 et F2 sont deux sous-espaces vectoriels d' un K - espace vectoriel E , alors F1 F2 ne est jamais un sous-espace vectoriel de E sauf si F1 F2 ou F1 F2

**3439**: Finalement , F est un sous-espace vectoriel de E. Si F1 et F2 ***sont*** deux sous-espaces vectoriels d' un K - espace vectoriel E , alors F1 F2 ne est jamais un sous-espace vectoriel de E sauf si F1 F2 ou F1 F2

**3455**: Finalement , F est un sous-espace vectoriel de E. Si F1 et F2 sont deux sous-espaces vectoriels d' un K - espace vectoriel E , alors F1 F2 ne ***est*** jamais un sous-espace vectoriel de E sauf si F1 F2 ou F1 F2

**3498**: On ***a*** ne est pas un sous-espace vectoriel de R2 ( il ne est pas stable par addition )

**3500**: On a ne ***est*** pas un sous-espace vectoriel de R2 ( il ne est pas stable par addition )

**3510**: On a ne est pas un sous-espace vectoriel de R2 ( il ne ***est*** pas stable par addition )

**3530**: Soit E un K - espace vectoriel et soit A E , on ***appelle*** sous-espace vectoriel engendré par A le plus petit sous-espace vectoriel de E contenant A. On le note : Démonstration que la notion de sous-espace vectoriel engendré par une partie est bien définie L' existence d' un plus petit sous-espace vectoriel contenant A mérite une justification

**3560**: Soit E un K - espace vectoriel et soit A E , on appelle sous-espace vectoriel engendré par A le plus petit sous-espace vectoriel de E contenant A. On le note : Démonstration que la notion de sous-espace vectoriel engendré par une partie ***est*** bien définie L' existence d' un plus petit sous-espace vectoriel contenant A mérite une justification

**3598**: F F sous-espace vectoriel de E , F A et Vect(A ) Alors Vect(A ) 6 car E F et c' ***est*** bien un sous-espace vectoriel de E d' après la propriété 1.4 , page ci - contre

**3625**: Par définition , tout sous-espace vectoriel de E qui ***contient*** A contient aussi Vect(A ) , c' est donc bien le petit sous-espace vectoriel de E contenant A. Soit E un K - espace vectoriel

**3627**: Par définition , tout sous-espace vectoriel de E qui contient A ***contient*** aussi Vect(A ) , c' est donc bien le petit sous-espace vectoriel de E contenant A. Soit E un K - espace vectoriel

**3633**: Par définition , tout sous-espace vectoriel de E qui contient A contient aussi Vect(A ) , c' ***est*** donc bien le petit sous-espace vectoriel de E contenant A. Soit E un K - espace vectoriel

**3656**: Si F ***est*** un sous-espace vectoriel de E , alors Vect(F ) F

**3686**: Si E 6 0E et si x E , x 6 0E , alors Vect(x ) ***est*** la droite vectorielle engendrée par x : Proposition 1.1 Si E est un K - espace vectoriel et si A E , A 6 , alors Vect(A ) est l' ensemble des combinaisons linéaires construites à partir des vecteurs de A : Vect(A ) A Vect(A ) A Démonstration ( ) L' ensemble des combinaisons linéaires de vecteurs de A est un sous-espace vectoriel de E qui contient A , il contient donc Vect(A )

**3698**: Si E 6 0E et si x E , x 6 0E , alors Vect(x ) est la droite vectorielle engendrée par x : Proposition 1.1 Si E ***est*** un K - espace vectoriel et si A E , A 6 , alors Vect(A ) est l' ensemble des combinaisons linéaires construites à partir des vecteurs de A : Vect(A ) A Vect(A ) A Démonstration ( ) L' ensemble des combinaisons linéaires de vecteurs de A est un sous-espace vectoriel de E qui contient A , il contient donc Vect(A )

**3706**: Si E 6 0E et si x E , x 6 0E , alors Vect(x ) est la droite vectorielle engendrée par x : Proposition 1.1 Si E est un K - espace vectoriel et si ***A*** E , A 6 , alors Vect(A ) est l' ensemble des combinaisons linéaires construites à partir des vecteurs de A : Vect(A ) A Vect(A ) A Démonstration ( ) L' ensemble des combinaisons linéaires de vecteurs de A est un sous-espace vectoriel de E qui contient A , il contient donc Vect(A )

**3715**: Si E 6 0E et si x E , x 6 0E , alors Vect(x ) est la droite vectorielle engendrée par x : Proposition 1.1 Si E est un K - espace vectoriel et si A E , A 6 , alors Vect(A ) ***est*** l' ensemble des combinaisons linéaires construites à partir des vecteurs de A : Vect(A ) A Vect(A ) A Démonstration ( ) L' ensemble des combinaisons linéaires de vecteurs de A est un sous-espace vectoriel de E qui contient A , il contient donc Vect(A )

**3747**: Si E 6 0E et si x E , x 6 0E , alors Vect(x ) est la droite vectorielle engendrée par x : Proposition 1.1 Si E est un K - espace vectoriel et si A E , A 6 , alors Vect(A ) est l' ensemble des combinaisons linéaires construites à partir des vecteurs de A : Vect(A ) A Vect(A ) A Démonstration ( ) L' ensemble des combinaisons linéaires de vecteurs de A ***est*** un sous-espace vectoriel de E qui contient A , il contient donc Vect(A )

**3754**: Si E 6 0E et si x E , x 6 0E , alors Vect(x ) est la droite vectorielle engendrée par x : Proposition 1.1 Si E est un K - espace vectoriel et si A E , A 6 , alors Vect(A ) est l' ensemble des combinaisons linéaires construites à partir des vecteurs de A : Vect(A ) A Vect(A ) A Démonstration ( ) L' ensemble des combinaisons linéaires de vecteurs de A est un sous-espace vectoriel de E qui ***contient*** A , il contient donc Vect(A )

**3758**: Si E 6 0E et si x E , x 6 0E , alors Vect(x ) est la droite vectorielle engendrée par x : Proposition 1.1 Si E est un K - espace vectoriel et si A E , A 6 , alors Vect(A ) est l' ensemble des combinaisons linéaires construites à partir des vecteurs de A : Vect(A ) A Vect(A ) A Démonstration ( ) L' ensemble des combinaisons linéaires de vecteurs de A est un sous-espace vectoriel de E qui contient A , il ***contient*** donc Vect(A )

**3766**: ( ) A ***est*** inclus dans Vect(A ) ( par définition ) qui est stable par combinaisons linéaires , donc Vect(A ) contient toutes les combinaisons linéaires de vecteurs de A. Soit E un K - espace vectoriel

**3767**: ( ) A est ***inclus*** dans Vect(A ) ( par définition ) qui est stable par combinaisons linéaires , donc Vect(A ) contient toutes les combinaisons linéaires de vecteurs de A. Soit E un K - espace vectoriel

**3776**: ( ) A est inclus dans Vect(A ) ( par définition ) qui ***est*** stable par combinaisons linéaires , donc Vect(A ) contient toutes les combinaisons linéaires de vecteurs de A. Soit E un K - espace vectoriel

**3785**: ( ) A est inclus dans Vect(A ) ( par définition ) qui est stable par combinaisons linéaires , donc Vect(A ) ***contient*** toutes les combinaisons linéaires de vecteurs de A. Soit E un K - espace vectoriel

**3809**: Partie génératrice Soit A E , on ***dit*** que A est une partie génératrice de E si Vect(A ) E Famille génératrice Soit ( ai ) iI E I une famille de vecteurs de E , on dit que c' est une famille génératrice de E si Quelle différence y a -t -il entre la famille ( ai ) iI et ai , i I ? Dans un ensemble , les termes ne apparaissent qu' une seule fois , alors que dans une famille , il est possible de les répéter

**3812**: Partie génératrice Soit A E , on dit que A ***est*** une partie génératrice de E si Vect(A ) E Famille génératrice Soit ( ai ) iI E I une famille de vecteurs de E , on dit que c' est une famille génératrice de E si Quelle différence y a -t -il entre la famille ( ai ) iI et ai , i I ? Dans un ensemble , les termes ne apparaissent qu' une seule fois , alors que dans une famille , il est possible de les répéter

**3839**: Partie génératrice Soit A E , on dit que A est une partie génératrice de E si Vect(A ) E Famille génératrice Soit ( ai ) iI E I une famille de vecteurs de E , on ***dit*** que c' est une famille génératrice de E si Quelle différence y a -t -il entre la famille ( ai ) iI et ai , i I ? Dans un ensemble , les termes ne apparaissent qu' une seule fois , alors que dans une famille , il est possible de les répéter

**3842**: Partie génératrice Soit A E , on dit que A est une partie génératrice de E si Vect(A ) E Famille génératrice Soit ( ai ) iI E I une famille de vecteurs de E , on dit que c' ***est*** une famille génératrice de E si Quelle différence y a -t -il entre la famille ( ai ) iI et ai , i I ? Dans un ensemble , les termes ne apparaissent qu' une seule fois , alors que dans une famille , il est possible de les répéter

**3852**: Partie génératrice Soit A E , on dit que A est une partie génératrice de E si Vect(A ) E Famille génératrice Soit ( ai ) iI E I une famille de vecteurs de E , on dit que c' est une famille génératrice de E si Quelle différence y ***a*** -t -il entre la famille ( ai ) iI et ai , i I ? Dans un ensemble , les termes ne apparaissent qu' une seule fois , alors que dans une famille , il est possible de les répéter

**3875**: Partie génératrice Soit A E , on dit que A est une partie génératrice de E si Vect(A ) E Famille génératrice Soit ( ai ) iI E I une famille de vecteurs de E , on dit que c' est une famille génératrice de E si Quelle différence y a -t -il entre la famille ( ai ) iI et ai , i I ? Dans un ensemble , les termes ne ***apparaissent*** qu' une seule fois , alors que dans une famille , il est possible de les répéter

**3888**: Partie génératrice Soit A E , on dit que A est une partie génératrice de E si Vect(A ) E Famille génératrice Soit ( ai ) iI E I une famille de vecteurs de E , on dit que c' est une famille génératrice de E si Quelle différence y a -t -il entre la famille ( ai ) iI et ai , i I ? Dans un ensemble , les termes ne apparaissent qu' une seule fois , alors que dans une famille , il ***est*** possible de les répéter

**3895**: C' ***est*** la même différence qu' entre une liste et un ensemble en informatique ! a Voir le code 1.1 , de la présente a. L' ensemble ai , i I associé à la famille ( ai ) iI s' appelle le support de la famille

**3909**: C' est la même différence qu' entre une liste et un ensemble en informatique ! ***a*** Voir le code 1.1 , de la présente a. L' ensemble ai , i I associé à la famille ( ai ) iI s' appelle le support de la famille

**3918**: C' est la même différence qu' entre une liste et un ensemble en informatique ! a Voir le code 1.1 , de la présente ***a.*** L' ensemble ai , i I associé à la famille ( ai ) iI s' appelle le support de la famille

**3934**: C' est la même différence qu' entre une liste et un ensemble en informatique ! a Voir le code 1.1 , de la présente a. L' ensemble ai , i I associé à la famille ( ai ) iI s' ***appelle*** le support de la famille

**3968**: ( 1 , i ) , ( 1 , j ) , ( i , j ) , ( 1 , i , j ) ***sont*** des familles génératrices de C vu comme un R - espace vectoriel

**3991**: La base canonique ( ei ) i1,n ***est*** une famille génératrice de Rn et de Cn , où pour tout i 1 , n , i - ième coordonnée 3

**4023**: Plus généralement , si ( ei ) iI ***est*** une famille génératrice d' un K - espace vectoriel E et si ( fj ) jJ est une famille génératrice d' un K - espace vectoriel F , alors la partie ( ei , 0F ) , i I ( 0E , fj ) , j J est une partie génératrice de E F 1.2.1 Démontrer que est une partie génératrice de K3

**4040**: Plus généralement , si ( ei ) iI est une famille génératrice d' un K - espace vectoriel E et si ( fj ) jJ ***est*** une famille génératrice d' un K - espace vectoriel F , alors la partie ( ei , 0F ) , i I ( 0E , fj ) , j J est une partie génératrice de E F 1.2.1 Démontrer que est une partie génératrice de K3

**4071**: Plus généralement , si ( ei ) iI est une famille génératrice d' un K - espace vectoriel E et si ( fj ) jJ est une famille génératrice d' un K - espace vectoriel F , alors la partie ( ei , 0F ) , i I ( 0E , fj ) , j J ***est*** une partie génératrice de E F 1.2.1 Démontrer que est une partie génératrice de K3

**4081**: Plus généralement , si ( ei ) iI est une famille génératrice d' un K - espace vectoriel E et si ( fj ) jJ est une famille génératrice d' un K - espace vectoriel F , alors la partie ( ei , 0F ) , i I ( 0E , fj ) , j J est une partie génératrice de E F 1.2.1 Démontrer que ***est*** une partie génératrice de K3

**4117**: 1.2.2 Déterminer une partie génératrice simple du plan de K3 d' équation 1.2.3 Déterminer une famille génératrice simple du plan vectoriel de K4 , d' équations : 1.2.4 Soit ***a*** R , démontrer que la famille ( x 7 ( x a)n ) nN est une famille génératrice du R - espace vectoriel des fonctions polynomiales

**4132**: 1.2.2 Déterminer une partie génératrice simple du plan de K3 d' équation 1.2.3 Déterminer une famille génératrice simple du plan vectoriel de K4 , d' équations : 1.2.4 Soit a R , démontrer que la famille ( x 7 ( x a)n ) nN ***est*** une famille génératrice du R - espace vectoriel des fonctions polynomiales

**4186**: 1.2.6 Soit F un sous-espace vectoriel d' un K - espace vectoriel E , que dire de 1.2.7 Soit E un K - espace vectoriel , soit E1 , E2 et E3 trois sous-espaces vectoriels de E tels que : Que ***peut*** -on en conclure ? 1.2.8 Soit E un K - espace vectoriel , A et B deux parties de E , comparer : Vect(A B ) et Vect(A ) Vect(B ) Vect(A B ) et Vect(A ) Vect(B ) 1.2.9 Soit E un K - espace vectoriel tel que E 6 0E et soit ( Ei ) i1,n des sous-espaces vectoriels de E , tous distincts de E. Démontrer que : À quelle condition est -il un sous-espace vectoriel de E ? 1.2.10 Reprendre l' exercice précédent lorsque l' on considère une famille dénombrable de sous-espaces vectoriels ( En ) nN

**4261**: 1.2.6 Soit F un sous-espace vectoriel d' un K - espace vectoriel E , que dire de 1.2.7 Soit E un K - espace vectoriel , soit E1 , E2 et E3 trois sous-espaces vectoriels de E tels que : Que peut -on en conclure ? 1.2.8 Soit E un K - espace vectoriel , A et B deux parties de E , comparer : Vect(A B ) et Vect(A ) Vect(B ) Vect(A B ) et Vect(A ) Vect(B ) 1.2.9 Soit E un K - espace vectoriel tel que E 6 0E et soit ( Ei ) i1,n des sous-espaces vectoriels de E , tous distincts de E. Démontrer que : À quelle condition ***est*** -il un sous-espace vectoriel de E ? 1.2.10 Reprendre l' exercice précédent lorsque l' on considère une famille dénombrable de sous-espaces vectoriels ( En ) nN

**4277**: 1.2.6 Soit F un sous-espace vectoriel d' un K - espace vectoriel E , que dire de 1.2.7 Soit E un K - espace vectoriel , soit E1 , E2 et E3 trois sous-espaces vectoriels de E tels que : Que peut -on en conclure ? 1.2.8 Soit E un K - espace vectoriel , A et B deux parties de E , comparer : Vect(A B ) et Vect(A ) Vect(B ) Vect(A B ) et Vect(A ) Vect(B ) 1.2.9 Soit E un K - espace vectoriel tel que E 6 0E et soit ( Ei ) i1,n des sous-espaces vectoriels de E , tous distincts de E. Démontrer que : À quelle condition est -il un sous-espace vectoriel de E ? 1.2.10 Reprendre l' exercice précédent lorsque l' on ***considère*** une famille dénombrable de sous-espaces vectoriels ( En ) nN

**4302**: Plus précisément , pour E Rp , on pourra démontrer que le résultat ***est*** toujours vrai , mais qu' il devient faux pour l' espace vectoriel des fonctions polynomiales

**4309**: Plus précisément , pour E Rp , on pourra démontrer que le résultat est toujours vrai , mais qu' il ***devient*** faux pour l' espace vectoriel des fonctions polynomiales

**4366**: Somme finie Soit E1 et E2 deux sous-espaces vectoriels de E , on ***appelle*** somme de E1 et E2 et on note E1 E2 le sous-espace vectoriel : Somme quelconque Plus généralement , si ( Ei ) iI est une famille de sous-espaces vectoriels de E , on appelle somme des Ei et Ei le sous-espace vectoriel : Il est faux de penser : 1

**4374**: Somme finie Soit E1 et E2 deux sous-espaces vectoriels de E , on appelle somme de E1 et E2 et on ***note*** E1 E2 le sous-espace vectoriel : Somme quelconque Plus généralement , si ( Ei ) iI est une famille de sous-espaces vectoriels de E , on appelle somme des Ei et Ei le sous-espace vectoriel : Il est faux de penser : 1

**4391**: Somme finie Soit E1 et E2 deux sous-espaces vectoriels de E , on appelle somme de E1 et E2 et on note E1 E2 le sous-espace vectoriel : Somme quelconque Plus généralement , si ( Ei ) iI ***est*** une famille de sous-espaces vectoriels de E , on appelle somme des Ei et Ei le sous-espace vectoriel : Il est faux de penser : 1

**4401**: Somme finie Soit E1 et E2 deux sous-espaces vectoriels de E , on appelle somme de E1 et E2 et on note E1 E2 le sous-espace vectoriel : Somme quelconque Plus généralement , si ( Ei ) iI est une famille de sous-espaces vectoriels de E , on ***appelle*** somme des Ei et Ei le sous-espace vectoriel : Il est faux de penser : 1

**4412**: Somme finie Soit E1 et E2 deux sous-espaces vectoriels de E , on appelle somme de E1 et E2 et on note E1 E2 le sous-espace vectoriel : Somme quelconque Plus généralement , si ( Ei ) iI est une famille de sous-espaces vectoriels de E , on appelle somme des Ei et Ei le sous-espace vectoriel : Il ***est*** faux de penser : 1

**4420**: R2 ***est*** somme de R. 2

**4421**: R2 est ***somme*** de R. 2

**4427**: C ***est*** somme de R et de R.i

**4442**: Si ( xi ) iI ***est*** une famille génératrice d' un K - espace vectoriel E , alors : 4

**4460**: Si P1 ***est*** le plan d' équation x y z 0 et P2 est le plan d' équation x 2 y z 0 de R3 , alors Soit E un K - espace vectoriel

**4471**: Si P1 est le plan d' équation x y z 0 et P2 ***est*** le plan d' équation x 2 y z 0 de R3 , alors Soit E un K - espace vectoriel

**4499**: Somme finie Si E1 et E2 ***sont*** deux sous-espaces vectoriels de E , alors : Somme quelconque Plus généralement , si ( Ei ) iI est une famille de sous-espaces vectoriels de E , alors : Démonstration iI Ei

**4518**: Somme finie Si E1 et E2 sont deux sous-espaces vectoriels de E , alors : Somme quelconque Plus généralement , si ( Ei ) iI ***est*** une famille de sous-espaces vectoriels de E , alors : Démonstration iI Ei

**4548**: En reprenant les notations de la définition 1.7 , page ci - contre , on ***a*** où xik iI Ei Eik pour tout k 1 , n. Le vecteur x s' écrit donc comme une combinaison linéaire d' éléments de iI Ei , donc x Vect ( ) Soit x Vect iI Ei

**4564**: En reprenant les notations de la définition 1.7 , page ci - contre , on a où xik iI Ei Eik pour tout k 1 , n. Le vecteur x s' ***écrit*** donc comme une combinaison linéaire d' éléments de iI Ei , donc x Vect ( ) Soit x Vect iI Ei

**4588**: On ***a*** donc où xk iI Ei pour particulier , pour tout k 1 , n , il existe ik I tel que xk Eik , donc x iI Ei

**4605**: On a donc où xk iI Ei pour particulier , pour tout k 1 , n , il ***existe*** ik I tel que xk Eik , donc x iI Ei

**4619**: Il ***est*** équivalent de dire : 1

**4620**: Il est ***équivalent*** de dire : 1

**4630**: ( ei ) iI ***est*** une famille génératrice de E Vect(ei ) Démonstration Immédiat à partir des définitions

**4666**: Soit E1 , E2 et E3 trois sous-espaces vectoriels de E. ( ***a*** ) Comparer pour l' inclusion les sous-espaces : ( b ) Comparer pour l' inclusion les sous-espaces : ( c ) Comparer pour l' inclusion les sous-espaces : 1.3.2 Soit E un K - espace vectoriel

**4722**: Soit E1 , E2 et E3 trois sous-espaces vectoriels de E. Démontrer que : Démontrer que ce ne ***est*** plus vrai , si l' on enlève une des hypothèses à gauche

**4729**: Soit E1 , E2 et E3 trois sous-espaces vectoriels de E. Démontrer que : Démontrer que ce ne est plus vrai , si l' on ***enlève*** une des hypothèses à gauche

**4757**: Soit E1 et E2 deux sous-espaces vectoriels de E , on ***dit*** que E1 et E2 sont en somme directe si tout élément de E1 E2 s' écrit , de manière unique sous la forme x1 x2 , où x1 E1 et x2 E2

**4762**: Soit E1 et E2 deux sous-espaces vectoriels de E , on dit que E1 et E2 ***sont*** en somme directe si tout élément de E1 E2 s' écrit , de manière unique sous la forme x1 x2 , où x1 E1 et x2 E2

**4773**: Soit E1 et E2 deux sous-espaces vectoriels de E , on dit que E1 et E2 sont en somme directe si tout élément de E1 E2 s' ***écrit*** , de manière unique sous la forme x1 x2 , où x1 E1 et x2 E2

**4792**: On ***écrit*** alors : E1 E2 à la place de E1 E2 Soit E un K - espace vectoriel

**4819**: Deux sous-espaces vectoriels E1 et E2 de E ***sont*** en somme directe si , et seulement si , Démonstration Supposons que E1 et E2 soient en somme directe

**4846**: On ***a*** donc x 0E par définition de la somme directe

**4858**: On ***a*** donc E1 E2 0E et comme 0E E1 E2 ( car E1 E2 est un sous-espace vectoriel de E ) , on a E1 E2 0E

**4872**: On a donc E1 E2 0E et comme 0E E1 E2 ( car E1 E2 ***est*** un sous-espace vectoriel de E ) , on a E1 E2 0E

**4881**: On a donc E1 E2 0E et comme 0E E1 E2 ( car E1 E2 est un sous-espace vectoriel de E ) , on ***a*** E1 E2 0E

**4886**: ***Supposons*** que E1 E2 0E

**4898**: Soit x E , qu' on ***écrit*** sous la forme donc x1 y1 et x2 y2

**4911**: On en ***déduit*** que E1 et E2 sont en somme directe

**4916**: On en déduit que E1 et E2 ***sont*** en somme directe

**4937**: Deux sous-espaces vectoriels E1 et E2 de E ***sont*** en somme directe si , et seulement si , il y a écriture unique de 0E , c' est - à - dire : Démonstration Supposons que E1 et E2 soient en somme directe

**4949**: Deux sous-espaces vectoriels E1 et E2 de E sont en somme directe si , et seulement si , il y ***a*** écriture unique de 0E , c' est - à - dire : Démonstration Supposons que E1 et E2 soient en somme directe

**4988**: On ***a*** Supposons qu' il y ait écriture unique de 0E

**4989**: On a ***Supposons*** qu' il y ait écriture unique de 0E

**5004**: Soit x E qu' on ***écrit*** sous la forme donc x1 y1 x2 y2 0E d' où x1 y1 et x2 y2

**5024**: On en ***déduit*** que E1 et E2 sont en somme Soit E un K - espace vectoriel , soit ( Ei ) iI une famille de sous-espaces vectoriels de E , on dit qu' ils sont en somme directe , si on a écriture unique de 0E , soit : distincts 2 à 2 On écrit alors Ei à la place de Ne pas oublier que l' on ne sait faire que des sommes finies de vecteurs ! ! Si I contient strictement plus de deux éléments , la condition iI Ei 0E ne suffit pas pour assurer que les Ei sont en somme directe , de même que les conditions Ei Ej 0E pour tout Pour s' en convaincre , on peut considérer les sous-espaces vectoriels E1 R.(1 , 0 ) , E2 R.(0 , 1 ) et 1

**5029**: On en déduit que E1 et E2 ***sont*** en somme Soit E un K - espace vectoriel , soit ( Ei ) iI une famille de sous-espaces vectoriels de E , on dit qu' ils sont en somme directe , si on a écriture unique de 0E , soit : distincts 2 à 2 On écrit alors Ei à la place de Ne pas oublier que l' on ne sait faire que des sommes finies de vecteurs ! ! Si I contient strictement plus de deux éléments , la condition iI Ei 0E ne suffit pas pour assurer que les Ei sont en somme directe , de même que les conditions Ei Ej 0E pour tout Pour s' en convaincre , on peut considérer les sous-espaces vectoriels E1 R.(1 , 0 ) , E2 R.(0 , 1 ) et 1

**5054**: On en déduit que E1 et E2 sont en somme Soit E un K - espace vectoriel , soit ( Ei ) iI une famille de sous-espaces vectoriels de E , on ***dit*** qu' ils sont en somme directe , si on a écriture unique de 0E , soit : distincts 2 à 2 On écrit alors Ei à la place de Ne pas oublier que l' on ne sait faire que des sommes finies de vecteurs ! ! Si I contient strictement plus de deux éléments , la condition iI Ei 0E ne suffit pas pour assurer que les Ei sont en somme directe , de même que les conditions Ei Ej 0E pour tout Pour s' en convaincre , on peut considérer les sous-espaces vectoriels E1 R.(1 , 0 ) , E2 R.(0 , 1 ) et 1

**5057**: On en déduit que E1 et E2 sont en somme Soit E un K - espace vectoriel , soit ( Ei ) iI une famille de sous-espaces vectoriels de E , on dit qu' ils ***sont*** en somme directe , si on a écriture unique de 0E , soit : distincts 2 à 2 On écrit alors Ei à la place de Ne pas oublier que l' on ne sait faire que des sommes finies de vecteurs ! ! Si I contient strictement plus de deux éléments , la condition iI Ei 0E ne suffit pas pour assurer que les Ei sont en somme directe , de même que les conditions Ei Ej 0E pour tout Pour s' en convaincre , on peut considérer les sous-espaces vectoriels E1 R.(1 , 0 ) , E2 R.(0 , 1 ) et 1

**5064**: On en déduit que E1 et E2 sont en somme Soit E un K - espace vectoriel , soit ( Ei ) iI une famille de sous-espaces vectoriels de E , on dit qu' ils sont en somme directe , si on ***a*** écriture unique de 0E , soit : distincts 2 à 2 On écrit alors Ei à la place de Ne pas oublier que l' on ne sait faire que des sommes finies de vecteurs ! ! Si I contient strictement plus de deux éléments , la condition iI Ei 0E ne suffit pas pour assurer que les Ei sont en somme directe , de même que les conditions Ei Ej 0E pour tout Pour s' en convaincre , on peut considérer les sous-espaces vectoriels E1 R.(1 , 0 ) , E2 R.(0 , 1 ) et 1

**5077**: On en déduit que E1 et E2 sont en somme Soit E un K - espace vectoriel , soit ( Ei ) iI une famille de sous-espaces vectoriels de E , on dit qu' ils sont en somme directe , si on a écriture unique de 0E , soit : distincts 2 à 2 On ***écrit*** alors Ei à la place de Ne pas oublier que l' on ne sait faire que des sommes finies de vecteurs ! ! Si I contient strictement plus de deux éléments , la condition iI Ei 0E ne suffit pas pour assurer que les Ei sont en somme directe , de même que les conditions Ei Ej 0E pour tout Pour s' en convaincre , on peut considérer les sous-espaces vectoriels E1 R.(1 , 0 ) , E2 R.(0 , 1 ) et 1

**5091**: On en déduit que E1 et E2 sont en somme Soit E un K - espace vectoriel , soit ( Ei ) iI une famille de sous-espaces vectoriels de E , on dit qu' ils sont en somme directe , si on a écriture unique de 0E , soit : distincts 2 à 2 On écrit alors Ei à la place de Ne pas oublier que l' on ne ***sait*** faire que des sommes finies de vecteurs ! ! Si I contient strictement plus de deux éléments , la condition iI Ei 0E ne suffit pas pour assurer que les Ei sont en somme directe , de même que les conditions Ei Ej 0E pour tout Pour s' en convaincre , on peut considérer les sous-espaces vectoriels E1 R.(1 , 0 ) , E2 R.(0 , 1 ) et 1

**5103**: On en déduit que E1 et E2 sont en somme Soit E un K - espace vectoriel , soit ( Ei ) iI une famille de sous-espaces vectoriels de E , on dit qu' ils sont en somme directe , si on a écriture unique de 0E , soit : distincts 2 à 2 On écrit alors Ei à la place de Ne pas oublier que l' on ne sait faire que des sommes finies de vecteurs ! ! Si I ***contient*** strictement plus de deux éléments , la condition iI Ei 0E ne suffit pas pour assurer que les Ei sont en somme directe , de même que les conditions Ei Ej 0E pour tout Pour s' en convaincre , on peut considérer les sous-espaces vectoriels E1 R.(1 , 0 ) , E2 R.(0 , 1 ) et 1

**5116**: On en déduit que E1 et E2 sont en somme Soit E un K - espace vectoriel , soit ( Ei ) iI une famille de sous-espaces vectoriels de E , on dit qu' ils sont en somme directe , si on a écriture unique de 0E , soit : distincts 2 à 2 On écrit alors Ei à la place de Ne pas oublier que l' on ne sait faire que des sommes finies de vecteurs ! ! Si I contient strictement plus de deux éléments , la condition iI Ei 0E ne ***suffit*** pas pour assurer que les Ei sont en somme directe , de même que les conditions Ei Ej 0E pour tout Pour s' en convaincre , on peut considérer les sous-espaces vectoriels E1 R.(1 , 0 ) , E2 R.(0 , 1 ) et 1

**5123**: On en déduit que E1 et E2 sont en somme Soit E un K - espace vectoriel , soit ( Ei ) iI une famille de sous-espaces vectoriels de E , on dit qu' ils sont en somme directe , si on a écriture unique de 0E , soit : distincts 2 à 2 On écrit alors Ei à la place de Ne pas oublier que l' on ne sait faire que des sommes finies de vecteurs ! ! Si I contient strictement plus de deux éléments , la condition iI Ei 0E ne suffit pas pour assurer que les Ei ***sont*** en somme directe , de même que les conditions Ei Ej 0E pour tout Pour s' en convaincre , on peut considérer les sous-espaces vectoriels E1 R.(1 , 0 ) , E2 R.(0 , 1 ) et 1

**5144**: On en déduit que E1 et E2 sont en somme Soit E un K - espace vectoriel , soit ( Ei ) iI une famille de sous-espaces vectoriels de E , on dit qu' ils sont en somme directe , si on a écriture unique de 0E , soit : distincts 2 à 2 On écrit alors Ei à la place de Ne pas oublier que l' on ne sait faire que des sommes finies de vecteurs ! ! Si I contient strictement plus de deux éléments , la condition iI Ei 0E ne suffit pas pour assurer que les Ei sont en somme directe , de même que les conditions Ei Ej 0E pour tout Pour s' en convaincre , on ***peut*** considérer les sous-espaces vectoriels E1 R.(1 , 0 ) , E2 R.(0 , 1 ) et 1

**5164**: R2 ***est*** somme directe de R. 2

**5172**: C ***est*** somme directe de R et de R.i

**5185**: Si P1 ***est*** le plan d' équation x y z 0 et P2 est le plan d' équation x 2 y z 0 de R3 , alors mais la somme ne est pas directe car ( 1 , 2 , 3 ) P1 P2

**5196**: Si P1 est le plan d' équation x y z 0 et P2 ***est*** le plan d' équation x 2 y z 0 de R3 , alors mais la somme ne est pas directe car ( 1 , 2 , 3 ) P1 P2

**5214**: Si P1 est le plan d' équation x y z 0 et P2 est le plan d' équation x 2 y z 0 de R3 , alors mais la somme ne ***est*** pas directe car ( 1 , 2 , 3 ) P1 P2

**5252**: 1.4.1 Soit E C 0 ( R , R ) , démontrer que le sous-espace vectoriel des fonctions paires et celui des fonctions impaires ***sont*** en somme directe

**5258**: Quelle ***est*** leur somme ? 1.4.2 Soit E un K - espace vectoriel , soit E1 , E2 et E3 trois sous-espaces vectoriels de E. On suppose que ( a ) Démontrer que si E2 E3 , alors ( b ) Démontrer que le résultat devient faux lorsque E2 6 E3 et E1 6 E3

**5283**: Quelle est leur somme ? 1.4.2 Soit E un K - espace vectoriel , soit E1 , E2 et E3 trois sous-espaces vectoriels de E. On ***suppose*** que ( a ) Démontrer que si E2 E3 , alors ( b ) Démontrer que le résultat devient faux lorsque E2 6 E3 et E1 6 E3

**5286**: Quelle est leur somme ? 1.4.2 Soit E un K - espace vectoriel , soit E1 , E2 et E3 trois sous-espaces vectoriels de E. On suppose que ( ***a*** ) Démontrer que si E2 E3 , alors ( b ) Démontrer que le résultat devient faux lorsque E2 6 E3 et E1 6 E3

**5302**: Quelle est leur somme ? 1.4.2 Soit E un K - espace vectoriel , soit E1 , E2 et E3 trois sous-espaces vectoriels de E. On suppose que ( a ) Démontrer que si E2 E3 , alors ( b ) Démontrer que le résultat ***devient*** faux lorsque E2 6 E3 et E1 6 E3

**5387**: Supplémentaire Soit E1 et E2 des sous-espace vectoriels de E , on ***dit*** que E2 est un supplémentaire de E1 si : On dit aussi que les deux espaces E1 et E2 sont supplémentaires

**5390**: Supplémentaire Soit E1 et E2 des sous-espace vectoriels de E , on dit que E2 ***est*** un supplémentaire de E1 si : On dit aussi que les deux espaces E1 et E2 sont supplémentaires

**5398**: Supplémentaire Soit E1 et E2 des sous-espace vectoriels de E , on dit que E2 est un supplémentaire de E1 si : On ***dit*** aussi que les deux espaces E1 et E2 sont supplémentaires

**5407**: Supplémentaire Soit E1 et E2 des sous-espace vectoriels de E , on dit que E2 est un supplémentaire de E1 si : On dit aussi que les deux espaces E1 et E2 ***sont*** supplémentaires

**5415**: Décomposition en somme directe On ***appelle*** décomposition en somme directe de E toute famille ( Ei ) iI de sous-espaces vectoriels de E telle 1

**5443**: Dans R2 , deux droites vectorielles distinctes ***sont*** supplémentaires

**5474**: Dans C ( vu comme un R - espace vectoriel ) , deux droites vectorielles engendrées par des complexes z1 et z2 non nuls tels que ***sont*** supplémentaires

**5498**: Dans R3 , une droite vectorielle D et un plan vectoriel P tels que D 6 P ***sont*** supplémentaires

**5520**: Dans C 0 ( R , R ) , les sous-espaces vectoriels des fonctions paires et impaires ***sont*** supplémentaires

**5545**: Dans E Cpm ( 0 , 1 , R ) , la droite engendrée par la fonction x 7 1 ***est*** supplémentaire de Soit E un K - espace vectoriel

**5564**: Deux sous-espaces vectoriels E1 et E2 de E ***sont*** supplémentaires dans E si , et seulement si : double inclusion Si E1 6 E et E 6 0E , il y a une infinité de supplémentaires de E1 ( cela peut être faux sur d' autres corps K que R ou C )

**5587**: Deux sous-espaces vectoriels E1 et E2 de E sont supplémentaires dans E si , et seulement si : double inclusion Si E1 6 E et E 6 0E , il y ***a*** une infinité de supplémentaires de E1 ( cela peut être faux sur d' autres corps K que R ou C )

**5596**: Deux sous-espaces vectoriels E1 et E2 de E sont supplémentaires dans E si , et seulement si : double inclusion Si E1 6 E et E 6 0E , il y a une infinité de supplémentaires de E1 ( cela ***peut*** être faux sur d' autres corps K que R ou C )

**5621**: Nous admettrons que tout sous-espace vectoriel de E ( quelconque ) ***admet*** un supplémentaire

**5707**: 1.5.2 Dans R3 , trouver un supplémentaire de la droite d' équations : 1.5.3 Dans E C 0 ( R , R ) , trouver des supplémentaires de : Vect(x 7 x ) 1.5.4 Soit E C ( R , R ) , soit n N , trouver un supplémentaire de 1.5.5 On ***considère*** ici R comme un Q - espace vectoriel ( on a ici K Q )

**5718**: 1.5.2 Dans R3 , trouver un supplémentaire de la droite d' équations : 1.5.3 Dans E C 0 ( R , R ) , trouver des supplémentaires de : Vect(x 7 x ) 1.5.4 Soit E C ( R , R ) , soit n N , trouver un supplémentaire de 1.5.5 On considère ici R comme un Q - espace vectoriel ( on ***a*** ici K Q )

**5739**: En prenant en considération un supplémentaire de Vect(1 , 2 ) dans R qu' il ***existe*** une fonction f : R R qui est 1-périodique et une fonction g : R R qui est 2-périodique telles que : Soit E un K - espace vectoriel

**5747**: En prenant en considération un supplémentaire de Vect(1 , 2 ) dans R qu' il existe une fonction f : R R qui ***est*** 1-périodique et une fonction g : R R qui est 2-périodique telles que : Soit E un K - espace vectoriel

**5757**: En prenant en considération un supplémentaire de Vect(1 , 2 ) dans R qu' il existe une fonction f : R R qui est 1-périodique et une fonction g : R R qui ***est*** 2-périodique telles que : Soit E un K - espace vectoriel

**5789**: , xn ) E n , on ***dit*** que cette famille est libre , ou que les vecteurs sont linéairement indépendants , si ( écriture unique de 0E ) : On parle aussi de partie libre

**5793**: , xn ) E n , on dit que cette famille ***est*** libre , ou que les vecteurs sont linéairement indépendants , si ( écriture unique de 0E ) : On parle aussi de partie libre

**5800**: , xn ) E n , on dit que cette famille est libre , ou que les vecteurs ***sont*** linéairement indépendants , si ( écriture unique de 0E ) : On parle aussi de partie libre

**5813**: , xn ) E n , on dit que cette famille est libre , ou que les vecteurs sont linéairement indépendants , si ( écriture unique de 0E ) : On ***parle*** aussi de partie libre

**5828**: Famille liée ( finie ) Une famille qui ne ***est*** pas libre est dite famille liée et les vecteurs de cette famille sont dits linéairement dépendants

**5831**: Famille liée ( finie ) Une famille qui ne est pas libre ***est*** dite famille liée et les vecteurs de cette famille sont dits linéairement dépendants

**5841**: Famille liée ( finie ) Une famille qui ne est pas libre est dite famille liée et les vecteurs de cette famille ***sont*** dits linéairement dépendants

**5857**: Famille libre Plus généralement , une famille ( xi ) iI ***est*** dite libre ( vecteurs indépendants ) , si ( écriture unique de 0E ) : distincts 2 à 2 Autrement dit , ( xi ) iI est libre si toute sous-famille finie est libre

**5878**: Famille libre Plus généralement , une famille ( xi ) iI est dite libre ( vecteurs indépendants ) , si ( écriture unique de 0E ) : distincts 2 à 2 Autrement ***dit*** , ( xi ) iI est libre si toute sous-famille finie est libre

**5884**: Famille libre Plus généralement , une famille ( xi ) iI est dite libre ( vecteurs indépendants ) , si ( écriture unique de 0E ) : distincts 2 à 2 Autrement dit , ( xi ) iI ***est*** libre si toute sous-famille finie est libre

**5890**: Famille libre Plus généralement , une famille ( xi ) iI est dite libre ( vecteurs indépendants ) , si ( écriture unique de 0E ) : distincts 2 à 2 Autrement dit , ( xi ) iI est libre si toute sous-famille finie ***est*** libre

**5915**: Proposition 1.2 Soit E un K - espace vectoriel et soit ( xi ) iI une famille de vecteurs de E. Il ***est*** équivalent de dire : 1

**5916**: Proposition 1.2 Soit E un K - espace vectoriel et soit ( xi ) iI une famille de vecteurs de E. Il est ***équivalent*** de dire : 1

**5928**: la famille ( xi ) iI ***est*** liée 2

**5933**: il ***existe*** une écriture de 0E non triviale , soit a : distincts 2 à 2 ik .xik 0E 3

**5942**: il existe une écriture de 0E non triviale , soit ***a*** : distincts 2 à 2 ik .xik 0E 3

**5956**: autre formulation : ***a.*** Nous utiliserons parfois l' expression non tous nuls , pour parler de coefficients qui ne sont pas tous nuls

**5972**: autre formulation : a. Nous utiliserons parfois l' expression non tous nuls , pour parler de coefficients qui ne ***sont*** pas tous nuls

**5985**: Notons la différence avec tous non nuls qui ***signifient*** que tous les coefficients sont non nuls , alors que non tous nuls signifie qu' il en existe au moins un non nul ! Démonstration 1

**5990**: Notons la différence avec tous non nuls qui signifient que tous les coefficients ***sont*** non nuls , alors que non tous nuls signifie qu' il en existe au moins un non nul ! Démonstration 1

**5999**: Notons la différence avec tous non nuls qui signifient que tous les coefficients sont non nuls , alors que non tous nuls ***signifie*** qu' il en existe au moins un non nul ! Démonstration 1

**6003**: Notons la différence avec tous non nuls qui signifient que tous les coefficients sont non nuls , alors que non tous nuls signifie qu' il en ***existe*** au moins un non nul ! Démonstration 1

**6029**: Dans R2 , les vecteurs ( 1 , 2 ) et ( 1 , 1 ) ***sont*** linéairement indépendants alors que les vecteurs ( 1 , 2 ) , ( 1 , 1 ) 2

**6071**: Dans C en tant que R - espace vectoriel les parties 1 , i , 1 , j et i , j ***sont*** libres , alors que la partie 1 , i , j est liée

**6083**: Dans C en tant que R - espace vectoriel les parties 1 , i , 1 , j et i , j sont libres , alors que la partie 1 , i , j ***est*** liée

**6111**: Dans C en tant que C - espace vectoriel , les parties 1 , i , 1 , j et i , j ***sont*** liées

**6119**: Si ei , i I ***est*** une partie libre d' un K - espace vectoriel E , et si x 6 Vect(ei , i I ) , alors x ei , i I est encore une partie libre de E Démonstration ik .xik 0E Supposons que 6 0 K

**6147**: Si ei , i I est une partie libre d' un K - espace vectoriel E , et si x 6 Vect(ei , i I ) , alors x ei , i I ***est*** encore une partie libre de E Démonstration ik .xik 0E Supposons que 6 0 K

**6173**: Alors .xik Vect(ei , i I ) ce qui ***contredit*** x 6 Vect(ei , i I )

**6183**: On ***a*** donc 0 K donc ik .xik 0E d' où , puisque ei , i I est une partie libre de E , i1 in 0 K

**6199**: On a donc 0 K donc ik .xik 0E d' où , puisque ei , i I ***est*** une partie libre de E , i1 in 0 K

**6213**: On en ***déduit*** que x ei , i I est une partie libre de E. Soit X ( xi ) iI une famille de vecteurs d' un K - espace vectoriel E. Alors : 1

**6220**: On en déduit que x ei , i I ***est*** une partie libre de E. Soit X ( xi ) iI une famille de vecteurs d' un K - espace vectoriel E. Alors : 1

**6249**: si il ***existe*** i0 I tel que xi0 0E , alors X est liée 2

**6259**: si il existe i0 I tel que xi0 0E , alors X ***est*** liée 2

**6265**: si il ***existe*** ( i , j ) I 2 tel que i 6 j et xi xj , alors X est liée 3

**6284**: si il existe ( i , j ) I 2 tel que i 6 j et xi xj , alors X ***est*** liée 3

**6298**: si une sous-famille ( xi ) iJ où J I ***est*** liée , alors X est liée 4

**6303**: si une sous-famille ( xi ) iJ où J I est liée , alors X ***est*** liée 4

**6309**: si X ***est*** libre , alors toute sous-famille de X est libre

**6317**: si X est libre , alors toute sous-famille de X ***est*** libre

**6324**: Il ***existe*** une écriture de 0E non trivial , car 0E 1.xi0 , donc X est liée

**6338**: Il existe une écriture de 0E non trivial , car 0E 1.xi0 , donc X ***est*** liée

**6344**: Il ***existe*** une écriture de 0 K non trivial , car 0E xi xj 1.xi ( 1).xj , donc X est liée

**6363**: Il existe une écriture de 0 K non trivial , car 0E xi xj 1.xi ( 1).xj , donc X ***est*** liée

**6370**: Si il ***existe*** une écriture de 0E non trivial à partir des ( xi ) iJ , c' est encore une écriture de 0E non trivial à partir des vecteurs de X , car ( xi ) iJ est une sous-famille de X

**6386**: Si il existe une écriture de 0E non trivial à partir des ( xi ) iJ , c' ***est*** encore une écriture de 0E non trivial à partir des vecteurs de X , car ( xi ) iJ est une sous-famille de X

**6406**: Si il existe une écriture de 0E non trivial à partir des ( xi ) iJ , c' est encore une écriture de 0E non trivial à partir des vecteurs de X , car ( xi ) iJ ***est*** une sous-famille de X

**6415**: C' ***est*** la contraposée de la propriété précédente

**6437**: Soit E un K - espace vectoriel , une famille ( xi ) iI ***est*** appelée base de E , si elle est à la fois famille libre et famille génératrice

**6445**: Soit E un K - espace vectoriel , une famille ( xi ) iI est appelée base de E , si elle ***est*** à la fois famille libre et famille génératrice

**6469**: Si B ( ei ) iI ***est*** une base de E , alors a : distincts 2 à 2 Cela signifie que tout vecteur de E s' écrit de manière unique comme une combinaison linéaire d' éléments a. Notons que dans cette écriture , lorsque x 0E , on a n 0

**6476**: Si B ( ei ) iI est une base de E , alors ***a*** : distincts 2 à 2 Cela signifie que tout vecteur de E s' écrit de manière unique comme une combinaison linéaire d' éléments a. Notons que dans cette écriture , lorsque x 0E , on a n 0

**6483**: Si B ( ei ) iI est une base de E , alors a : distincts 2 à 2 Cela ***signifie*** que tout vecteur de E s' écrit de manière unique comme une combinaison linéaire d' éléments a. Notons que dans cette écriture , lorsque x 0E , on a n 0

**6490**: Si B ( ei ) iI est une base de E , alors a : distincts 2 à 2 Cela signifie que tout vecteur de E s' ***écrit*** de manière unique comme une combinaison linéaire d' éléments a. Notons que dans cette écriture , lorsque x 0E , on a n 0

**6500**: Si B ( ei ) iI est une base de E , alors a : distincts 2 à 2 Cela signifie que tout vecteur de E s' écrit de manière unique comme une combinaison linéaire d' éléments ***a.*** Notons que dans cette écriture , lorsque x 0E , on a n 0

**6501**: Si B ( ei ) iI est une base de E , alors a : distincts 2 à 2 Cela signifie que tout vecteur de E s' écrit de manière unique comme une combinaison linéaire d' éléments a. ***Notons*** que dans cette écriture , lorsque x 0E , on a n 0

**6512**: Si B ( ei ) iI est une base de E , alors a : distincts 2 à 2 Cela signifie que tout vecteur de E s' écrit de manière unique comme une combinaison linéaire d' éléments a. Notons que dans cette écriture , lorsque x 0E , on ***a*** n 0

**6521**: Démonstration Immédiat : x s' ***écrit*** comme une combinaison linéaire d' éléments de B car B est génératrice et cette combinaison linéaire est unique car B est libre

**6532**: Démonstration Immédiat : x s' écrit comme une combinaison linéaire d' éléments de B car B ***est*** génératrice et cette combinaison linéaire est unique car B est libre

**6538**: Démonstration Immédiat : x s' écrit comme une combinaison linéaire d' éléments de B car B est génératrice et cette combinaison linéaire ***est*** unique car B est libre

**6542**: Démonstration Immédiat : x s' écrit comme une combinaison linéaire d' éléments de B car B est génératrice et cette combinaison linéaire est unique car B ***est*** libre

**6554**: La famille ( i ) iI définie par : ***est*** appelée coordonnées du vecteur x dans la base B. L' ensemble vide est une base de 0E

**6567**: La famille ( i ) iI définie par : est appelée coordonnées du vecteur x dans la base B. L' ensemble vide ***est*** une base de 0E

**6580**: La base canonique de Kn ***est*** la famille ( ei ) i1,n , où pour tout i 1 , n , i - ième élément La base canonique est une base de K

**6603**: La base canonique de Kn est la famille ( ei ) i1,n , où pour tout i 1 , n , i - ième élément La base canonique ***est*** une base de K

**6619**: La famille ( x 7 xn ) nN ***est*** une base du R - espace vectoriel des fonctions polynomiales

**6632**: Il ***est*** équivalent de dire : 1

**6633**: Il est ***équivalent*** de dire : 1

**6645**: la famille ( ei ) iI ***est*** une base de E 2

**6653**: on ***a*** la décomposition en somme directe de E suivante : Vect(ei ) Démonstration Immédiat : on utilise la proposition 1.7 , page 29 et on remarque que la famille ( ei ) iI est libre si et seulement si les Vect(ei ) sont en somme directe

**6669**: on a la décomposition en somme directe de E suivante : Vect(ei ) Démonstration Immédiat : on ***utilise*** la proposition 1.7 , page 29 et on remarque que la famille ( ei ) iI est libre si et seulement si les Vect(ei ) sont en somme directe

**6678**: on a la décomposition en somme directe de E suivante : Vect(ei ) Démonstration Immédiat : on utilise la proposition 1.7 , page 29 et on ***remarque*** que la famille ( ei ) iI est libre si et seulement si les Vect(ei ) sont en somme directe

**6686**: on a la décomposition en somme directe de E suivante : Vect(ei ) Démonstration Immédiat : on utilise la proposition 1.7 , page 29 et on remarque que la famille ( ei ) iI ***est*** libre si et seulement si les Vect(ei ) sont en somme directe

**6695**: on a la décomposition en somme directe de E suivante : Vect(ei ) Démonstration Immédiat : on utilise la proposition 1.7 , page 29 et on remarque que la famille ( ei ) iI est libre si et seulement si les Vect(ei ) ***sont*** en somme directe

**6705**: La notion de base ne ***est*** donc qu' un cas très particulier de la décomposition en somme directe

**6722**: 1.6.1 Si E ***est*** un K - espace vectoriel et si ( e1 ,

**6739**: , en ) ***est*** une famille de vecteurs de E , démontrer que : ( a ) Dans tous les cas : 1.6.2 Dans R4 , on considère les sous-espaces vectoriels suivants : ( a ) Démontrer que ( c ) Exprimer les coordonnées d' un vecteur quelconque de R4 dans cette base

**6751**: , en ) est une famille de vecteurs de E , démontrer que : ( ***a*** ) Dans tous les cas : 1.6.2 Dans R4 , on considère les sous-espaces vectoriels suivants : ( a ) Démontrer que ( c ) Exprimer les coordonnées d' un vecteur quelconque de R4 dans cette base

**6763**: , en ) est une famille de vecteurs de E , démontrer que : ( a ) Dans tous les cas : 1.6.2 Dans R4 , on ***considère*** les sous-espaces vectoriels suivants : ( a ) Démontrer que ( c ) Exprimer les coordonnées d' un vecteur quelconque de R4 dans cette base

**6770**: , en ) est une famille de vecteurs de E , démontrer que : ( a ) Dans tous les cas : 1.6.2 Dans R4 , on considère les sous-espaces vectoriels suivants : ( ***a*** ) Démontrer que ( c ) Exprimer les coordonnées d' un vecteur quelconque de R4 dans cette base

**6795**: 1.6.3 Dans R5 , on ***considère*** les vecteurs : F Vect(a , b , c ) et G Vect(d , e ) ( a ) Trouver des bases de F , G , F G et F G. ( b ) Trouver des équations de ces sous-espaces vectoriels

**6813**: 1.6.3 Dans R5 , on considère les vecteurs : F Vect(a , b , c ) et G Vect(d , e ) ( ***a*** ) Trouver des bases de F , G , F G et F G. ( b ) Trouver des équations de ces sous-espaces vectoriels

**6841**: 1.6.4 On ***considère*** dans R4 les cinq vecteurs suivants : Donner une équation du sous-espace vectoriel engendré par ces cinq vecteurs

**6895**: Démontrer que : ( f ) R ***est*** une famille libre de F ( R , R ) 1.6.7 Soit A un ensemble , f une application de A dans R , telle que f ( A ) est infini

**6926**: Démontrer que : ( f ) R est une famille libre de F ( R , R ) 1.6.7 Soit A un ensemble , f une application de A dans R , telle que f ( A ) ***est*** infini

**6937**: Démontrer que la famille ( f ) R ***est*** libre dans F ( A , R )

**6959**: On ***pose*** x0 et xn1

**6999**: Soit E l' ensemble des fonctions f : R R , de classe C 1 , telles que la restriction de f à chaque intervalle xi , xi1 , ( i 0 , n ) ***est*** une fonction polynomiale de degré 2

**7010**: Démontrer que E ***est*** un R - espace vectoriel et en donner une base

**7029**: 1.6.9 Démontrer que dans Rn , il ***existe*** une famille infinie F de vecteurs tels que chaque sous famille de cardinal n est libre

**7044**: 1.6.9 Démontrer que dans Rn , il existe une famille infinie F de vecteurs tels que chaque sous famille de cardinal n ***est*** libre

**7058**: On ***dit*** que E est de dimension finie si il possède une base de cardinal fini

**7061**: On dit que E ***est*** de dimension finie si il possède une base de cardinal fini

**7067**: On dit que E est de dimension finie si il ***possède*** une base de cardinal fini

**7080**: Dans le cas contraire , il ***est*** dit de dimension infinie

**7081**: Dans le cas contraire , il est ***dit*** de dimension infinie

**7094**: Pour tout n N , Kn ***est*** de dimension finie

**7103**: Si E ***est*** un C - espace vectoriel de dimension finie , alors E est un R - espace vectoriel de dimension finie

**7115**: Si E est un C - espace vectoriel de dimension finie , alors E ***est*** un R - espace vectoriel de dimension finie

**7136**: L' ensemble des fonctions polynomiales de R dans R ***est*** un K - espace vectoriel de dimension infinie

**7189**: On ***peut*** échanger certains vecteurs de la famille génératrice avec des vecteurs de la famille libre tout en gardant la propriété d' être génératrice , soit : Démonstration Puisque ( g1 ,

**7226**: , gn ) ***est*** génératrice , 1 est combinaison linéaire de ( g1 ,

**7230**: , gn ) est génératrice , 1 ***est*** combinaison linéaire de ( g1 ,

**7255**: De plus , 1 6 0E ( car ( 1 ) ***est*** libre ) , on peut supposer que les coefficients ne sont pas tous nuls , par exemple le premier : avec 1 6 0

**7260**: De plus , 1 6 0E ( car ( 1 ) est libre ) , on ***peut*** supposer que les coefficients ne sont pas tous nuls , par exemple le premier : avec 1 6 0

**7266**: De plus , 1 6 0E ( car ( 1 ) est libre ) , on peut supposer que les coefficients ne ***sont*** pas tous nuls , par exemple le premier : avec 1 6 0

**7287**: ***Supposons*** qu' on ait formé une famille génératrice de la forme donc on peut écrire liées )

**7300**: Supposons qu' on ait formé une famille génératrice de la forme donc on ***peut*** écrire liées )

**7315**: Par le même argument qui ci - dessus , on ***a*** donc Il reste à démontrer que p n. Si p n , en prenant k n 1 dans la construction ci - dessus , on obtient que ( 1 ,

**7318**: Par le même argument qui ci - dessus , on a donc Il ***reste*** à démontrer que p n. Si p n , en prenant k n 1 dans la construction ci - dessus , on obtient que ( 1 ,

**7341**: Par le même argument qui ci - dessus , on a donc Il reste à démontrer que p n. Si p n , en prenant k n 1 dans la construction ci - dessus , on ***obtient*** que ( 1 ,

**7352**: , n ) ***est*** une partie génératrice de E. On aurait alors n1 Vect(1 ,

**7386**: , n1 ) serait liée , ce qui ***contredit*** le fait que ( 1 ,

**7399**: , p ) ***est*** libre

**7403**: On ***a*** donc p n. On peut aussi énoncer ce résultat avec une famille génératrice de cardinal quelconque ( peut-être infini ) , où l' on pourra échanger p vecteurs de cette famille tout en la gardant génératrice

**7408**: On a donc p n. On ***peut*** aussi énoncer ce résultat avec une famille génératrice de cardinal quelconque ( peut-être infini ) , où l' on pourra échanger p vecteurs de cette famille tout en la gardant génératrice

**7458**: Théorème 1.2 Soit E un K - espace vectoriel de dimension finie , alors toutes les bases ***ont*** même cardinal

**7465**: Ce cardinal commun ***est*** appelé dimension de E et est notée a : dimK E ou dim E si il ne y a pas ambiguïté sur le corps K a. On utiliser aussi parfois la notation dim(E ) ou dimK ( E )

**7471**: Ce cardinal commun est appelé dimension de E et ***est*** notée a : dimK E ou dim E si il ne y a pas ambiguïté sur le corps K a. On utiliser aussi parfois la notation dim(E ) ou dimK ( E )

**7484**: Ce cardinal commun est appelé dimension de E et est notée a : dimK E ou dim E si il ne y ***a*** pas ambiguïté sur le corps K a. On utiliser aussi parfois la notation dim(E ) ou dimK ( E )

**7491**: Ce cardinal commun est appelé dimension de E et est notée a : dimK E ou dim E si il ne y a pas ambiguïté sur le corps K ***a.*** On utiliser aussi parfois la notation dim(E ) ou dimK ( E )

**7508**: Démonstration C' ***est*** une application immédiate du théorème de l' échange car les bases sont à la fois libres et génératrices

**7520**: Démonstration C' est une application immédiate du théorème de l' échange car les bases ***sont*** à la fois libres et génératrices

**7529**: On ***appelle*** droite vectorielle tout K - espace vectoriel de dimension 1

**7542**: On ***appelle*** plan vectoriel tout K - espace vectoriel de dimension 2

**7557**: Dimension nulle On ***convient*** que l' espace vectoriel 0E est de dimension 0

**7563**: Dimension nulle On convient que l' espace vectoriel 0E ***est*** de dimension 0

**7570**: Cette définition ***est*** cohérente avec celle de droite vectorielle engendrée : si E est un K - espace vectoriel différent de 0E et si x E , x 6 0E , alors x est une base de K.x , la droite vectorielle engendrée par x , qui est bien de dimension 1

**7581**: Cette définition est cohérente avec celle de droite vectorielle engendrée : si E ***est*** un K - espace vectoriel différent de 0E et si x E , x 6 0E , alors x est une base de K.x , la droite vectorielle engendrée par x , qui est bien de dimension 1

**7601**: Cette définition est cohérente avec celle de droite vectorielle engendrée : si E est un K - espace vectoriel différent de 0E et si x E , x 6 0E , alors x ***est*** une base de K.x , la droite vectorielle engendrée par x , qui est bien de dimension 1

**7615**: Cette définition est cohérente avec celle de droite vectorielle engendrée : si E est un K - espace vectoriel différent de 0E et si x E , x 6 0E , alors x est une base de K.x , la droite vectorielle engendrée par x , qui ***est*** bien de dimension 1

**7624**: On ***a*** clairement : dimK Kn n , dimR C 2 2

**7638**: Si E ***est*** un C - espace vectoriel de dimension finie , alors : dimC E n dimR E 2 n 3

**7713**: On ***peut*** compléter la famille libre en une base , soit : Démonstration C' est une application immédiate du théorème de l' échange , en prenant comme famille génératrice une base de E ( qui a n éléments )

**7726**: On peut compléter la famille libre en une base , soit : Démonstration C' ***est*** une application immédiate du théorème de l' échange , en prenant comme famille génératrice une base de E ( qui a n éléments )

**7747**: On peut compléter la famille libre en une base , soit : Démonstration C' est une application immédiate du théorème de l' échange , en prenant comme famille génératrice une base de E ( qui ***a*** n éléments )

**7765**: Si on ***veut*** trouver une base d' un K - espace vectoriel E de dimension finie , on procède par itération : 1

**7781**: Si on veut trouver une base d' un K - espace vectoriel E de dimension finie , on ***procède*** par itération : 1

**7787**: ***Supposons*** construits les p premiers vecteurs de la base ( au départ p 0 ) , notés ( e1 ,

**7815**: , ep ) , on ***prend*** alors un vecteur de E qui ne est pas dans Vect(e1 ,

**7823**: , ep ) , on prend alors un vecteur de E qui ne ***est*** pas dans Vect(e1 ,

**7836**: , ep ) , cela ***fait*** un nouveau vecteur

**7844**: Si l' on ***dispose*** d' une partie génératrice de E , on cherche le premier vecteur de la partie génératrice qui ne est 2

**7853**: Si l' on dispose d' une partie génératrice de E , on ***cherche*** le premier vecteur de la partie génératrice qui ne est 2

**7863**: Si l' on dispose d' une partie génératrice de E , on cherche le premier vecteur de la partie génératrice qui ne ***est*** 2

**7867**: On ***réitère***

**7869**: ***Trouvons*** une base du sous-espace vectoriel F de R4 , d' équations x y z t 0 et x y 2 z 3 t 0

**7897**: Les calculs ***sont*** explicités dans la session Wxmaxima 1.2 , de la présente page

**7913**: Ce qui nous ***donne*** , par exemple , Cherchons un supplémentaire de F ( et une base de ce supplémentaire ) : on cherche un vecteur qui ne est pas dans Vect(e1 , e2 )

**7918**: Ce qui nous donne , par exemple , ***Cherchons*** un supplémentaire de F ( et une base de ce supplémentaire ) : on cherche un vecteur qui ne est pas dans Vect(e1 , e2 )

**7933**: Ce qui nous donne , par exemple , Cherchons un supplémentaire de F ( et une base de ce supplémentaire ) : on ***cherche*** un vecteur qui ne est pas dans Vect(e1 , e2 )

**7938**: Ce qui nous donne , par exemple , Cherchons un supplémentaire de F ( et une base de ce supplémentaire ) : on cherche un vecteur qui ne ***est*** pas dans Vect(e1 , e2 )

**7950**: Pour cela , il ***suffit*** d' imposer z t 0 et ( x , y ) 6 ( 0 , 0 ) , car Par exemple , on peut prendre : Remarquons que nous avons utilisé une partie génératrice bien connue : la base canonique de R4

**7974**: Pour cela , il suffit d' imposer z t 0 et ( x , y ) 6 ( 0 , 0 ) , car Par exemple , on ***peut*** prendre : Remarquons que nous avons utilisé une partie génératrice bien connue : la base canonique de R4

**8022**: toute famille de plus de n 1 éléments ***est*** liée 2

**8029**: toute partie génératrice ***a*** plus de n éléments 3

**8039**: toute partie libre ***a*** moins de n éléments 4

**8052**: toute partie génératrice de n éléments ***est*** une base 5

**8063**: toute partie libre de n éléments ***est*** une base

**8079**: Si une famille de plus de n 1 éléments ***est*** libre , on pourrait la compléter en une base qui contiendrait au moins n 1 éléments , ce qui contredit dim E n. 2

**8099**: Si une famille de plus de n 1 éléments est libre , on pourrait la compléter en une base qui contiendrait au moins n 1 éléments , ce qui ***contredit*** dim E n. 2

**8106**: On ***utilise*** le théorème de l' échange avec une base comme famille libre

**8122**: C' ***est*** le théorème de la base incomplète

**8142**: On ***utilise*** le théorème de l' échange pour extraire une base B G de E ( en partant d' une une sous-famille libre de G )

**8170**: Mais B ***a*** n dim E élément car c' est une base , donc on a G B , c' est donc une base

**8177**: Mais B a n dim E élément car c' ***est*** une base , donc on a G B , c' est donc une base

**8183**: Mais B a n dim E élément car c' est une base , donc on ***a*** G B , c' est donc une base

**8188**: Mais B a n dim E élément car c' est une base , donc on a G B , c' ***est*** donc une base

**8205**: On ***utilise*** le théorème de la base incomplète pour obtenir une base L0 L de E. Mais L0 a n dim E élément car c' est une base , donc on a L L0 , c' est donc une base

**8222**: On utilise le théorème de la base incomplète pour obtenir une base L0 L de E. Mais L0 ***a*** n dim E élément car c' est une base , donc on a L L0 , c' est donc une base

**8229**: On utilise le théorème de la base incomplète pour obtenir une base L0 L de E. Mais L0 a n dim E élément car c' ***est*** une base , donc on a L L0 , c' est donc une base

**8235**: On utilise le théorème de la base incomplète pour obtenir une base L0 L de E. Mais L0 a n dim E élément car c' est une base , donc on ***a*** L L0 , c' est donc une base

**8240**: On utilise le théorème de la base incomplète pour obtenir une base L0 L de E. Mais L0 a n dim E élément car c' est une base , donc on a L L0 , c' ***est*** donc une base

**8262**: Alors de toute partie génératrice on ***peut*** extraire une base ( finie )

**8278**: Démonstration Si E 0E , il ne y ***a*** rien à démontrer

**8283**: ***Supposons*** E 6 0E

**8301**: Il ***existe*** une base B de E qui est finie

**8308**: Il existe une base B de E qui ***est*** finie

**8321**: Tous les vecteurs ( en nombre fini ) de B ***peuvent*** s' écrire comme une combinaison linéaire d' éléments de G donc E est en fait engendré par une sous-famille finie G 0 de G. Comme E 6 0E , G 0 contient un vecteur x 6 0E

**8334**: Tous les vecteurs ( en nombre fini ) de B peuvent s' écrire comme une combinaison linéaire d' éléments de G donc E ***est*** en fait engendré par une sous-famille finie G 0 de G. Comme E 6 0E , G 0 contient un vecteur x 6 0E

**8353**: Tous les vecteurs ( en nombre fini ) de B peuvent s' écrire comme une combinaison linéaire d' éléments de G donc E est en fait engendré par une sous-famille finie G 0 de G. Comme E 6 0E , G 0 ***contient*** un vecteur x 6 0E

**8366**: Le théorème de la base incomplète ***permet*** alors de construire une base de E en complétant la famille ( x ) par des éléments de G. Soit E un K - espace vectoriel

**8396**: Alors E ***est*** de dimension finie si , et seulement si , il possède une partie génératrice finie ( c' est d' ailleurs une définition fréquente des espaces de dimension finie )

**8407**: Alors E est de dimension finie si , et seulement si , il ***possède*** une partie génératrice finie ( c' est d' ailleurs une définition fréquente des espaces de dimension finie )

**8414**: Alors E est de dimension finie si , et seulement si , il possède une partie génératrice finie ( c' ***est*** d' ailleurs une définition fréquente des espaces de dimension finie )

**8429**: Démonstration C' ***est*** une conséquence immédiate de la définition de la dimension finie et de la proposition précédente

**8456**: Alors E ***est*** de dimension infinie si , et seulement si , il existe des parties libres de cardinal quelconque n N. Démonstration Cela revient à démontrer que E est de dimension finie si , et seulement si , il existe n N tel que toute famille de E à n éléments soit liée

**8467**: Alors E est de dimension infinie si , et seulement si , il ***existe*** des parties libres de cardinal quelconque n N. Démonstration Cela revient à démontrer que E est de dimension finie si , et seulement si , il existe n N tel que toute famille de E à n éléments soit liée

**8478**: Alors E est de dimension infinie si , et seulement si , il existe des parties libres de cardinal quelconque n N. Démonstration Cela ***revient*** à démontrer que E est de dimension finie si , et seulement si , il existe n N tel que toute famille de E à n éléments soit liée

**8483**: Alors E est de dimension infinie si , et seulement si , il existe des parties libres de cardinal quelconque n N. Démonstration Cela revient à démontrer que E ***est*** de dimension finie si , et seulement si , il existe n N tel que toute famille de E à n éléments soit liée

**8494**: Alors E est de dimension infinie si , et seulement si , il existe des parties libres de cardinal quelconque n N. Démonstration Cela revient à démontrer que E est de dimension finie si , et seulement si , il ***existe*** n N tel que toute famille de E à n éléments soit liée

**8510**: On ***peut*** prendre n dim E 1

**8527**: Si E ***est*** de dimension finie , alors tout sous-espace vectoriel F de E est de dimension finie et , de plus : dim F dim E E F dim E dim F Démonstration Si F 0E , il ne y a rien à démontrer

**8539**: Si E est de dimension finie , alors tout sous-espace vectoriel F de E ***est*** de dimension finie et , de plus : dim F dim E E F dim E dim F Démonstration Si F 0E , il ne y a rien à démontrer

**8566**: Si E est de dimension finie , alors tout sous-espace vectoriel F de E est de dimension finie et , de plus : dim F dim E E F dim E dim F Démonstration Si F 0E , il ne y ***a*** rien à démontrer

**8584**: Notons p le nombre maximal d' éléments que ***peut*** avoir une famille libre L de F

**8594**: On ***a*** 1 p dim E ( car F 6 0E et que L est une famille libre de E )

**8607**: On a 1 p dim E ( car F 6 0E et que L ***est*** une famille libre de E )

**8639**: Soit L une famille libre d' éléments de F de cardinal p. Pour tout x F , la famille ( L , x ) ***est*** liée ( car sinon cela contredit la définition de p ) , donc x est combinaison linéaire d' éléments de L , donc L est une famille génératrice de F , c' est donc une base de F

**8645**: Soit L une famille libre d' éléments de F de cardinal p. Pour tout x F , la famille ( L , x ) est liée ( car sinon cela ***contredit*** la définition de p ) , donc x est combinaison linéaire d' éléments de L , donc L est une famille génératrice de F , c' est donc une base de F

**8654**: Soit L une famille libre d' éléments de F de cardinal p. Pour tout x F , la famille ( L , x ) est liée ( car sinon cela contredit la définition de p ) , donc x ***est*** combinaison linéaire d' éléments de L , donc L est une famille génératrice de F , c' est donc une base de F

**8664**: Soit L une famille libre d' éléments de F de cardinal p. Pour tout x F , la famille ( L , x ) est liée ( car sinon cela contredit la définition de p ) , donc x est combinaison linéaire d' éléments de L , donc L ***est*** une famille génératrice de F , c' est donc une base de F

**8672**: Soit L une famille libre d' éléments de F de cardinal p. Pour tout x F , la famille ( L , x ) est liée ( car sinon cela contredit la définition de p ) , donc x est combinaison linéaire d' éléments de L , donc L est une famille génératrice de F , c' ***est*** donc une base de F

**8680**: On ***a*** donc dim F p dim E. Si dim E dim F p , L est une famille libre de E à p dim E éléments , donc c' est une base de E : on a Vect(L ) E. Comme de plus Vect(L ) F , on a bien E F

**8695**: On a donc dim F p dim E. Si dim E dim F p , L ***est*** une famille libre de E à p dim E éléments , donc c' est une base de E : on a Vect(L ) E. Comme de plus Vect(L ) F , on a bien E F

**8709**: On a donc dim F p dim E. Si dim E dim F p , L est une famille libre de E à p dim E éléments , donc c' ***est*** une base de E : on a Vect(L ) E. Comme de plus Vect(L ) F , on a bien E F

**8716**: On a donc dim F p dim E. Si dim E dim F p , L est une famille libre de E à p dim E éléments , donc c' est une base de E : on ***a*** Vect(L ) E. Comme de plus Vect(L ) F , on a bien E F

**8728**: On a donc dim F p dim E. Si dim E dim F p , L est une famille libre de E à p dim E éléments , donc c' est une base de E : on a Vect(L ) E. Comme de plus Vect(L ) F , on ***a*** bien E F

**8736**: La proposition précédente ***est*** très pratique pour démontrer l' égalité de deux espaces vectoriels : au lieu de démontrer le résultat par double inclusion , on peut se limiter à démontrer une inclusion et conclure par une égalité de dimension

**8759**: La proposition précédente est très pratique pour démontrer l' égalité de deux espaces vectoriels : au lieu de démontrer le résultat par double inclusion , on ***peut*** se limiter à démontrer une inclusion et conclure par une égalité de dimension

**8782**: Attention à une erreur courante : si B ***est*** une base de E et si F est un sous-espace vectoriel de E , il est faux en général qu' une sous-famille de B soit une base de F

**8790**: Attention à une erreur courante : si B est une base de E et si F ***est*** un sous-espace vectoriel de E , il est faux en général qu' une sous-famille de B soit une base de F

**8798**: Attention à une erreur courante : si B est une base de E et si F est un sous-espace vectoriel de E , il ***est*** faux en général qu' une sous-famille de B soit une base de F

**8825**: Par exemple , une base de F R.(1 , 1 ) ne ***est*** jamais une sous-famille de la base canonique Soit E un K - espace vectoriel de dimension finie

**8855**: Base adaptée à une somme directe de deux sous-espaces vectoriels On ***suppose*** que E F G , on peut alors construire une base ( e1 ,

**8862**: Base adaptée à une somme directe de deux sous-espaces vectoriels On suppose que E F G , on ***peut*** alors construire une base ( e1 ,

**8883**: , en ) de E telle que Une telle base ***est*** dite base adaptée à la somme directe

**8905**: Base adaptée à une somme directe finie De même , lorsque l' on ***a*** une décomposition en somme directe de E , on peut toujours construire une base adaptée à cette somme directe

**8915**: Base adaptée à une somme directe finie De même , lorsque l' on a une décomposition en somme directe de E , on ***peut*** toujours construire une base adaptée à cette somme directe

**8928**: Si E ***est*** un K - espace vectoriel de dimension finie et si E F G , alors : dim E dim F dim G Plus généralement , si E admet une décomposition en somme directe a : Ek dim E a. Il y alors un nombre fini de termes dans la décomposition en somme directe , si on enlève les espaces réduits à 0E

**8956**: Si E est un K - espace vectoriel de dimension finie et si E F G , alors : dim E dim F dim G Plus généralement , si E ***admet*** une décomposition en somme directe a : Ek dim E a. Il y alors un nombre fini de termes dans la décomposition en somme directe , si on enlève les espaces réduits à 0E

**8962**: Si E est un K - espace vectoriel de dimension finie et si E F G , alors : dim E dim F dim G Plus généralement , si E admet une décomposition en somme directe ***a*** : Ek dim E a. Il y alors un nombre fini de termes dans la décomposition en somme directe , si on enlève les espaces réduits à 0E

**8967**: Si E est un K - espace vectoriel de dimension finie et si E F G , alors : dim E dim F dim G Plus généralement , si E admet une décomposition en somme directe a : Ek dim E ***a.*** Il y alors un nombre fini de termes dans la décomposition en somme directe , si on enlève les espaces réduits à 0E

**8985**: Si E est un K - espace vectoriel de dimension finie et si E F G , alors : dim E dim F dim G Plus généralement , si E admet une décomposition en somme directe a : Ek dim E a. Il y alors un nombre fini de termes dans la décomposition en somme directe , si on ***enlève*** les espaces réduits à 0E

**8992**: ***Notons*** ce nombre p. Démonstration C' est immédiat en considérant une base adaptée à cette décomposition en somme directe

**8998**: Notons ce nombre p. Démonstration C' ***est*** immédiat en considérant une base adaptée à cette décomposition en somme directe

**9016**: Si E1 et E2 ***sont*** des K - espaces vectoriels de dimensions finies , alors E1 E2 est de dimension finie et : Plus généralement , si E1 ,

**9029**: Si E1 et E2 sont des K - espaces vectoriels de dimensions finies , alors E1 E2 ***est*** de dimension finie et : Plus généralement , si E1 ,

**9046**: , Ep ***sont*** des K - espaces vectoriels de dimensions finies , alors E1 Ep est de dimension finie et : Démonstration Si E1 0E ou E2 0E , c' est immédiat

**9059**: , Ep sont des K - espaces vectoriels de dimensions finies , alors E1 Ep ***est*** de dimension finie et : Démonstration Si E1 0E ou E2 0E , c' est immédiat

**9074**: , Ep sont des K - espaces vectoriels de dimensions finies , alors E1 Ep est de dimension finie et : Démonstration Si E1 0E ou E2 0E , c' ***est*** immédiat

**9077**: ***Supposons*** que E1 6 0E et E2 6 0E

**9089**: alors on ***vérifie*** que est une base de E1 E2 ( en démontrant que c' est une famille libre et génératrice ) et qui a n1 n2 dim E1 dim E2 éléments

**9091**: alors on vérifie que ***est*** une base de E1 E2 ( en démontrant que c' est une famille libre et génératrice ) et qui a n1 n2 dim E1 dim E2 éléments

**9102**: alors on vérifie que est une base de E1 E2 ( en démontrant que c' ***est*** une famille libre et génératrice ) et qui a n1 n2 dim E1 dim E2 éléments

**9111**: alors on vérifie que est une base de E1 E2 ( en démontrant que c' est une famille libre et génératrice ) et qui ***a*** n1 n2 dim E1 dim E2 éléments

**9122**: On en ***conclut*** que E1 E2 est de dimension finie et que dim ( E1 E2 ) dim E1 dim E2

**9126**: On en conclut que E1 E2 ***est*** de dimension finie et que dim ( E1 E2 ) dim E1 dim E2

**9148**: Dans le cas général , on ***procède*** par récurrence sur p. Proposition 1.3 Formule de Grassman Si E est un K - espace vectoriel de dimension finie , si F et G sont deux sous-espaces vectoriels de E , alors : Démonstration Soit F1 un supplémentaire de F G dans F et soit G1 un supplémentaire de F G dans G : On a alors Pour conclure , il suffit de prendre une base adaptée à cette décomposition en somme directe

**9160**: Dans le cas général , on procède par récurrence sur p. Proposition 1.3 Formule de Grassman Si E ***est*** un K - espace vectoriel de dimension finie , si F et G sont deux sous-espaces vectoriels de E , alors : Démonstration Soit F1 un supplémentaire de F G dans F et soit G1 un supplémentaire de F G dans G : On a alors Pour conclure , il suffit de prendre une base adaptée à cette décomposition en somme directe

**9174**: Dans le cas général , on procède par récurrence sur p. Proposition 1.3 Formule de Grassman Si E est un K - espace vectoriel de dimension finie , si F et G ***sont*** deux sous-espaces vectoriels de E , alors : Démonstration Soit F1 un supplémentaire de F G dans F et soit G1 un supplémentaire de F G dans G : On a alors Pour conclure , il suffit de prendre une base adaptée à cette décomposition en somme directe

**9205**: Dans le cas général , on procède par récurrence sur p. Proposition 1.3 Formule de Grassman Si E est un K - espace vectoriel de dimension finie , si F et G sont deux sous-espaces vectoriels de E , alors : Démonstration Soit F1 un supplémentaire de F G dans F et soit G1 un supplémentaire de F G dans G : On ***a*** alors Pour conclure , il suffit de prendre une base adaptée à cette décomposition en somme directe

**9211**: Dans le cas général , on procède par récurrence sur p. Proposition 1.3 Formule de Grassman Si E est un K - espace vectoriel de dimension finie , si F et G sont deux sous-espaces vectoriels de E , alors : Démonstration Soit F1 un supplémentaire de F G dans F et soit G1 un supplémentaire de F G dans G : On a alors Pour conclure , il ***suffit*** de prendre une base adaptée à cette décomposition en somme directe

**9252**: , en ) ***sont*** supplémentaires dans E. 2

**9260**: Si F ***est*** un sous-espace vectoriel de E alors il existe ( au moins ) un sous-espace supplémentaire de Démonstration 1

**9268**: Si F est un sous-espace vectoriel de E alors il ***existe*** ( au moins ) un sous-espace supplémentaire de Démonstration 1

**9281**: C' ***est*** immédiat : puisque ( e1 ,

**9294**: , en ) ***est*** une base de E , tout élément x E s' écrit de manière unique sous la forme 2

**9305**: , en ) est une base de E , tout élément x E s' ***écrit*** de manière unique sous la forme 2

**9315**: On ***considère*** une base ( e1 ,

**9340**: C' ***est*** une famille libre de E qu' on peut compléter en une base ( e1 ,

**9348**: C' est une famille libre de E qu' on ***peut*** compléter en une base ( e1 ,

**9377**: , en ) de E. Alors d' après ce qui ***précède*** , Démontrer que F1 F2

**9385**: Quelle ***est*** la dimension de F Ga ? Quelle est la dimension de F Ga ? 1.7.3 Soit E un K - espace vectoriel de dimension finie , on suppose qu' on a la somme directe E F1 F2

**9393**: Quelle est la dimension de F Ga ? Quelle ***est*** la dimension de F Ga ? 1.7.3 Soit E un K - espace vectoriel de dimension finie , on suppose qu' on a la somme directe E F1 F2

**9413**: Quelle est la dimension de F Ga ? Quelle est la dimension de F Ga ? 1.7.3 Soit E un K - espace vectoriel de dimension finie , on ***suppose*** qu' on a la somme directe E F1 F2

**9416**: Quelle est la dimension de F Ga ? Quelle est la dimension de F Ga ? 1.7.3 Soit E un K - espace vectoriel de dimension finie , on suppose qu' on ***a*** la somme directe E F1 F2

**9468**: Démontrer qu' il ***existe*** un supplémentaire de F contenant G. 1.7.5 Soit E un K - espace vectoriel de dimension finie et F un sous-espace vectoriel strict de E ( F 6 E )

**9503**: Démontrer que F ***admet*** une infinité de supplémentaires

**9567**: , Vk des sous-espaces vectoriels de V tels que Démontrer que 1.7.7 Soit E un K - espace vectoriel de dimension finie n , soit F1 et F2 deux sous-espaces vectoriels de dimension p n. ( ***a*** ) Démontrer que l' on peut alors trouver un supplémentaire commun à F1 et F2

**9573**: , Vk des sous-espaces vectoriels de V tels que Démontrer que 1.7.7 Soit E un K - espace vectoriel de dimension finie n , soit F1 et F2 deux sous-espaces vectoriels de dimension p n. ( a ) Démontrer que l' on ***peut*** alors trouver un supplémentaire commun à F1 et F2

**9591**: ( b ) Généraliser lorsque le corps ***est*** R à un nombre fini de sous-espaces vectoriels de dimension p. ( c ) Puis , à une infinité dénombrable de tels sous-espaces vectoriels

**9649**: Applications linéaires Généralités Soit E et E 0 deux K - espaces vectoriels , une application f : E E 0 ***est*** dite application linéaire si elle est compatible avec les structures d' espaces vectoriels , c' est - à - dire a : L' ensemble des applications linéaires de E dans E 0 se note : LK ( E , E 0 ) ou L ( E , E 0 ) lorsqu' il ne y a pas ambiguïté sur le corps K 1

**9655**: Applications linéaires Généralités Soit E et E 0 deux K - espaces vectoriels , une application f : E E 0 est dite application linéaire si elle ***est*** compatible avec les structures d' espaces vectoriels , c' est - à - dire a : L' ensemble des applications linéaires de E dans E 0 se note : LK ( E , E 0 ) ou L ( E , E 0 ) lorsqu' il ne y a pas ambiguïté sur le corps K 1

**9670**: Applications linéaires Généralités Soit E et E 0 deux K - espaces vectoriels , une application f : E E 0 est dite application linéaire si elle est compatible avec les structures d' espaces vectoriels , c' est - à - dire ***a*** : L' ensemble des applications linéaires de E dans E 0 se note : LK ( E , E 0 ) ou L ( E , E 0 ) lorsqu' il ne y a pas ambiguïté sur le corps K 1

**9683**: Applications linéaires Généralités Soit E et E 0 deux K - espaces vectoriels , une application f : E E 0 est dite application linéaire si elle est compatible avec les structures d' espaces vectoriels , c' est - à - dire a : L' ensemble des applications linéaires de E dans E 0 se ***note*** : LK ( E , E 0 ) ou L ( E , E 0 ) lorsqu' il ne y a pas ambiguïté sur le corps K 1

**9704**: Applications linéaires Généralités Soit E et E 0 deux K - espaces vectoriels , une application f : E E 0 est dite application linéaire si elle est compatible avec les structures d' espaces vectoriels , c' est - à - dire a : L' ensemble des applications linéaires de E dans E 0 se note : LK ( E , E 0 ) ou L ( E , E 0 ) lorsqu' il ne y ***a*** pas ambiguïté sur le corps K 1

**9719**: Lorsque E E 0 , on ***parle*** d' endomorphisme de E et on note LK ( E ) ou L ( E )

**9741**: Lorsque f ***est*** bijective , on parle d' isomorphisme entre E et E 0

**9745**: Lorsque f est bijective , on ***parle*** d' isomorphisme entre E et E 0

**9762**: Lorsque E E 0 et f ***est*** bijective , on parle d' automorphisme de E et on note GL K ( E ) ou GL ( E )

**9766**: Lorsque E E 0 et f est bijective , on ***parle*** d' automorphisme de E et on note GL K ( E ) ou GL ( E )

**9773**: Lorsque E E 0 et f est bijective , on parle d' automorphisme de E et on ***note*** GL K ( E ) ou GL ( E )

**9793**: Lorsque E 0 K , on ***parle*** de forme linéaire et on note E ?

**9803**: ***a.*** L' image d' une combinaison linéaire est donc la combinaison linéaire des images

**9810**: a. L' image d' une combinaison linéaire ***est*** donc la combinaison linéaire des images

**9822**: Si E ***est*** un K - espace vectoriel , alors l' application identité , notée idE et définie par idE ( x ) x pour tout x E est une application linéaire de E dans lui-même ( c' est donc un endomorphisme de E )

**9848**: Si E est un K - espace vectoriel , alors l' application identité , notée idE et définie par idE ( x ) x pour tout x E ***est*** une application linéaire de E dans lui-même ( c' est donc un endomorphisme de E )

**9858**: Si E est un K - espace vectoriel , alors l' application identité , notée idE et définie par idE ( x ) x pour tout x E est une application linéaire de E dans lui-même ( c' ***est*** donc un endomorphisme de E )

**9883**: Plus généralement , si K , l' homothétie de rapport , définie par x 7 .x , ***est*** un endomorphisme de E. En particulier , l' application nulle x 7 0E est un endomorphisme de E. 2

**9897**: Plus généralement , si K , l' homothétie de rapport , définie par x 7 .x , est un endomorphisme de E. En particulier , l' application nulle x 7 0E ***est*** un endomorphisme de E. 2

**9917**: Dans Rn euclidien , les projections , symétries , rotations , homothéties vectorielles ***sont*** des endomorphismes

**9933**: Les symétries , rotations et homothéties ( de rapport non nul ) ***sont*** des automorphismes

**9944**: Les translations de vecteur non nul ne ***sont*** pas des applications linéaires ! 3

**9960**: La dérivation des fonctions de classe C 1 ***est*** une application linéaire de 4

**9971**: L' application définie pour ***a*** b par : est un isomorphisme

**9975**: L' application définie pour a b par : ***est*** un isomorphisme

**9986**: L' application définie par : ***est*** une forme linéaire Si E et E 0 sont deux K - espaces vectoriels , alors : L ( E , E 0 ) est un K - espace vectoriel Démonstration Montrons que c' est un sous-espace vectoriel de F ( E , E 0 )

**9995**: L' application définie par : est une forme linéaire Si E et E 0 ***sont*** deux K - espaces vectoriels , alors : L ( E , E 0 ) est un K - espace vectoriel Démonstration Montrons que c' est un sous-espace vectoriel de F ( E , E 0 )

**10011**: L' application définie par : est une forme linéaire Si E et E 0 sont deux K - espaces vectoriels , alors : L ( E , E 0 ) ***est*** un K - espace vectoriel Démonstration Montrons que c' est un sous-espace vectoriel de F ( E , E 0 )

**10021**: L' application définie par : est une forme linéaire Si E et E 0 sont deux K - espaces vectoriels , alors : L ( E , E 0 ) est un K - espace vectoriel Démonstration Montrons que c' ***est*** un sous-espace vectoriel de F ( E , E 0 )

**10035**: Il ***est*** immédiat que l' application nulle x 7 0E 0 est linéaire de E dans E 0

**10045**: Il est immédiat que l' application nulle x 7 0E 0 ***est*** linéaire de E dans E 0

**10055**: ce qui ***démontre*** que .f .g L ( E , E 0 )

**10076**: Finalement , L ( E , E 0 ) ***est*** un sous-espace vectoriel de F ( E , E 0 ) , c' est donc un K - espace vectoriel

**10090**: Finalement , L ( E , E 0 ) est un sous-espace vectoriel de F ( E , E 0 ) , c' ***est*** donc un K - espace vectoriel

**10121**: Si F ***est*** un sous-espace vectoriel de E , alors : f ( F ) f ( x ) , x F est un sous-espace vectoriel de E 0 De même , si F 0 est un sous-espace vectoriel de E 0 , alors : f 1 ( F 0 ) x E , f ( x ) F 0 est un sous-espace vectoriel de E Démonstration On a vu ( voir la remarque 1.17 , de la présente page ) que 0E 0 f ( 0E ) f ( F ) ( avec 0E F )

**10141**: Si F est un sous-espace vectoriel de E , alors : f ( F ) f ( x ) , x F ***est*** un sous-espace vectoriel de E 0 De même , si F 0 est un sous-espace vectoriel de E 0 , alors : f 1 ( F 0 ) x E , f ( x ) F 0 est un sous-espace vectoriel de E Démonstration On a vu ( voir la remarque 1.17 , de la présente page ) que 0E 0 f ( 0E ) f ( F ) ( avec 0E F )

**10154**: Si F est un sous-espace vectoriel de E , alors : f ( F ) f ( x ) , x F est un sous-espace vectoriel de E 0 De même , si F 0 ***est*** un sous-espace vectoriel de E 0 , alors : f 1 ( F 0 ) x E , f ( x ) F 0 est un sous-espace vectoriel de E Démonstration On a vu ( voir la remarque 1.17 , de la présente page ) que 0E 0 f ( 0E ) f ( F ) ( avec 0E F )

**10179**: Si F est un sous-espace vectoriel de E , alors : f ( F ) f ( x ) , x F est un sous-espace vectoriel de E 0 De même , si F 0 est un sous-espace vectoriel de E 0 , alors : f 1 ( F 0 ) x E , f ( x ) F 0 ***est*** un sous-espace vectoriel de E Démonstration On a vu ( voir la remarque 1.17 , de la présente page ) que 0E 0 f ( 0E ) f ( F ) ( avec 0E F )

**10221**: de f , on ***a*** donc f ( F ) est stable par combinaison linéaire

**10227**: de f , on a donc f ( F ) ***est*** stable par combinaison linéaire

**10234**: C' ***est*** donc un sous-espace vectoriel de E. car F 0 est stable par combinaison linéaire

**10244**: C' est donc un sous-espace vectoriel de E. car F 0 ***est*** stable par combinaison linéaire

**10252**: On en ***déduit*** que f 1 ( F 0 ) est stable par combinaison linéaire

**10260**: On en déduit que f 1 ( F 0 ) ***est*** stable par combinaison linéaire

**10267**: C' ***est*** donc un sous-espace vectoriel de E. La composée de deux applications linéaires est une application linéaire

**10280**: C' est donc un sous-espace vectoriel de E. La composée de deux applications linéaires ***est*** une application linéaire

**10289**: En particulier , on ***a*** : 1

**10301**: Si E , E 0 et E 00 ***sont*** trois K - espaces vectoriels , si f L ( E , E 0 ) et g L ( E 0 , E 00 ) les applications : sont des applications linéaires

**10330**: Si E , E 0 et E 00 sont trois K - espaces vectoriels , si f L ( E , E 0 ) et g L ( E 0 , E 00 ) les applications : ***sont*** des applications linéaires

**10338**: On ***peut*** donc définir pour un endomorphisme f de E , la notion d' itéré de la manière suivante : On a alors les formules suivantes , si f et g sont deux endomorphismes de E tels que f g g f : ( a ) ( Formule du binôme ) : ( b ) ( Identité remarquable ) : 3

**10358**: On peut donc définir pour un endomorphisme f de E , la notion d' itéré de la manière suivante : On ***a*** alors les formules suivantes , si f et g sont deux endomorphismes de E tels que f g g f : ( a ) ( Formule du binôme ) : ( b ) ( Identité remarquable ) : 3

**10368**: On peut donc définir pour un endomorphisme f de E , la notion d' itéré de la manière suivante : On a alors les formules suivantes , si f et g ***sont*** deux endomorphismes de E tels que f g g f : ( a ) ( Formule du binôme ) : ( b ) ( Identité remarquable ) : 3

**10381**: On peut donc définir pour un endomorphisme f de E , la notion d' itéré de la manière suivante : On a alors les formules suivantes , si f et g sont deux endomorphismes de E tels que f g g f : ( ***a*** ) ( Formule du binôme ) : ( b ) ( Identité remarquable ) : 3

**10401**: Si f ***est*** un automorphisme de E , alors f 1 est aussi un automorphisme de E ( en particulier c' est un endomorphisme de E ) et on définit de même : 1.8.1 Soit f un endomorphisme de E , calculer 1.8.2 Soit f L ( E ) tel qu' il existe n N tel que f n 0L ( E ) ( on dit que f est nilpotent )

**10410**: Si f est un automorphisme de E , alors f 1 ***est*** aussi un automorphisme de E ( en particulier c' est un endomorphisme de E ) et on définit de même : 1.8.1 Soit f un endomorphisme de E , calculer 1.8.2 Soit f L ( E ) tel qu' il existe n N tel que f n 0L ( E ) ( on dit que f est nilpotent )

**10420**: Si f est un automorphisme de E , alors f 1 est aussi un automorphisme de E ( en particulier c' ***est*** un endomorphisme de E ) et on définit de même : 1.8.1 Soit f un endomorphisme de E , calculer 1.8.2 Soit f L ( E ) tel qu' il existe n N tel que f n 0L ( E ) ( on dit que f est nilpotent )

**10428**: Si f est un automorphisme de E , alors f 1 est aussi un automorphisme de E ( en particulier c' est un endomorphisme de E ) et on ***définit*** de même : 1.8.1 Soit f un endomorphisme de E , calculer 1.8.2 Soit f L ( E ) tel qu' il existe n N tel que f n 0L ( E ) ( on dit que f est nilpotent )

**10451**: Si f est un automorphisme de E , alors f 1 est aussi un automorphisme de E ( en particulier c' est un endomorphisme de E ) et on définit de même : 1.8.1 Soit f un endomorphisme de E , calculer 1.8.2 Soit f L ( E ) tel qu' il ***existe*** n N tel que f n 0L ( E ) ( on dit que f est nilpotent )

**10464**: Si f est un automorphisme de E , alors f 1 est aussi un automorphisme de E ( en particulier c' est un endomorphisme de E ) et on définit de même : 1.8.1 Soit f un endomorphisme de E , calculer 1.8.2 Soit f L ( E ) tel qu' il existe n N tel que f n 0L ( E ) ( on ***dit*** que f est nilpotent )

**10467**: Si f est un automorphisme de E , alors f 1 est aussi un automorphisme de E ( en particulier c' est un endomorphisme de E ) et on définit de même : 1.8.1 Soit f un endomorphisme de E , calculer 1.8.2 Soit f L ( E ) tel qu' il existe n N tel que f n 0L ( E ) ( on dit que f ***est*** nilpotent )

**10475**: Démontrer que idE f ***est*** inversible

**10510**: Images et noyaux Soit E et E 0 deux K - espaces vectoriels , et f L ( E , E 0 ) , alors : L' image de E par f ***est*** un sous-espace vectoriel de E 0 noté : L' image réciproque de 0E 0 par f est un sous-espace vectoriel de E appelé noyau de f et noté 1

**10527**: Images et noyaux Soit E et E 0 deux K - espaces vectoriels , et f L ( E , E 0 ) , alors : L' image de E par f est un sous-espace vectoriel de E 0 noté : L' image réciproque de 0E 0 par f ***est*** un sous-espace vectoriel de E appelé noyau de f et noté 1

**10561**: Pour tout endomorphisme p d' un K - espace vectoriel E En effet , un raisonnement par analyse - synthèse ***démontre*** que x se décompose de manière unique sous 2

**10565**: Pour tout endomorphisme p d' un K - espace vectoriel E En effet , un raisonnement par analyse - synthèse démontre que x se ***décompose*** de manière unique sous 2

**10573**: On ***peut*** utiliser les images et noyaux pour démontrer que des ensembles sont des sous-espaces vectoriels : ( a ) ( Image peu fréquent ) : est un sous-espace vectoriel de C 0 ( a , b , R ) en considérant l' endomorphisme de C 0 ( a , b , R ) : ( b ) ( Noyau très fréquent ) : est un sous-espace vectoriel de C 1 ( a , b , R ) en considérant la forme linéaire de C 1 ( a , b , R ) : Proposition 1.4 Soit E et E 0 deux K - espaces vectoriels , f L ( E , E 0 ) , alors : f est injective si , et seulement si , Ker(f ) 0E f est surjective si , et seulement si , Im(f ) E 0 Démonstration La caractérisation de la surjectivité est immédiate

**10584**: On peut utiliser les images et noyaux pour démontrer que des ensembles ***sont*** des sous-espaces vectoriels : ( a ) ( Image peu fréquent ) : est un sous-espace vectoriel de C 0 ( a , b , R ) en considérant l' endomorphisme de C 0 ( a , b , R ) : ( b ) ( Noyau très fréquent ) : est un sous-espace vectoriel de C 1 ( a , b , R ) en considérant la forme linéaire de C 1 ( a , b , R ) : Proposition 1.4 Soit E et E 0 deux K - espaces vectoriels , f L ( E , E 0 ) , alors : f est injective si , et seulement si , Ker(f ) 0E f est surjective si , et seulement si , Im(f ) E 0 Démonstration La caractérisation de la surjectivité est immédiate

**10590**: On peut utiliser les images et noyaux pour démontrer que des ensembles sont des sous-espaces vectoriels : ( ***a*** ) ( Image peu fréquent ) : est un sous-espace vectoriel de C 0 ( a , b , R ) en considérant l' endomorphisme de C 0 ( a , b , R ) : ( b ) ( Noyau très fréquent ) : est un sous-espace vectoriel de C 1 ( a , b , R ) en considérant la forme linéaire de C 1 ( a , b , R ) : Proposition 1.4 Soit E et E 0 deux K - espaces vectoriels , f L ( E , E 0 ) , alors : f est injective si , et seulement si , Ker(f ) 0E f est surjective si , et seulement si , Im(f ) E 0 Démonstration La caractérisation de la surjectivité est immédiate

**10598**: On peut utiliser les images et noyaux pour démontrer que des ensembles sont des sous-espaces vectoriels : ( a ) ( Image peu fréquent ) : ***est*** un sous-espace vectoriel de C 0 ( a , b , R ) en considérant l' endomorphisme de C 0 ( a , b , R ) : ( b ) ( Noyau très fréquent ) : est un sous-espace vectoriel de C 1 ( a , b , R ) en considérant la forme linéaire de C 1 ( a , b , R ) : Proposition 1.4 Soit E et E 0 deux K - espaces vectoriels , f L ( E , E 0 ) , alors : f est injective si , et seulement si , Ker(f ) 0E f est surjective si , et seulement si , Im(f ) E 0 Démonstration La caractérisation de la surjectivité est immédiate

**10606**: On peut utiliser les images et noyaux pour démontrer que des ensembles sont des sous-espaces vectoriels : ( a ) ( Image peu fréquent ) : est un sous-espace vectoriel de C 0 ( ***a*** , b , R ) en considérant l' endomorphisme de C 0 ( a , b , R ) : ( b ) ( Noyau très fréquent ) : est un sous-espace vectoriel de C 1 ( a , b , R ) en considérant la forme linéaire de C 1 ( a , b , R ) : Proposition 1.4 Soit E et E 0 deux K - espaces vectoriels , f L ( E , E 0 ) , alors : f est injective si , et seulement si , Ker(f ) 0E f est surjective si , et seulement si , Im(f ) E 0 Démonstration La caractérisation de la surjectivité est immédiate

**10620**: On peut utiliser les images et noyaux pour démontrer que des ensembles sont des sous-espaces vectoriels : ( a ) ( Image peu fréquent ) : est un sous-espace vectoriel de C 0 ( a , b , R ) en considérant l' endomorphisme de C 0 ( ***a*** , b , R ) : ( b ) ( Noyau très fréquent ) : est un sous-espace vectoriel de C 1 ( a , b , R ) en considérant la forme linéaire de C 1 ( a , b , R ) : Proposition 1.4 Soit E et E 0 deux K - espaces vectoriels , f L ( E , E 0 ) , alors : f est injective si , et seulement si , Ker(f ) 0E f est surjective si , et seulement si , Im(f ) E 0 Démonstration La caractérisation de la surjectivité est immédiate

**10636**: On peut utiliser les images et noyaux pour démontrer que des ensembles sont des sous-espaces vectoriels : ( a ) ( Image peu fréquent ) : est un sous-espace vectoriel de C 0 ( a , b , R ) en considérant l' endomorphisme de C 0 ( a , b , R ) : ( b ) ( Noyau très fréquent ) : ***est*** un sous-espace vectoriel de C 1 ( a , b , R ) en considérant la forme linéaire de C 1 ( a , b , R ) : Proposition 1.4 Soit E et E 0 deux K - espaces vectoriels , f L ( E , E 0 ) , alors : f est injective si , et seulement si , Ker(f ) 0E f est surjective si , et seulement si , Im(f ) E 0 Démonstration La caractérisation de la surjectivité est immédiate

**10644**: On peut utiliser les images et noyaux pour démontrer que des ensembles sont des sous-espaces vectoriels : ( a ) ( Image peu fréquent ) : est un sous-espace vectoriel de C 0 ( a , b , R ) en considérant l' endomorphisme de C 0 ( a , b , R ) : ( b ) ( Noyau très fréquent ) : est un sous-espace vectoriel de C 1 ( ***a*** , b , R ) en considérant la forme linéaire de C 1 ( a , b , R ) : Proposition 1.4 Soit E et E 0 deux K - espaces vectoriels , f L ( E , E 0 ) , alors : f est injective si , et seulement si , Ker(f ) 0E f est surjective si , et seulement si , Im(f ) E 0 Démonstration La caractérisation de la surjectivité est immédiate

**10659**: On peut utiliser les images et noyaux pour démontrer que des ensembles sont des sous-espaces vectoriels : ( a ) ( Image peu fréquent ) : est un sous-espace vectoriel de C 0 ( a , b , R ) en considérant l' endomorphisme de C 0 ( a , b , R ) : ( b ) ( Noyau très fréquent ) : est un sous-espace vectoriel de C 1 ( a , b , R ) en considérant la forme linéaire de C 1 ( ***a*** , b , R ) : Proposition 1.4 Soit E et E 0 deux K - espaces vectoriels , f L ( E , E 0 ) , alors : f est injective si , et seulement si , Ker(f ) 0E f est surjective si , et seulement si , Im(f ) E 0 Démonstration La caractérisation de la surjectivité est immédiate

**10691**: On peut utiliser les images et noyaux pour démontrer que des ensembles sont des sous-espaces vectoriels : ( a ) ( Image peu fréquent ) : est un sous-espace vectoriel de C 0 ( a , b , R ) en considérant l' endomorphisme de C 0 ( a , b , R ) : ( b ) ( Noyau très fréquent ) : est un sous-espace vectoriel de C 1 ( a , b , R ) en considérant la forme linéaire de C 1 ( a , b , R ) : Proposition 1.4 Soit E et E 0 deux K - espaces vectoriels , f L ( E , E 0 ) , alors : f ***est*** injective si , et seulement si , Ker(f ) 0E f est surjective si , et seulement si , Im(f ) E 0 Démonstration La caractérisation de la surjectivité est immédiate

**10703**: On peut utiliser les images et noyaux pour démontrer que des ensembles sont des sous-espaces vectoriels : ( a ) ( Image peu fréquent ) : est un sous-espace vectoriel de C 0 ( a , b , R ) en considérant l' endomorphisme de C 0 ( a , b , R ) : ( b ) ( Noyau très fréquent ) : est un sous-espace vectoriel de C 1 ( a , b , R ) en considérant la forme linéaire de C 1 ( a , b , R ) : Proposition 1.4 Soit E et E 0 deux K - espaces vectoriels , f L ( E , E 0 ) , alors : f est injective si , et seulement si , Ker(f ) 0E f ***est*** surjective si , et seulement si , Im(f ) E 0 Démonstration La caractérisation de la surjectivité est immédiate

**10721**: On peut utiliser les images et noyaux pour démontrer que des ensembles sont des sous-espaces vectoriels : ( a ) ( Image peu fréquent ) : est un sous-espace vectoriel de C 0 ( a , b , R ) en considérant l' endomorphisme de C 0 ( a , b , R ) : ( b ) ( Noyau très fréquent ) : est un sous-espace vectoriel de C 1 ( a , b , R ) en considérant la forme linéaire de C 1 ( a , b , R ) : Proposition 1.4 Soit E et E 0 deux K - espaces vectoriels , f L ( E , E 0 ) , alors : f est injective si , et seulement si , Ker(f ) 0E f est surjective si , et seulement si , Im(f ) E 0 Démonstration La caractérisation de la surjectivité ***est*** immédiate

**10729**: Pour l' injectivité , on ***utilise*** le fait que : Si E est un K - espace vectoriel et ( ei ) iI une base de E , alors pour connaître une application linéaire f L ( E , E 0 ) , il faut et il suffit de connaître la famille des images ( f ( ei ) ) iI

**10736**: Pour l' injectivité , on utilise le fait que : Si E ***est*** un K - espace vectoriel et ( ei ) iI une base de E , alors pour connaître une application linéaire f L ( E , E 0 ) , il faut et il suffit de connaître la famille des images ( f ( ei ) ) iI

**10768**: Pour l' injectivité , on utilise le fait que : Si E est un K - espace vectoriel et ( ei ) iI une base de E , alors pour connaître une application linéaire f L ( E , E 0 ) , il ***faut*** et il suffit de connaître la famille des images ( f ( ei ) ) iI

**10771**: Pour l' injectivité , on utilise le fait que : Si E est un K - espace vectoriel et ( ei ) iI une base de E , alors pour connaître une application linéaire f L ( E , E 0 ) , il faut et il ***suffit*** de connaître la famille des images ( f ( ei ) ) iI

**10789**: Plus formellement : ***est*** un isomorphisme C' est pourquoi il suffit de donner uniquement les images des vecteurs d' une base pour décrire une application linéaire

**10793**: Plus formellement : est un isomorphisme C' ***est*** pourquoi il suffit de donner uniquement les images des vecteurs d' une base pour décrire une application linéaire

**10796**: Plus formellement : est un isomorphisme C' est pourquoi il ***suffit*** de donner uniquement les images des vecteurs d' une base pour décrire une application linéaire

**10826**: Plus généralement , pour tout ( xi ) iI E 0 , il ***existe*** une unique application linéaire f L ( E , E 0 ) Soit E un K - espace vectoriel

**10863**: Si f L ( E , E 0 ) et si X ( xi ) iI ***est*** une famille de E , alors : f injective et X libre ( f ( xi ) ) iI libre f bijective et X base ( f ( xi ) ) iI base f surjective et X génératrice ( f ( xi ) ) iI génératrice X génératrice Ces propriétés sont vraies même si E est de dimension infinie ( mais ce sera surtout utile en dimension Démonstration 1

**10896**: Si f L ( E , E 0 ) et si X ( xi ) iI est une famille de E , alors : f injective et X libre ( f ( xi ) ) iI libre f bijective et X base ( f ( xi ) ) iI ***base*** f surjective et X génératrice ( f ( xi ) ) iI génératrice X génératrice Ces propriétés sont vraies même si E est de dimension infinie ( mais ce sera surtout utile en dimension Démonstration 1

**10914**: Si f L ( E , E 0 ) et si X ( xi ) iI est une famille de E , alors : f injective et X libre ( f ( xi ) ) iI libre f bijective et X base ( f ( xi ) ) iI base f surjective et X génératrice ( f ( xi ) ) iI génératrice X génératrice Ces propriétés ***sont*** vraies même si E est de dimension infinie ( mais ce sera surtout utile en dimension Démonstration 1

**10919**: Si f L ( E , E 0 ) et si X ( xi ) iI est une famille de E , alors : f injective et X libre ( f ( xi ) ) iI libre f bijective et X base ( f ( xi ) ) iI base f surjective et X génératrice ( f ( xi ) ) iI génératrice X génératrice Ces propriétés sont vraies même si E ***est*** de dimension infinie ( mais ce sera surtout utile en dimension Démonstration 1

**10967**: , p ) Kp tel que Par linéarité , on ***a*** donc i .xik Ker f Puisque f est injective , on a Ker f 0E donc Comme X est libre , on a 1 p 0 , ce qui démontre que ( f ( xi1 ) ,

**10975**: , p ) Kp tel que Par linéarité , on a donc i .xik Ker f Puisque f ***est*** injective , on a Ker f 0E donc Comme X est libre , on a 1 p 0 , ce qui démontre que ( f ( xi1 ) ,

**10979**: , p ) Kp tel que Par linéarité , on a donc i .xik Ker f Puisque f est injective , on ***a*** Ker f 0E donc Comme X est libre , on a 1 p 0 , ce qui démontre que ( f ( xi1 ) ,

**10986**: , p ) Kp tel que Par linéarité , on a donc i .xik Ker f Puisque f est injective , on a Ker f 0E donc Comme X ***est*** libre , on a 1 p 0 , ce qui démontre que ( f ( xi1 ) ,

**10990**: , p ) Kp tel que Par linéarité , on a donc i .xik Ker f Puisque f est injective , on a Ker f 0E donc Comme X est libre , on ***a*** 1 p 0 , ce qui démontre que ( f ( xi1 ) ,

**10997**: , p ) Kp tel que Par linéarité , on a donc i .xik Ker f Puisque f est injective , on a Ker f 0E donc Comme X est libre , on a 1 p 0 , ce qui ***démontre*** que ( f ( xi1 ) ,

**11014**: , f ( xip ) ) ***est*** libre , donc que ( f ( xi ) ) iI est libre

**11026**: , f ( xip ) ) est libre , donc que ( f ( xi ) ) iI ***est*** libre

**11038**: Comme f ***est*** surjective , il existe x E tel que y f ( x )

**11042**: Comme f est surjective , il ***existe*** x E tel que y f ( x )

**11055**: Puisque X ***est*** génératrice , il On a alors par linéarité de f ce qui démontre que y s' écrit comme une combinaison linéaire finie d' éléments de ( f ( xi ) ) iI

**11060**: Puisque X est génératrice , il On ***a*** alors par linéarité de f ce qui démontre que y s' écrit comme une combinaison linéaire finie d' éléments de ( f ( xi ) ) iI

**11068**: Puisque X est génératrice , il On a alors par linéarité de f ce qui ***démontre*** que y s' écrit comme une combinaison linéaire finie d' éléments de ( f ( xi ) ) iI

**11072**: Puisque X est génératrice , il On a alors par linéarité de f ce qui démontre que y s' ***écrit*** comme une combinaison linéaire finie d' éléments de ( f ( xi ) ) iI

**11098**: Finalement , ( f ( xi ) ) iI ***est*** génératrice

**11104**: On ***applique*** les points 1 et 2 pour démontrer que ( f ( xi ) ) iI est à la fois libre et génératrice

**11120**: On applique les points 1 et 2 pour démontrer que ( f ( xi ) ) iI ***est*** à la fois libre et génératrice

**11132**: Il s' ***agit*** du point 2 en remplaçant E 0 par Im f et en remarquant que f est surjective sur son image

**11148**: Il s' agit du point 2 en remplaçant E 0 par Im f et en remarquant que f ***est*** surjective sur son image

**11162**: En particulier , si ( ei ) iI ***est*** une base de E et si f L ( E , E 0 ) , alors 1

**11182**: f ***est*** surjective si , et seulement si , ( f ( ei ) ) iI engendre E 0 2

**11197**: f est surjective si , et seulement si , ( f ( ei ) ) iI ***engendre*** E 0 2

**11203**: f ***est*** injective si , et seulement si , ( f ( ei ) ) iI est libre 3

**11218**: f est injective si , et seulement si , ( f ( ei ) ) iI ***est*** libre 3

**11223**: f ***est*** bijective si , et seulement si , ( f ( ei ) ) iI est une base de E 0

**11238**: f est bijective si , et seulement si , ( f ( ei ) ) iI ***est*** une base de E 0

**11260**: On ***a*** : Démonstration Soit f et g deux endomorphismes d' un K - espace vectoriel E. Si f g g f , alors : Démonstration Si f est un endomorphisme d' un K - espace vectoriel E , alors : Démonstration On utilise les deux premiers points de la propriété 1.26 , de la présente page avec f n à la place de f et f à la place de g. Soit E un K - espace vectoriel et f L ( E ) , soit F un sous-espace vectoriel de E , on dit que F est f -stable ou stable par f si : f ( F ) F , c' est - à - dire que , pour tout x F , f ( x ) F 1

**11287**: On a : Démonstration Soit f et g deux endomorphismes d' un K - espace vectoriel E. Si f g g f , alors : Démonstration Si f ***est*** un endomorphisme d' un K - espace vectoriel E , alors : Démonstration On utilise les deux premiers points de la propriété 1.26 , de la présente page avec f n à la place de f et f à la place de g. Soit E un K - espace vectoriel et f L ( E ) , soit F un sous-espace vectoriel de E , on dit que F est f -stable ou stable par f si : f ( F ) F , c' est - à - dire que , pour tout x F , f ( x ) F 1

**11302**: On a : Démonstration Soit f et g deux endomorphismes d' un K - espace vectoriel E. Si f g g f , alors : Démonstration Si f est un endomorphisme d' un K - espace vectoriel E , alors : Démonstration On ***utilise*** les deux premiers points de la propriété 1.26 , de la présente page avec f n à la place de f et f à la place de g. Soit E un K - espace vectoriel et f L ( E ) , soit F un sous-espace vectoriel de E , on dit que F est f -stable ou stable par f si : f ( F ) F , c' est - à - dire que , pour tout x F , f ( x ) F 1

**11354**: On a : Démonstration Soit f et g deux endomorphismes d' un K - espace vectoriel E. Si f g g f , alors : Démonstration Si f est un endomorphisme d' un K - espace vectoriel E , alors : Démonstration On utilise les deux premiers points de la propriété 1.26 , de la présente page avec f n à la place de f et f à la place de g. Soit E un K - espace vectoriel et f L ( E ) , soit F un sous-espace vectoriel de E , on ***dit*** que F est f -stable ou stable par f si : f ( F ) F , c' est - à - dire que , pour tout x F , f ( x ) F 1

**11357**: On a : Démonstration Soit f et g deux endomorphismes d' un K - espace vectoriel E. Si f g g f , alors : Démonstration Si f est un endomorphisme d' un K - espace vectoriel E , alors : Démonstration On utilise les deux premiers points de la propriété 1.26 , de la présente page avec f n à la place de f et f à la place de g. Soit E un K - espace vectoriel et f L ( E ) , soit F un sous-espace vectoriel de E , on dit que F ***est*** f -stable ou stable par f si : f ( F ) F , c' est - à - dire que , pour tout x F , f ( x ) F 1

**11397**: Im(f ) et Ker(f ) ***sont*** stables par f

**11407**: Plus généralement , ***sont*** stables par f

**11416**: Si f ***est*** un automorphisme et F de dimension finie , alors F est stable par f F est stable par f 1 1.9.1 Soit f , g et h trois endomorphismes d' un K - espace vectoriel E tels que : Démontrer que ces trois endomorphismes ont même image et même noyau

**11427**: Si f est un automorphisme et F de dimension finie , alors F ***est*** stable par f F est stable par f 1 1.9.1 Soit f , g et h trois endomorphismes d' un K - espace vectoriel E tels que : Démontrer que ces trois endomorphismes ont même image et même noyau

**11432**: Si f est un automorphisme et F de dimension finie , alors F est stable par f F ***est*** stable par f 1 1.9.1 Soit f , g et h trois endomorphismes d' un K - espace vectoriel E tels que : Démontrer que ces trois endomorphismes ont même image et même noyau

**11461**: Si f est un automorphisme et F de dimension finie , alors F est stable par f F est stable par f 1 1.9.1 Soit f , g et h trois endomorphismes d' un K - espace vectoriel E tels que : Démontrer que ces trois endomorphismes ***ont*** même image et même noyau

**11527**: 1.9.2 Soit f un endomorphisme d' un K - espace vectoriel E , démontrer que : 1.9.3 Soit f et g deux endomorphismes d' un K - espace vectoriel E , démontrer que : Projecteurs et symétries Soit E un K - espace vectoriel , F et G deux sous-espaces vectoriels de E tels que E F G. On ***appelle*** projection de E sur F parallèlement à G l' endomorphisme de E défini par : x 7 xF où x xF xG avec xF F et xG G Puisque E F G , xF et xG sont uniques donc pF kG est bien définie

**11564**: 1.9.2 Soit f un endomorphisme d' un K - espace vectoriel E , démontrer que : 1.9.3 Soit f et g deux endomorphismes d' un K - espace vectoriel E , démontrer que : Projecteurs et symétries Soit E un K - espace vectoriel , F et G deux sous-espaces vectoriels de E tels que E F G. On appelle projection de E sur F parallèlement à G l' endomorphisme de E défini par : x 7 xF où x xF xG avec xF F et xG G Puisque E F G , xF et xG ***sont*** uniques donc pF kG est bien définie

**11569**: 1.9.2 Soit f un endomorphisme d' un K - espace vectoriel E , démontrer que : 1.9.3 Soit f et g deux endomorphismes d' un K - espace vectoriel E , démontrer que : Projecteurs et symétries Soit E un K - espace vectoriel , F et G deux sous-espaces vectoriels de E tels que E F G. On appelle projection de E sur F parallèlement à G l' endomorphisme de E défini par : x 7 xF où x xF xG avec xF F et xG G Puisque E F G , xF et xG sont uniques donc pF kG ***est*** bien définie

**11577**: De plus , c' ***est*** un endomorphisme Soit E un K - espace vectoriel , F et G deux sous-espaces vectoriels de E tels que E F G. Alors : pF kG pGkF idE On a de plus : G Ker(pF kG ) et , pour tout x G , pF kG ( x ) 0E F Im(pF kG ) Ker(pF kG idE ) et , pour tout x F , pF kG ( x ) x Démonstration Soit x E qu' on écrit de manière unique x xF xG avec xF pF kG ( x ) F et xG pGkF ( x ) G. donc pF kG pGkF idE

**11608**: De plus , c' est un endomorphisme Soit E un K - espace vectoriel , F et G deux sous-espaces vectoriels de E tels que E F G. Alors : pF kG pGkF idE On ***a*** de plus : G Ker(pF kG ) et , pour tout x G , pF kG ( x ) 0E F Im(pF kG ) Ker(pF kG idE ) et , pour tout x F , pF kG ( x ) x Démonstration Soit x E qu' on écrit de manière unique x xF xG avec xF pF kG ( x ) F et xG pGkF ( x ) G. donc pF kG pGkF idE

**11656**: De plus , c' est un endomorphisme Soit E un K - espace vectoriel , F et G deux sous-espaces vectoriels de E tels que E F G. Alors : pF kG pGkF idE On a de plus : G Ker(pF kG ) et , pour tout x G , pF kG ( x ) 0E F Im(pF kG ) Ker(pF kG idE ) et , pour tout x F , pF kG ( x ) x Démonstration Soit x E qu' on ***écrit*** de manière unique x xF xG avec xF pF kG ( x ) F et xG pGkF ( x ) G. donc pF kG pGkF idE

**11740**: Soit E un espace vectoriel , on ***appelle*** projecteur de E tout endomorphisme p de E tel que p p p. Proposition 1.5 Soit E un K - espace vectoriel

**11768**: Toute projection de E ***est*** un projecteur de E et , réciproquement , tout projecteur de E est une projection de E. Démonstration 1

**11781**: Toute projection de E est un projecteur de E et , réciproquement , tout projecteur de E ***est*** une projection de E. Démonstration 1

**11791**: Une projection ***est*** un projecteur , d' après la propriété 1.29 , de la présente page

**11836**: En effet , si F et G deux sous-espaces vectoriels de E tels que E F G et si p pF kG , alors pour tout x E qu' on ***écrit*** de manière unique x xF xG avec xF F et xG G , on a 2

**11851**: En effet , si F et G deux sous-espaces vectoriels de E tels que E F G et si p pF kG , alors pour tout x E qu' on écrit de manière unique x xF xG avec xF F et xG G , on ***a*** 2

**11856**: Si p ***est*** un projecteur , on a alors : E Im p Ker p d' après l' exemple 1.18 , page 49 , donc p est la projection de E sur son image Im p parallèlement à son noyau Ker p. Soit E un K - espace vectoriel , F et G deux sous-espaces vectoriels de E tels que E F G. On peut définir de même la notion de symétrie de E par rapport à F , parallèlement à G. C' est l' automorphisme de E défini x xF xG 7 xF xG , où x xF xG avec xF F et xG G Soit E un K - espace vectoriel , F et G deux sous-espaces vectoriels de E tels que E F G. On a alors : sF kG sF kG idE De plus , on a sF kG 2.pF kG idE et pF kG

**11861**: Si p est un projecteur , on ***a*** alors : E Im p Ker p d' après l' exemple 1.18 , page 49 , donc p est la projection de E sur son image Im p parallèlement à son noyau Ker p. Soit E un K - espace vectoriel , F et G deux sous-espaces vectoriels de E tels que E F G. On peut définir de même la notion de symétrie de E par rapport à F , parallèlement à G. C' est l' automorphisme de E défini x xF xG 7 xF xG , où x xF xG avec xF F et xG G Soit E un K - espace vectoriel , F et G deux sous-espaces vectoriels de E tels que E F G. On a alors : sF kG sF kG idE De plus , on a sF kG 2.pF kG idE et pF kG

**11880**: Si p est un projecteur , on a alors : E Im p Ker p d' après l' exemple 1.18 , page 49 , donc p ***est*** la projection de E sur son image Im p parallèlement à son noyau Ker p. Soit E un K - espace vectoriel , F et G deux sous-espaces vectoriels de E tels que E F G. On peut définir de même la notion de symétrie de E par rapport à F , parallèlement à G. C' est l' automorphisme de E défini x xF xG 7 xF xG , où x xF xG avec xF F et xG G Soit E un K - espace vectoriel , F et G deux sous-espaces vectoriels de E tels que E F G. On a alors : sF kG sF kG idE De plus , on a sF kG 2.pF kG idE et pF kG

**11918**: Si p est un projecteur , on a alors : E Im p Ker p d' après l' exemple 1.18 , page 49 , donc p est la projection de E sur son image Im p parallèlement à son noyau Ker p. Soit E un K - espace vectoriel , F et G deux sous-espaces vectoriels de E tels que E F G. On ***peut*** définir de même la notion de symétrie de E par rapport à F , parallèlement à G. C' est l' automorphisme de E défini x xF xG 7 xF xG , où x xF xG avec xF F et xG G Soit E un K - espace vectoriel , F et G deux sous-espaces vectoriels de E tels que E F G. On a alors : sF kG sF kG idE De plus , on a sF kG 2.pF kG idE et pF kG

**11937**: Si p est un projecteur , on a alors : E Im p Ker p d' après l' exemple 1.18 , page 49 , donc p est la projection de E sur son image Im p parallèlement à son noyau Ker p. Soit E un K - espace vectoriel , F et G deux sous-espaces vectoriels de E tels que E F G. On peut définir de même la notion de symétrie de E par rapport à F , parallèlement à G. C' ***est*** l' automorphisme de E défini x xF xG 7 xF xG , où x xF xG avec xF F et xG G Soit E un K - espace vectoriel , F et G deux sous-espaces vectoriels de E tels que E F G. On a alors : sF kG sF kG idE De plus , on a sF kG 2.pF kG idE et pF kG

**11982**: Si p est un projecteur , on a alors : E Im p Ker p d' après l' exemple 1.18 , page 49 , donc p est la projection de E sur son image Im p parallèlement à son noyau Ker p. Soit E un K - espace vectoriel , F et G deux sous-espaces vectoriels de E tels que E F G. On peut définir de même la notion de symétrie de E par rapport à F , parallèlement à G. C' est l' automorphisme de E défini x xF xG 7 xF xG , où x xF xG avec xF F et xG G Soit E un K - espace vectoriel , F et G deux sous-espaces vectoriels de E tels que E F G. On ***a*** alors : sF kG sF kG idE De plus , on a sF kG 2.pF kG idE et pF kG

**11994**: Si p est un projecteur , on a alors : E Im p Ker p d' après l' exemple 1.18 , page 49 , donc p est la projection de E sur son image Im p parallèlement à son noyau Ker p. Soit E un K - espace vectoriel , F et G deux sous-espaces vectoriels de E tels que E F G. On peut définir de même la notion de symétrie de E par rapport à F , parallèlement à G. C' est l' automorphisme de E défini x xF xG 7 xF xG , où x xF xG avec xF F et xG G Soit E un K - espace vectoriel , F et G deux sous-espaces vectoriels de E tels que E F G. On a alors : sF kG sF kG idE De plus , on ***a*** sF kG 2.pF kG idE et pF kG

**12032**: On ***peut*** définir plus généralement la notion de symétrie de E , ce sont les endomorphismes s de E tels que s s idE

**12044**: On peut définir plus généralement la notion de symétrie de E , ce ***sont*** les endomorphismes s de E tels que s s idE

**12057**: On ***peut*** alors démontrer ( à l' aide de la propriété 1.30 , de la présente page ) que si sF kG est la symétrie de E par rapport à F , parallèlement à G , alors sF kG sF kG idE

**12078**: On peut alors démontrer ( à l' aide de la propriété 1.30 , de la présente page ) que si sF kG ***est*** la symétrie de E par rapport à F , parallèlement à G , alors sF kG sF kG idE

**12103**: Réciproquement , si s ***est*** une symétrie de E , c' est - à - dire un endomorphisme s de E tel que s s idE , alors ( par un raisonnement par analyse - synthèse ) E Ker(s idE ) Ker(s idE ) et que s est la symétrie de E par rapport à Ker(s idE ) parallèlement à Ker(s idE )

**12146**: Réciproquement , si s est une symétrie de E , c' est - à - dire un endomorphisme s de E tel que s s idE , alors ( par un raisonnement par analyse - synthèse ) E Ker(s idE ) Ker(s idE ) et que s ***est*** la symétrie de E par rapport à Ker(s idE ) parallèlement à Ker(s idE )

**12167**: Enfin , si p ***est*** un projecteur de E , alors s 2.p idE est une symétrie

**12177**: Enfin , si p est un projecteur de E , alors s 2.p idE ***est*** une symétrie

**12185**: Réciproquement , si s ***est*** une symétrie de E , alors p 21 .(s idE ) est un projecteur de E. 1.10.1 Soit p est une projection d' un K - espace vectoriel E , et soit K 0 , 1

**12197**: Réciproquement , si s est une symétrie de E , alors p 21 .(s idE ) ***est*** un projecteur de E. 1.10.1 Soit p est une projection d' un K - espace vectoriel E , et soit K 0 , 1

**12205**: Réciproquement , si s est une symétrie de E , alors p 21 .(s idE ) est un projecteur de E. 1.10.1 Soit p ***est*** une projection d' un K - espace vectoriel E , et soit K 0 , 1

**12228**: idE ***est*** bijective

**12233**: Démontrer que ***est*** une projection ( sur quoi ? parallèlement à quoi ? )

**12253**: 1.10.3 Soit l' application : Démontrer que ***est*** une symétrie ( par rapport à quoi ? parallèlement à quoi ? )

**12291**: 1.10.4 Soit p et q deux projecteurs d' un K - espace vectoriel E. Démontrer l' équivalence des trois propriétés suivantes : ( ***a*** ) p q est un projecteur

**12295**: 1.10.4 Soit p et q deux projecteurs d' un K - espace vectoriel E. Démontrer l' équivalence des trois propriétés suivantes : ( a ) p q ***est*** un projecteur

**12304**: 1.10.5 Si p et q ***sont*** deux projecteurs d' un K - espace vectoriel E tels que p q q p 0L ( E ) , démontrer que p q est un projecteur et en calculer son image et son noyau

**12329**: 1.10.5 Si p et q sont deux projecteurs d' un K - espace vectoriel E tels que p q q p 0L ( E ) , démontrer que p q ***est*** un projecteur et en calculer son image et son noyau

**12346**: 1.10.6 Si p et q ***sont*** deux projecteurs d' un K - espace vectoriel E tels que p q 0L ( E ) , démontrer que p q q p est un projecteur et en calculer son image et son noyau

**12371**: 1.10.6 Si p et q sont deux projecteurs d' un K - espace vectoriel E tels que p q 0L ( E ) , démontrer que p q q p ***est*** un projecteur et en calculer son image et son noyau

**12424**: Si E ***est*** un K - espace vectoriel de dimension finie et si f L ( E , E 0 ) , alors Im(f ) est de dimension finie et dim Im(f ) dim E De plus , on a dim Im(f ) dim E f injective Démonstration La famille ( f ( e1 ) ,

**12447**: Si E est un K - espace vectoriel de dimension finie et si f L ( E , E 0 ) , alors Im(f ) ***est*** de dimension finie et dim Im(f ) dim E De plus , on a dim Im(f ) dim E f injective Démonstration La famille ( f ( e1 ) ,

**12461**: Si E est un K - espace vectoriel de dimension finie et si f L ( E , E 0 ) , alors Im(f ) est de dimension finie et dim Im(f ) dim E De plus , on ***a*** dim Im(f ) dim E f injective Démonstration La famille ( f ( e1 ) ,

**12487**: , f ( en ) ) ***est*** génératrice de Im f et à n éléments , donc dim Im(f ) n dim E. aussi

**12507**: C' ***est*** donc une base de Im f , d' où dim Im f n dim E. Si dim Im(f ) dim E , la famille génératrice ( f ( e1 ) ,

**12551**: , f ( en ) ) de Im f ***a*** n dim E éléments , c' est donc une base de Im f

**12558**: , f ( en ) ) de Im f a n dim E éléments , c' ***est*** donc une base de Im f

**12570**: En particulier , c' ***est*** une famille libre , donc f est injective

**12577**: En particulier , c' est une famille libre , donc f ***est*** injective

**12609**: Soit E et E 0 deux K - espaces vectoriels , f L ( E , E 0 ) telle que Im(f ) soit de dimension finie , on ***appelle*** rang de f et on note : rang(f ) dim(Im(f ) ) Lorsque E est de dimension finie , Im(f ) est de dimension finie

**12615**: Soit E et E 0 deux K - espaces vectoriels , f L ( E , E 0 ) telle que Im(f ) soit de dimension finie , on appelle rang de f et on ***note*** : rang(f ) dim(Im(f ) ) Lorsque E est de dimension finie , Im(f ) est de dimension finie

**12624**: Soit E et E 0 deux K - espaces vectoriels , f L ( E , E 0 ) telle que Im(f ) soit de dimension finie , on appelle rang de f et on note : rang(f ) dim(Im(f ) ) Lorsque E ***est*** de dimension finie , Im(f ) est de dimension finie

**12631**: Soit E et E 0 deux K - espaces vectoriels , f L ( E , E 0 ) telle que Im(f ) soit de dimension finie , on appelle rang de f et on note : rang(f ) dim(Im(f ) ) Lorsque E est de dimension finie , Im(f ) ***est*** de dimension finie

**12694**: Soit E et E 0 deux K - espaces vectoriels et f L ( E , E 0 ) , alors : E 0 de dimension finie rang(f ) dim E 0 et rang(f ) dim E 0 f surjective E de dimension finie rang(f ) dim E et rang(f ) dim E f injective Démonstration Im f ***est*** un sous-espace vectoriel de E 0 qui est de dimension finie donc Im f est de dimension finie et rang f dim Im f dim E 0

**12702**: Soit E et E 0 deux K - espaces vectoriels et f L ( E , E 0 ) , alors : E 0 de dimension finie rang(f ) dim E 0 et rang(f ) dim E 0 f surjective E de dimension finie rang(f ) dim E et rang(f ) dim E f injective Démonstration Im f est un sous-espace vectoriel de E 0 qui ***est*** de dimension finie donc Im f est de dimension finie et rang f dim Im f dim E 0

**12709**: Soit E et E 0 deux K - espaces vectoriels et f L ( E , E 0 ) , alors : E 0 de dimension finie rang(f ) dim E 0 et rang(f ) dim E 0 f surjective E de dimension finie rang(f ) dim E et rang(f ) dim E f injective Démonstration Im f est un sous-espace vectoriel de E 0 qui est de dimension finie donc Im f ***est*** de dimension finie et rang f dim Im f dim E 0

**12725**: Si f ***est*** surjective , alors Im f E 0 donc rang f dim Im f dim E 0

**12754**: Si dim Im f dim E 0 , comme Im f ***est*** un sous-espace vectoriel de E 0 de dimension finie , on a Im f E ( propriété 1.19 , page 43 ) donc f est surjective

**12766**: Si dim Im f dim E 0 , comme Im f est un sous-espace vectoriel de E 0 de dimension finie , on ***a*** Im f E ( propriété 1.19 , page 43 ) donc f est surjective

**12779**: Si dim Im f dim E 0 , comme Im f est un sous-espace vectoriel de E 0 de dimension finie , on a Im f E ( propriété 1.19 , page 43 ) donc f ***est*** surjective

**12783**: C' ***est*** la propriété 1.31 , page précédente

**12807**: Si E et E 0 ***sont*** de dimensions finies , alors : dim E dim E 0 f L ( E , E 0 ) injective dim E dim E 0 f L ( E , E 0 ) bijective dim E dim E 0 f L ( E , E 0 ) surjective En particulier , tout K - espace vectoriel de dimension finie n est isomorphe à Kn

**12868**: Si E et E 0 sont de dimensions finies , alors : dim E dim E 0 f L ( E , E 0 ) injective dim E dim E 0 f L ( E , E 0 ) bijective dim E dim E 0 f L ( E , E 0 ) surjective En particulier , tout K - espace vectoriel de dimension finie n ***est*** isomorphe à Kn

**12879**: Démonstration Si n p , on ***peut*** définir l' application linéaire f L ( E , E 0 ) telle que f ( ei ) e0i pour tout i 1 , n. elle est donc libre , ce qui démontre que f est injective ( voir la remarque 1.19 , page 51 )

**12906**: Démonstration Si n p , on peut définir l' application linéaire f L ( E , E 0 ) telle que f ( ei ) e0i pour tout i 1 , n. elle ***est*** donc libre , ce qui démontre que f est injective ( voir la remarque 1.19 , page 51 )

**12912**: Démonstration Si n p , on peut définir l' application linéaire f L ( E , E 0 ) telle que f ( ei ) e0i pour tout i 1 , n. elle est donc libre , ce qui ***démontre*** que f est injective ( voir la remarque 1.19 , page 51 )

**12915**: Démonstration Si n p , on peut définir l' application linéaire f L ( E , E 0 ) telle que f ( ei ) e0i pour tout i 1 , n. elle est donc libre , ce qui démontre que f ***est*** injective ( voir la remarque 1.19 , page 51 )

**12931**: Réciproquement , si il ***existe*** f L ( E , E 0 ) injective , alors ( f ( e1 ) ,

**12958**: , f ( en ) ) ***est*** une famille libre à n éléments dans un espace de dimension p , donc n p. Si n p , on peut définir l' application linéaire f L ( E , E 0 ) telle que f ( ei ) e0i pour tout i 1 , p et E 0 , donc est génératrice

**12980**: , f ( en ) ) est une famille libre à n éléments dans un espace de dimension p , donc n p. Si n p , on ***peut*** définir l' application linéaire f L ( E , E 0 ) telle que f ( ei ) e0i pour tout i 1 , p et E 0 , donc est génératrice

**13011**: , f ( en ) ) est une famille libre à n éléments dans un espace de dimension p , donc n p. Si n p , on peut définir l' application linéaire f L ( E , E 0 ) telle que f ( ei ) e0i pour tout i 1 , p et E 0 , donc ***est*** génératrice

**13018**: En particulier , f ***est*** surjective

**13025**: Réciproquement , si il ***existe*** f L ( E , E 0 ) surjective , alors ( f ( e1 ) ,

**13052**: , f ( en ) ) ***est*** une famille génératrice à n éléments dans un espace de dimension p , donc n p. Si n p , on peut définir l' application linéaire f L ( E , E 0 ) telle que f ( ei ) e0i pour tout i 1 , n 1 , p. D' après ce qui précède , f est injective et surjective donc bijective

**13074**: , f ( en ) ) est une famille génératrice à n éléments dans un espace de dimension p , donc n p. Si n p , on ***peut*** définir l' application linéaire f L ( E , E 0 ) telle que f ( ei ) e0i pour tout i 1 , n 1 , p. D' après ce qui précède , f est injective et surjective donc bijective

**13107**: , f ( en ) ) est une famille génératrice à n éléments dans un espace de dimension p , donc n p. Si n p , on peut définir l' application linéaire f L ( E , E 0 ) telle que f ( ei ) e0i pour tout i 1 , n 1 , p. D' après ce qui ***précède*** , f est injective et surjective donc bijective

**13110**: , f ( en ) ) est une famille génératrice à n éléments dans un espace de dimension p , donc n p. Si n p , on peut définir l' application linéaire f L ( E , E 0 ) telle que f ( ei ) e0i pour tout i 1 , n 1 , p. D' après ce qui précède , f ***est*** injective et surjective donc bijective

**13121**: Réciproquement , si il ***existe*** f L ( E , E 0 ) bijective , alors d' après ce qui précède n p et p n , donc Ces propriétés sont très importantes pour démontrer des égalités ou des inégalités de dimensions ! Proposition 1.6 Rang d' une composition Soit E , E 0 et E 00 trois K - espaces vectoriels de dimensions finies , u L ( E , E 0 ) et v L ( E 0 , E 00 ) , alors : rang(v u ) dim E 0 rang(v ) rang(u ) Démonstration On peut déjà remarquer que : donc , en introduisant des supplémentaires : l' inégalité demandée devient : Or , il existe une application naturelle qui va de F 0 dans F 00 , l' application : L' énoncé devient : La démonstration ne est alors qu' une vérification : soit x00 F 00 , alors x00 Im(v ) , donc , il existe Théorème 1.4 Théorème du rang Soit E et E 0 des K - espace vectoriels et f L ( E , E 0 )

**13137**: Réciproquement , si il existe f L ( E , E 0 ) bijective , alors d' après ce qui ***précède*** n p et p n , donc Ces propriétés sont très importantes pour démontrer des égalités ou des inégalités de dimensions ! Proposition 1.6 Rang d' une composition Soit E , E 0 et E 00 trois K - espaces vectoriels de dimensions finies , u L ( E , E 0 ) et v L ( E 0 , E 00 ) , alors : rang(v u ) dim E 0 rang(v ) rang(u ) Démonstration On peut déjà remarquer que : donc , en introduisant des supplémentaires : l' inégalité demandée devient : Or , il existe une application naturelle qui va de F 0 dans F 00 , l' application : L' énoncé devient : La démonstration ne est alors qu' une vérification : soit x00 F 00 , alors x00 Im(v ) , donc , il existe Théorème 1.4 Théorème du rang Soit E et E 0 des K - espace vectoriels et f L ( E , E 0 )

**13147**: Réciproquement , si il existe f L ( E , E 0 ) bijective , alors d' après ce qui précède n p et p n , donc Ces propriétés ***sont*** très importantes pour démontrer des égalités ou des inégalités de dimensions ! Proposition 1.6 Rang d' une composition Soit E , E 0 et E 00 trois K - espaces vectoriels de dimensions finies , u L ( E , E 0 ) et v L ( E 0 , E 00 ) , alors : rang(v u ) dim E 0 rang(v ) rang(u ) Démonstration On peut déjà remarquer que : donc , en introduisant des supplémentaires : l' inégalité demandée devient : Or , il existe une application naturelle qui va de F 0 dans F 00 , l' application : L' énoncé devient : La démonstration ne est alors qu' une vérification : soit x00 F 00 , alors x00 Im(v ) , donc , il existe Théorème 1.4 Théorème du rang Soit E et E 0 des K - espace vectoriels et f L ( E , E 0 )

**13216**: Réciproquement , si il existe f L ( E , E 0 ) bijective , alors d' après ce qui précède n p et p n , donc Ces propriétés sont très importantes pour démontrer des égalités ou des inégalités de dimensions ! Proposition 1.6 Rang d' une composition Soit E , E 0 et E 00 trois K - espaces vectoriels de dimensions finies , u L ( E , E 0 ) et v L ( E 0 , E 00 ) , alors : rang(v u ) dim E 0 rang(v ) rang(u ) Démonstration On ***peut*** déjà remarquer que : donc , en introduisant des supplémentaires : l' inégalité demandée devient : Or , il existe une application naturelle qui va de F 0 dans F 00 , l' application : L' énoncé devient : La démonstration ne est alors qu' une vérification : soit x00 F 00 , alors x00 Im(v ) , donc , il existe Théorème 1.4 Théorème du rang Soit E et E 0 des K - espace vectoriels et f L ( E , E 0 )

**13231**: Réciproquement , si il existe f L ( E , E 0 ) bijective , alors d' après ce qui précède n p et p n , donc Ces propriétés sont très importantes pour démontrer des égalités ou des inégalités de dimensions ! Proposition 1.6 Rang d' une composition Soit E , E 0 et E 00 trois K - espaces vectoriels de dimensions finies , u L ( E , E 0 ) et v L ( E 0 , E 00 ) , alors : rang(v u ) dim E 0 rang(v ) rang(u ) Démonstration On peut déjà remarquer que : donc , en introduisant des supplémentaires : l' inégalité demandée ***devient*** : Or , il existe une application naturelle qui va de F 0 dans F 00 , l' application : L' énoncé devient : La démonstration ne est alors qu' une vérification : soit x00 F 00 , alors x00 Im(v ) , donc , il existe Théorème 1.4 Théorème du rang Soit E et E 0 des K - espace vectoriels et f L ( E , E 0 )

**13236**: Réciproquement , si il existe f L ( E , E 0 ) bijective , alors d' après ce qui précède n p et p n , donc Ces propriétés sont très importantes pour démontrer des égalités ou des inégalités de dimensions ! Proposition 1.6 Rang d' une composition Soit E , E 0 et E 00 trois K - espaces vectoriels de dimensions finies , u L ( E , E 0 ) et v L ( E 0 , E 00 ) , alors : rang(v u ) dim E 0 rang(v ) rang(u ) Démonstration On peut déjà remarquer que : donc , en introduisant des supplémentaires : l' inégalité demandée devient : Or , il ***existe*** une application naturelle qui va de F 0 dans F 00 , l' application : L' énoncé devient : La démonstration ne est alors qu' une vérification : soit x00 F 00 , alors x00 Im(v ) , donc , il existe Théorème 1.4 Théorème du rang Soit E et E 0 des K - espace vectoriels et f L ( E , E 0 )

**13241**: Réciproquement , si il existe f L ( E , E 0 ) bijective , alors d' après ce qui précède n p et p n , donc Ces propriétés sont très importantes pour démontrer des égalités ou des inégalités de dimensions ! Proposition 1.6 Rang d' une composition Soit E , E 0 et E 00 trois K - espaces vectoriels de dimensions finies , u L ( E , E 0 ) et v L ( E 0 , E 00 ) , alors : rang(v u ) dim E 0 rang(v ) rang(u ) Démonstration On peut déjà remarquer que : donc , en introduisant des supplémentaires : l' inégalité demandée devient : Or , il existe une application naturelle qui ***va*** de F 0 dans F 00 , l' application : L' énoncé devient : La démonstration ne est alors qu' une vérification : soit x00 F 00 , alors x00 Im(v ) , donc , il existe Théorème 1.4 Théorème du rang Soit E et E 0 des K - espace vectoriels et f L ( E , E 0 )

**13254**: Réciproquement , si il existe f L ( E , E 0 ) bijective , alors d' après ce qui précède n p et p n , donc Ces propriétés sont très importantes pour démontrer des égalités ou des inégalités de dimensions ! Proposition 1.6 Rang d' une composition Soit E , E 0 et E 00 trois K - espaces vectoriels de dimensions finies , u L ( E , E 0 ) et v L ( E 0 , E 00 ) , alors : rang(v u ) dim E 0 rang(v ) rang(u ) Démonstration On peut déjà remarquer que : donc , en introduisant des supplémentaires : l' inégalité demandée devient : Or , il existe une application naturelle qui va de F 0 dans F 00 , l' application : L' énoncé ***devient*** : La démonstration ne est alors qu' une vérification : soit x00 F 00 , alors x00 Im(v ) , donc , il existe Théorème 1.4 Théorème du rang Soit E et E 0 des K - espace vectoriels et f L ( E , E 0 )

**13259**: Réciproquement , si il existe f L ( E , E 0 ) bijective , alors d' après ce qui précède n p et p n , donc Ces propriétés sont très importantes pour démontrer des égalités ou des inégalités de dimensions ! Proposition 1.6 Rang d' une composition Soit E , E 0 et E 00 trois K - espaces vectoriels de dimensions finies , u L ( E , E 0 ) et v L ( E 0 , E 00 ) , alors : rang(v u ) dim E 0 rang(v ) rang(u ) Démonstration On peut déjà remarquer que : donc , en introduisant des supplémentaires : l' inégalité demandée devient : Or , il existe une application naturelle qui va de F 0 dans F 00 , l' application : L' énoncé devient : La démonstration ne ***est*** alors qu' une vérification : soit x00 F 00 , alors x00 Im(v ) , donc , il existe Théorème 1.4 Théorème du rang Soit E et E 0 des K - espace vectoriels et f L ( E , E 0 )

**13278**: Réciproquement , si il existe f L ( E , E 0 ) bijective , alors d' après ce qui précède n p et p n , donc Ces propriétés sont très importantes pour démontrer des égalités ou des inégalités de dimensions ! Proposition 1.6 Rang d' une composition Soit E , E 0 et E 00 trois K - espaces vectoriels de dimensions finies , u L ( E , E 0 ) et v L ( E 0 , E 00 ) , alors : rang(v u ) dim E 0 rang(v ) rang(u ) Démonstration On peut déjà remarquer que : donc , en introduisant des supplémentaires : l' inégalité demandée devient : Or , il existe une application naturelle qui va de F 0 dans F 00 , l' application : L' énoncé devient : La démonstration ne est alors qu' une vérification : soit x00 F 00 , alors x00 Im(v ) , donc , il ***existe*** Théorème 1.4 Théorème du rang Soit E et E 0 des K - espace vectoriels et f L ( E , E 0 )

**13306**: Si E ***est*** de dimension finie , alors : rang(f ) dim E dim Ker(f ) Théorème du rang Démonstration C' est une application immédiate du théorème de factorisation ( théorème 1.5 , page 62 ) dans le cas de la dimension finie

**13325**: Si E est de dimension finie , alors : rang(f ) dim E dim Ker(f ) Théorème du rang Démonstration C' ***est*** une application immédiate du théorème de factorisation ( théorème 1.5 , page 62 ) dans le cas de la dimension finie

**13399**: Soit E un K - espace vectoriel de dimension finie et f L ( E ) , alors : ( ***a*** ) En posant f 0 idE , on a : ( b ) De plus , ( c ) On peut alors poser : Proposition 1.7 Caractérisation des automorphismes en dimension finie Soit E un K - espace vectoriel de dimension finie et f L ( E ) , alors : f injective f surjective f bijective C' est faux en dimension infinie

**13408**: Soit E un K - espace vectoriel de dimension finie et f L ( E ) , alors : ( a ) En posant f 0 idE , on ***a*** : ( b ) De plus , ( c ) On peut alors poser : Proposition 1.7 Caractérisation des automorphismes en dimension finie Soit E un K - espace vectoriel de dimension finie et f L ( E ) , alors : f injective f surjective f bijective C' est faux en dimension infinie

**13420**: Soit E un K - espace vectoriel de dimension finie et f L ( E ) , alors : ( a ) En posant f 0 idE , on a : ( b ) De plus , ( c ) On ***peut*** alors poser : Proposition 1.7 Caractérisation des automorphismes en dimension finie Soit E un K - espace vectoriel de dimension finie et f L ( E ) , alors : f injective f surjective f bijective C' est faux en dimension infinie

**13458**: Soit E un K - espace vectoriel de dimension finie et f L ( E ) , alors : ( a ) En posant f 0 idE , on a : ( b ) De plus , ( c ) On peut alors poser : Proposition 1.7 Caractérisation des automorphismes en dimension finie Soit E un K - espace vectoriel de dimension finie et f L ( E ) , alors : f injective f surjective f bijective C' ***est*** faux en dimension infinie

**13468**: C' ***est*** une application immédiate du théorème du rang

**13478**: On ***peut*** remarquer que le résultat est encore vrai lorsque f L ( E , E 0 ) et dim E dim E 0

**13483**: On peut remarquer que le résultat ***est*** encore vrai lorsque f L ( E , E 0 ) et dim E dim E 0

**13509**: En dimension infinie , on ***peut*** considérer , par exemple , la dérivation sur C ( R , R ) qui est surjective , mais pas injective ! Proposition 1.8 Soit E et E 0 deux K - espaces vectoriels de dimensions finies , alors L ( E E 0 ) est de dimension finie , égale à dim E dim E 0 Démonstration Si E 0E ou E 0 0E 0 , il ne y a rien à démontrer

**13525**: En dimension infinie , on peut considérer , par exemple , la dérivation sur C ( R , R ) qui ***est*** surjective , mais pas injective ! Proposition 1.8 Soit E et E 0 deux K - espaces vectoriels de dimensions finies , alors L ( E E 0 ) est de dimension finie , égale à dim E dim E 0 Démonstration Si E 0E ou E 0 0E 0 , il ne y a rien à démontrer

**13555**: En dimension infinie , on peut considérer , par exemple , la dérivation sur C ( R , R ) qui est surjective , mais pas injective ! Proposition 1.8 Soit E et E 0 deux K - espaces vectoriels de dimensions finies , alors L ( E E 0 ) ***est*** de dimension finie , égale à dim E dim E 0 Démonstration Si E 0E ou E 0 0E 0 , il ne y a rien à démontrer

**13580**: En dimension infinie , on peut considérer , par exemple , la dérivation sur C ( R , R ) qui est surjective , mais pas injective ! Proposition 1.8 Soit E et E 0 deux K - espaces vectoriels de dimensions finies , alors L ( E E 0 ) est de dimension finie , égale à dim E dim E 0 Démonstration Si E 0E ou E 0 0E 0 , il ne y ***a*** rien à démontrer

**13627**: e0n ) une base de E 0 , il ***suffit*** de vérifier alors que : où si ( k , l ) 1 , n 1 , p , uk , l est définie par a : a. On rappelle la définition du symbole de Kronecker : On voit donc que si E et E 0 sont de dimensions finies , alors L ( E E 0 ) et L ( E 0 E ) sont isomorphes car ils ont même dimension ! Nous verrons que c' est faux en dimension infinie ! Il y a en fait deux sortes d' isomorphismes : 1

**13650**: e0n ) une base de E 0 , il suffit de vérifier alors que : où si ( k , l ) 1 , n 1 , p , uk , l ***est*** définie par a : a. On rappelle la définition du symbole de Kronecker : On voit donc que si E et E 0 sont de dimensions finies , alors L ( E E 0 ) et L ( E 0 E ) sont isomorphes car ils ont même dimension ! Nous verrons que c' est faux en dimension infinie ! Il y a en fait deux sortes d' isomorphismes : 1

**13653**: e0n ) une base de E 0 , il suffit de vérifier alors que : où si ( k , l ) 1 , n 1 , p , uk , l est définie par ***a*** : a. On rappelle la définition du symbole de Kronecker : On voit donc que si E et E 0 sont de dimensions finies , alors L ( E E 0 ) et L ( E 0 E ) sont isomorphes car ils ont même dimension ! Nous verrons que c' est faux en dimension infinie ! Il y a en fait deux sortes d' isomorphismes : 1

**13655**: e0n ) une base de E 0 , il suffit de vérifier alors que : où si ( k , l ) 1 , n 1 , p , uk , l est définie par a : ***a.*** On rappelle la définition du symbole de Kronecker : On voit donc que si E et E 0 sont de dimensions finies , alors L ( E E 0 ) et L ( E 0 E ) sont isomorphes car ils ont même dimension ! Nous verrons que c' est faux en dimension infinie ! Il y a en fait deux sortes d' isomorphismes : 1

**13657**: e0n ) une base de E 0 , il suffit de vérifier alors que : où si ( k , l ) 1 , n 1 , p , uk , l est définie par a : a. On ***rappelle*** la définition du symbole de Kronecker : On voit donc que si E et E 0 sont de dimensions finies , alors L ( E E 0 ) et L ( E 0 E ) sont isomorphes car ils ont même dimension ! Nous verrons que c' est faux en dimension infinie ! Il y a en fait deux sortes d' isomorphismes : 1

**13666**: e0n ) une base de E 0 , il suffit de vérifier alors que : où si ( k , l ) 1 , n 1 , p , uk , l est définie par a : a. On rappelle la définition du symbole de Kronecker : On ***voit*** donc que si E et E 0 sont de dimensions finies , alors L ( E E 0 ) et L ( E 0 E ) sont isomorphes car ils ont même dimension ! Nous verrons que c' est faux en dimension infinie ! Il y a en fait deux sortes d' isomorphismes : 1

**13674**: e0n ) une base de E 0 , il suffit de vérifier alors que : où si ( k , l ) 1 , n 1 , p , uk , l est définie par a : a. On rappelle la définition du symbole de Kronecker : On voit donc que si E et E 0 ***sont*** de dimensions finies , alors L ( E E 0 ) et L ( E 0 E ) sont isomorphes car ils ont même dimension ! Nous verrons que c' est faux en dimension infinie ! Il y a en fait deux sortes d' isomorphismes : 1

**13693**: e0n ) une base de E 0 , il suffit de vérifier alors que : où si ( k , l ) 1 , n 1 , p , uk , l est définie par a : a. On rappelle la définition du symbole de Kronecker : On voit donc que si E et E 0 sont de dimensions finies , alors L ( E E 0 ) et L ( E 0 E ) ***sont*** isomorphes car ils ont même dimension ! Nous verrons que c' est faux en dimension infinie ! Il y a en fait deux sortes d' isomorphismes : 1

**13697**: e0n ) une base de E 0 , il suffit de vérifier alors que : où si ( k , l ) 1 , n 1 , p , uk , l est définie par a : a. On rappelle la définition du symbole de Kronecker : On voit donc que si E et E 0 sont de dimensions finies , alors L ( E E 0 ) et L ( E 0 E ) sont isomorphes car ils ***ont*** même dimension ! Nous verrons que c' est faux en dimension infinie ! Il y a en fait deux sortes d' isomorphismes : 1

**13705**: e0n ) une base de E 0 , il suffit de vérifier alors que : où si ( k , l ) 1 , n 1 , p , uk , l est définie par a : a. On rappelle la définition du symbole de Kronecker : On voit donc que si E et E 0 sont de dimensions finies , alors L ( E E 0 ) et L ( E 0 E ) sont isomorphes car ils ont même dimension ! Nous verrons que c' ***est*** faux en dimension infinie ! Il y a en fait deux sortes d' isomorphismes : 1

**13713**: e0n ) une base de E 0 , il suffit de vérifier alors que : où si ( k , l ) 1 , n 1 , p , uk , l est définie par a : a. On rappelle la définition du symbole de Kronecker : On voit donc que si E et E 0 sont de dimensions finies , alors L ( E E 0 ) et L ( E 0 E ) sont isomorphes car ils ont même dimension ! Nous verrons que c' est faux en dimension infinie ! Il y ***a*** en fait deux sortes d' isomorphismes : 1

**13755**: Des isomorphismes non géométriques : c' est - à - dire qu' ils ne ***traduisent*** qu' une égalité de dimensions

**13793**: Démontrer ( ***a*** ) Pour toute L ( E 0 , E ) surjective , rang(f ) rang f

**13853**: 1.11.3 Soit E un K - espace vectoriel de dimension n et f L ( E ) tel que : ( ***a*** ) Démontrer que : ( b ) Démontrer que , si f n1 6 0L ( E ) , alors 1.11.4 Soit E un K - espace vectoriel de dimension finie et f L ( E ) , démontrer qu' il existe un automorphisme g GL ( E ) et p un projecteur de E tel que : 1.11.5 Soit E un K - espace vectoriel de dimension finie et f L ( E ) , on considère les applications : ( a ) Démontrer que et sont dans L ( L ( E ) )

**13895**: 1.11.3 Soit E un K - espace vectoriel de dimension n et f L ( E ) tel que : ( a ) Démontrer que : ( b ) Démontrer que , si f n1 6 0L ( E ) , alors 1.11.4 Soit E un K - espace vectoriel de dimension finie et f L ( E ) , démontrer qu' il ***existe*** un automorphisme g GL ( E ) et p un projecteur de E tel que : 1.11.5 Soit E un K - espace vectoriel de dimension finie et f L ( E ) , on considère les applications : ( a ) Démontrer que et sont dans L ( L ( E ) )

**13931**: 1.11.3 Soit E un K - espace vectoriel de dimension n et f L ( E ) tel que : ( a ) Démontrer que : ( b ) Démontrer que , si f n1 6 0L ( E ) , alors 1.11.4 Soit E un K - espace vectoriel de dimension finie et f L ( E ) , démontrer qu' il existe un automorphisme g GL ( E ) et p un projecteur de E tel que : 1.11.5 Soit E un K - espace vectoriel de dimension finie et f L ( E ) , on ***considère*** les applications : ( a ) Démontrer que et sont dans L ( L ( E ) )

**13936**: 1.11.3 Soit E un K - espace vectoriel de dimension n et f L ( E ) tel que : ( a ) Démontrer que : ( b ) Démontrer que , si f n1 6 0L ( E ) , alors 1.11.4 Soit E un K - espace vectoriel de dimension finie et f L ( E ) , démontrer qu' il existe un automorphisme g GL ( E ) et p un projecteur de E tel que : 1.11.5 Soit E un K - espace vectoriel de dimension finie et f L ( E ) , on considère les applications : ( ***a*** ) Démontrer que et sont dans L ( L ( E ) )

**13941**: 1.11.3 Soit E un K - espace vectoriel de dimension n et f L ( E ) tel que : ( a ) Démontrer que : ( b ) Démontrer que , si f n1 6 0L ( E ) , alors 1.11.4 Soit E un K - espace vectoriel de dimension finie et f L ( E ) , démontrer qu' il existe un automorphisme g GL ( E ) et p un projecteur de E tel que : 1.11.5 Soit E un K - espace vectoriel de dimension finie et f L ( E ) , on considère les applications : ( a ) Démontrer que et ***sont*** dans L ( L ( E ) )

**13979**: On ***considère*** ( a ) Démontrer que F est non vide

**13981**: On considère ( ***a*** ) Démontrer que F est non vide

**13986**: On considère ( a ) Démontrer que F ***est*** non vide

**14006**: ( b ) Soit v0 F , démontrer que v0 F v v0 , v F ***est*** un sous-espace vectoriel de L ( E ) On suppose dorénavant que E est de dimension finie n. ( c ) Calculer la dimension de v0 F. ( d ) Que peut -on dire du rang de v lorsque v F ? 1.11.7 Soit E un K - espace vectoriel de dimension finie et f L ( E ) , démontrer que : rang f k rang f k2 1.11.8 Soit E un K - espace vectoriel de dimension finie et f L ( E ) , démontrer que : 1.11.9 Soit E , E 0 , E 00 et E 000 quatre K - espaces vectoriels de dimensions finies , f L ( E , E 0 ) , g L ( E 0 , E 00 ) 1.11.10 Soit E un K - espace vectoriel de dimension finie , f L ( E ) GL ( E )

**14016**: ( b ) Soit v0 F , démontrer que v0 F v v0 , v F est un sous-espace vectoriel de L ( E ) On ***suppose*** dorénavant que E est de dimension finie n. ( c ) Calculer la dimension de v0 F. ( d ) Que peut -on dire du rang de v lorsque v F ? 1.11.7 Soit E un K - espace vectoriel de dimension finie et f L ( E ) , démontrer que : rang f k rang f k2 1.11.8 Soit E un K - espace vectoriel de dimension finie et f L ( E ) , démontrer que : 1.11.9 Soit E , E 0 , E 00 et E 000 quatre K - espaces vectoriels de dimensions finies , f L ( E , E 0 ) , g L ( E 0 , E 00 ) 1.11.10 Soit E un K - espace vectoriel de dimension finie , f L ( E ) GL ( E )

**14020**: ( b ) Soit v0 F , démontrer que v0 F v v0 , v F est un sous-espace vectoriel de L ( E ) On suppose dorénavant que E ***est*** de dimension finie n. ( c ) Calculer la dimension de v0 F. ( d ) Que peut -on dire du rang de v lorsque v F ? 1.11.7 Soit E un K - espace vectoriel de dimension finie et f L ( E ) , démontrer que : rang f k rang f k2 1.11.8 Soit E un K - espace vectoriel de dimension finie et f L ( E ) , démontrer que : 1.11.9 Soit E , E 0 , E 00 et E 000 quatre K - espaces vectoriels de dimensions finies , f L ( E , E 0 ) , g L ( E 0 , E 00 ) 1.11.10 Soit E un K - espace vectoriel de dimension finie , f L ( E ) GL ( E )

**14038**: ( b ) Soit v0 F , démontrer que v0 F v v0 , v F est un sous-espace vectoriel de L ( E ) On suppose dorénavant que E est de dimension finie n. ( c ) Calculer la dimension de v0 F. ( d ) Que ***peut*** -on dire du rang de v lorsque v F ? 1.11.7 Soit E un K - espace vectoriel de dimension finie et f L ( E ) , démontrer que : rang f k rang f k2 1.11.8 Soit E un K - espace vectoriel de dimension finie et f L ( E ) , démontrer que : 1.11.9 Soit E , E 0 , E 00 et E 000 quatre K - espaces vectoriels de dimensions finies , f L ( E , E 0 ) , g L ( E 0 , E 00 ) 1.11.10 Soit E un K - espace vectoriel de dimension finie , f L ( E ) GL ( E )

**14159**: ( ***a*** ) Démontrer que : Démontrer que c' est un K - espace vectoriel et en calculer sa dimension

**14167**: ( a ) Démontrer que : Démontrer que c' ***est*** un K - espace vectoriel et en calculer sa dimension

**14185**: Factorisation des applications linéaires Si F ***est*** un sous-espace vectoriel d' un K - espace vectoriel E , alors la restriction d' une application linéaire f L ( E , E 0 ) , notée fF et définie par : est une application linéaire de F dans E 0

**14219**: Factorisation des applications linéaires Si F est un sous-espace vectoriel d' un K - espace vectoriel E , alors la restriction d' une application linéaire f L ( E , E 0 ) , notée fF et définie par : ***est*** une application linéaire de F dans E 0

**14232**: De plus , ***est*** une application linéaire Démonstration Si f L ( E , E 0 ) et F 0 est un sous-espace vectoriel de E 0 tel que de f , notée f , alors la co-restriction et définie par : est une application linéaire de E dans F 0

**14249**: De plus , est une application linéaire Démonstration Si f L ( E , E 0 ) et F 0 ***est*** un sous-espace vectoriel de E 0 tel que de f , notée f , alors la co-restriction et définie par : est une application linéaire de E dans F 0

**14271**: De plus , est une application linéaire Démonstration Si f L ( E , E 0 ) et F 0 est un sous-espace vectoriel de E 0 tel que de f , notée f , alors la co-restriction et définie par : ***est*** une application linéaire de E dans F 0

**14284**: De plus , ***est*** une application linéaire Démonstration Notation 1.1 Par abus de notation , si F est un sous-espace vectoriel d' un K - espace vectoriel E , F 0 un sous-espace vectoriel d' un K - espace vectoriel E 0 et f L ( E , E 0 ) tels que , nous noterons : Soit E , E 0 et E 00 des K - espaces vectoriels

**14298**: De plus , est une application linéaire Démonstration Notation 1.1 Par abus de notation , si F ***est*** un sous-espace vectoriel d' un K - espace vectoriel E , F 0 un sous-espace vectoriel d' un K - espace vectoriel E 0 et f L ( E , E 0 ) tels que , nous noterons : Soit E , E 0 et E 00 des K - espaces vectoriels

**14354**: Si F ***est*** un sous-espace vectoriel de E , si f L ( E , E 0 ) et g L ( E 0 , E 00 ) alors : De même , si F 00 est un sous-espace vectoriel de E 00 contenant g(E 0 ) , alors : Si F est un sous-espace vectoriel d' un K - espace vectoriel E et f une application linéaire de E dans E 0 , Ker fF F Ker(f ) Démonstration Soit x Ker fF

**14388**: Si F est un sous-espace vectoriel de E , si f L ( E , E 0 ) et g L ( E 0 , E 00 ) alors : De même , si F 00 ***est*** un sous-espace vectoriel de E 00 contenant g(E 0 ) , alors : Si F est un sous-espace vectoriel d' un K - espace vectoriel E et f une application linéaire de E dans E 0 , Ker fF F Ker(f ) Démonstration Soit x Ker fF

**14404**: Si F est un sous-espace vectoriel de E , si f L ( E , E 0 ) et g L ( E 0 , E 00 ) alors : De même , si F 00 est un sous-espace vectoriel de E 00 contenant g(E 0 ) , alors : Si F ***est*** un sous-espace vectoriel d' un K - espace vectoriel E et f une application linéaire de E dans E 0 , Ker fF F Ker(f ) Démonstration Soit x Ker fF

**14448**: Alors x F ( car l' ensemble de départ de fF ***est*** F ) donc f ( x ) fF ( x ) 0E 0 donc Par double inclusion , Ker fF F Ker(f )

**14493**: Notation 1.2 Inclusion canonique Soit E un K - espace vectoriel , F un sous-espace vectoriel de E , on ***peut*** définir l' inclusion canonique de F dans E par : On a donc : iF E ( idE ) F Si G est un supplémentaire de F dans E ( E F G ) , on a : iF E idF Ce ne sont pas des réciproques ! Pour connaître une application linéaire définie sur un K - espace vectoriel dont on connaît une décomposition en somme directe , il faut et il suffit d' en connaître ses restrictions à chaque sous-espace vectoriel composant la décomposition

**14505**: Notation 1.2 Inclusion canonique Soit E un K - espace vectoriel , F un sous-espace vectoriel de E , on peut définir l' inclusion canonique de F dans E par : On ***a*** donc : iF E ( idE ) F Si G est un supplémentaire de F dans E ( E F G ) , on a : iF E idF Ce ne sont pas des réciproques ! Pour connaître une application linéaire définie sur un K - espace vectoriel dont on connaît une décomposition en somme directe , il faut et il suffit d' en connaître ses restrictions à chaque sous-espace vectoriel composant la décomposition

**14516**: Notation 1.2 Inclusion canonique Soit E un K - espace vectoriel , F un sous-espace vectoriel de E , on peut définir l' inclusion canonique de F dans E par : On a donc : iF E ( idE ) F Si G ***est*** un supplémentaire de F dans E ( E F G ) , on a : iF E idF Ce ne sont pas des réciproques ! Pour connaître une application linéaire définie sur un K - espace vectoriel dont on connaît une décomposition en somme directe , il faut et il suffit d' en connaître ses restrictions à chaque sous-espace vectoriel composant la décomposition

**14530**: Notation 1.2 Inclusion canonique Soit E un K - espace vectoriel , F un sous-espace vectoriel de E , on peut définir l' inclusion canonique de F dans E par : On a donc : iF E ( idE ) F Si G est un supplémentaire de F dans E ( E F G ) , on ***a*** : iF E idF Ce ne sont pas des réciproques ! Pour connaître une application linéaire définie sur un K - espace vectoriel dont on connaît une décomposition en somme directe , il faut et il suffit d' en connaître ses restrictions à chaque sous-espace vectoriel composant la décomposition

**14537**: Notation 1.2 Inclusion canonique Soit E un K - espace vectoriel , F un sous-espace vectoriel de E , on peut définir l' inclusion canonique de F dans E par : On a donc : iF E ( idE ) F Si G est un supplémentaire de F dans E ( E F G ) , on a : iF E idF Ce ne ***sont*** pas des réciproques ! Pour connaître une application linéaire définie sur un K - espace vectoriel dont on connaît une décomposition en somme directe , il faut et il suffit d' en connaître ses restrictions à chaque sous-espace vectoriel composant la décomposition

**14556**: Notation 1.2 Inclusion canonique Soit E un K - espace vectoriel , F un sous-espace vectoriel de E , on peut définir l' inclusion canonique de F dans E par : On a donc : iF E ( idE ) F Si G est un supplémentaire de F dans E ( E F G ) , on a : iF E idF Ce ne sont pas des réciproques ! Pour connaître une application linéaire définie sur un K - espace vectoriel dont on ***connaît*** une décomposition en somme directe , il faut et il suffit d' en connaître ses restrictions à chaque sous-espace vectoriel composant la décomposition

**14564**: Notation 1.2 Inclusion canonique Soit E un K - espace vectoriel , F un sous-espace vectoriel de E , on peut définir l' inclusion canonique de F dans E par : On a donc : iF E ( idE ) F Si G est un supplémentaire de F dans E ( E F G ) , on a : iF E idF Ce ne sont pas des réciproques ! Pour connaître une application linéaire définie sur un K - espace vectoriel dont on connaît une décomposition en somme directe , il ***faut*** et il suffit d' en connaître ses restrictions à chaque sous-espace vectoriel composant la décomposition

**14567**: Notation 1.2 Inclusion canonique Soit E un K - espace vectoriel , F un sous-espace vectoriel de E , on peut définir l' inclusion canonique de F dans E par : On a donc : iF E ( idE ) F Si G est un supplémentaire de F dans E ( E F G ) , on a : iF E idF Ce ne sont pas des réciproques ! Pour connaître une application linéaire définie sur un K - espace vectoriel dont on connaît une décomposition en somme directe , il faut et il ***suffit*** d' en connaître ses restrictions à chaque sous-espace vectoriel composant la décomposition

**14582**: Démonstration ***Supposons*** que : 1

**14587**: ***Supposons*** connue f L ( E , E 0 ) , il est alors facile de connaître : 2

**14599**: Supposons connue f L ( E , E 0 ) , il ***est*** alors facile de connaître : 2

**14611**: Réciproquement , si on ***connaît*** : alors on connaît f car si x E , il existe un entier n , des indices 2 à 2 distincts ( i1 ,

**14615**: Réciproquement , si on connaît : alors on ***connaît*** f car si x E , il existe un entier n , des indices 2 à 2 distincts ( i1 ,

**14623**: Réciproquement , si on connaît : alors on connaît f car si x E , il ***existe*** un entier n , des indices 2 à 2 distincts ( i1 ,

**14661**: , in ) de I et des vecteurs xik Eik , quelque soit k 1 , n tels que : On ***a*** alors : Une autre façon de dire est que : Ei L ( E , E 0 ) est isomorphe à grâce à l' application ( clairement linéaire ) : Soit E F G , alors il existe un endomorphisme f L ( E ) , tel que On construit a : a. Remarquons que l' on obtient bien sûr la projection sur G parallèlement à F

**14669**: , in ) de I et des vecteurs xik Eik , quelque soit k 1 , n tels que : On a alors : Une autre façon de dire ***est*** que : Ei L ( E , E 0 ) est isomorphe à grâce à l' application ( clairement linéaire ) : Soit E F G , alors il existe un endomorphisme f L ( E ) , tel que On construit a : a. Remarquons que l' on obtient bien sûr la projection sur G parallèlement à F

**14680**: , in ) de I et des vecteurs xik Eik , quelque soit k 1 , n tels que : On a alors : Une autre façon de dire est que : Ei L ( E , E 0 ) ***est*** isomorphe à grâce à l' application ( clairement linéaire ) : Soit E F G , alors il existe un endomorphisme f L ( E ) , tel que On construit a : a. Remarquons que l' on obtient bien sûr la projection sur G parallèlement à F

**14699**: , in ) de I et des vecteurs xik Eik , quelque soit k 1 , n tels que : On a alors : Une autre façon de dire est que : Ei L ( E , E 0 ) est isomorphe à grâce à l' application ( clairement linéaire ) : Soit E F G , alors il ***existe*** un endomorphisme f L ( E ) , tel que On construit a : a. Remarquons que l' on obtient bien sûr la projection sur G parallèlement à F

**14711**: , in ) de I et des vecteurs xik Eik , quelque soit k 1 , n tels que : On a alors : Une autre façon de dire est que : Ei L ( E , E 0 ) est isomorphe à grâce à l' application ( clairement linéaire ) : Soit E F G , alors il existe un endomorphisme f L ( E ) , tel que On ***construit*** a : a. Remarquons que l' on obtient bien sûr la projection sur G parallèlement à F

**14712**: , in ) de I et des vecteurs xik Eik , quelque soit k 1 , n tels que : On a alors : Une autre façon de dire est que : Ei L ( E , E 0 ) est isomorphe à grâce à l' application ( clairement linéaire ) : Soit E F G , alors il existe un endomorphisme f L ( E ) , tel que On construit ***a*** : a. Remarquons que l' on obtient bien sûr la projection sur G parallèlement à F

**14714**: , in ) de I et des vecteurs xik Eik , quelque soit k 1 , n tels que : On a alors : Une autre façon de dire est que : Ei L ( E , E 0 ) est isomorphe à grâce à l' application ( clairement linéaire ) : Soit E F G , alors il existe un endomorphisme f L ( E ) , tel que On construit a : ***a.*** Remarquons que l' on obtient bien sûr la projection sur G parallèlement à F

**14719**: , in ) de I et des vecteurs xik Eik , quelque soit k 1 , n tels que : On a alors : Une autre façon de dire est que : Ei L ( E , E 0 ) est isomorphe à grâce à l' application ( clairement linéaire ) : Soit E F G , alors il existe un endomorphisme f L ( E ) , tel que On construit a : a. Remarquons que l' on ***obtient*** bien sûr la projection sur G parallèlement à F

**14772**: Théorème 1.5 Factorisation des applications linéaires Soit E et E 0 deux K - espaces vectoriels , f L ( E , E 0 ) , F un supplémentaire de Ker(f ) dans E ( E Ker(f ) F ) , alors ***est*** un isomorphisme entre F et Im(f ) Démonstration Posons fe f F 1

**14781**: Théorème 1.5 Factorisation des applications linéaires Soit E et E 0 deux K - espaces vectoriels , f L ( E , E 0 ) , F un supplémentaire de Ker(f ) dans E ( E Ker(f ) F ) , alors est un isomorphisme entre F et Im(f ) Démonstration ***Posons*** fe f F 1

**14788**: fe ***est*** injective

**14797**: fe ***est*** surjective

**14809**: En effet , soit x0 Im(f ) , on ***sait*** alors qu' il existe x E , tel que f ( x ) x0

**14813**: En effet , soit x0 Im(f ) , on sait alors qu' il ***existe*** x E , tel que f ( x ) x0

**14834**: Mais , par décomposition en somme directe , on ***a*** : En appliquant f , il vient : Dans le cas de la dimension finie ( dim E ) , on obtient que Im(f ) est de dimension finie et que dim Im(f ) dim F Or , la formule de Grasmann ( proposition 1.3 , page 44 ) nous donne que dim F dim E dim Ker(f ) soit , le théorème du rang ! ( théorème 1.4 , page 57 ) Pourquoi appelle -t -on ce résultat théorème de factorisation des applications linéaires ? Si on regarde le diagramme 1.3 , de la présente page

**14841**: Mais , par décomposition en somme directe , on a : En appliquant f , il ***vient*** : Dans le cas de la dimension finie ( dim E ) , on obtient que Im(f ) est de dimension finie et que dim Im(f ) dim F Or , la formule de Grasmann ( proposition 1.3 , page 44 ) nous donne que dim F dim E dim Ker(f ) soit , le théorème du rang ! ( théorème 1.4 , page 57 ) Pourquoi appelle -t -on ce résultat théorème de factorisation des applications linéaires ? Si on regarde le diagramme 1.3 , de la présente page

**14856**: Mais , par décomposition en somme directe , on a : En appliquant f , il vient : Dans le cas de la dimension finie ( dim E ) , on ***obtient*** que Im(f ) est de dimension finie et que dim Im(f ) dim F Or , la formule de Grasmann ( proposition 1.3 , page 44 ) nous donne que dim F dim E dim Ker(f ) soit , le théorème du rang ! ( théorème 1.4 , page 57 ) Pourquoi appelle -t -on ce résultat théorème de factorisation des applications linéaires ? Si on regarde le diagramme 1.3 , de la présente page

**14860**: Mais , par décomposition en somme directe , on a : En appliquant f , il vient : Dans le cas de la dimension finie ( dim E ) , on obtient que Im(f ) ***est*** de dimension finie et que dim Im(f ) dim F Or , la formule de Grasmann ( proposition 1.3 , page 44 ) nous donne que dim F dim E dim Ker(f ) soit , le théorème du rang ! ( théorème 1.4 , page 57 ) Pourquoi appelle -t -on ce résultat théorème de factorisation des applications linéaires ? Si on regarde le diagramme 1.3 , de la présente page

**14885**: Mais , par décomposition en somme directe , on a : En appliquant f , il vient : Dans le cas de la dimension finie ( dim E ) , on obtient que Im(f ) est de dimension finie et que dim Im(f ) dim F Or , la formule de Grasmann ( proposition 1.3 , page 44 ) nous ***donne*** que dim F dim E dim Ker(f ) soit , le théorème du rang ! ( théorème 1.4 , page 57 ) Pourquoi appelle -t -on ce résultat théorème de factorisation des applications linéaires ? Si on regarde le diagramme 1.3 , de la présente page

**14909**: Mais , par décomposition en somme directe , on a : En appliquant f , il vient : Dans le cas de la dimension finie ( dim E ) , on obtient que Im(f ) est de dimension finie et que dim Im(f ) dim F Or , la formule de Grasmann ( proposition 1.3 , page 44 ) nous donne que dim F dim E dim Ker(f ) soit , le théorème du rang ! ( théorème 1.4 , page 57 ) Pourquoi ***appelle*** -t -on ce résultat théorème de factorisation des applications linéaires ? Si on regarde le diagramme 1.3 , de la présente page

**14923**: Mais , par décomposition en somme directe , on a : En appliquant f , il vient : Dans le cas de la dimension finie ( dim E ) , on obtient que Im(f ) est de dimension finie et que dim Im(f ) dim F Or , la formule de Grasmann ( proposition 1.3 , page 44 ) nous donne que dim F dim E dim Ker(f ) soit , le théorème du rang ! ( théorème 1.4 , page 57 ) Pourquoi appelle -t -on ce résultat théorème de factorisation des applications linéaires ? Si on ***regarde*** le diagramme 1.3 , de la présente page

**14961**: On a donc obtenu une factorisation de f à l' aide d' applications linéaires simples et d' un isomorphisme : f iIm(f ) E0 fe pF kKer(f ) ***Figure*** 1.3 Factorisation d' une application linéaire À quoi cela sert -il ? À faire apparaître un isomorphisme , ce qui permet d' utiliser sa réciproque ! Ainsi , sur le schéma précédent , il apparaît une application linéaire naturelle permettant d' aller de E 0 dans E ( sans pour autant que f soit inversible ) , en introduisant un supplémentaire F 0 de Im(f ) dans E 0

**14971**: On a donc obtenu une factorisation de f à l' aide d' applications linéaires simples et d' un isomorphisme : f iIm(f ) E0 fe pF kKer(f ) Figure 1.3 Factorisation d' une application linéaire À quoi cela ***sert*** -il ? À faire apparaître un isomorphisme , ce qui permet d' utiliser sa réciproque ! Ainsi , sur le schéma précédent , il apparaît une application linéaire naturelle permettant d' aller de E 0 dans E ( sans pour autant que f soit inversible ) , en introduisant un supplémentaire F 0 de Im(f ) dans E 0

**14982**: On a donc obtenu une factorisation de f à l' aide d' applications linéaires simples et d' un isomorphisme : f iIm(f ) E0 fe pF kKer(f ) Figure 1.3 Factorisation d' une application linéaire À quoi cela sert -il ? À faire apparaître un isomorphisme , ce qui ***permet*** d' utiliser sa réciproque ! Ainsi , sur le schéma précédent , il apparaît une application linéaire naturelle permettant d' aller de E 0 dans E ( sans pour autant que f soit inversible ) , en introduisant un supplémentaire F 0 de Im(f ) dans E 0

**14996**: On a donc obtenu une factorisation de f à l' aide d' applications linéaires simples et d' un isomorphisme : f iIm(f ) E0 fe pF kKer(f ) Figure 1.3 Factorisation d' une application linéaire À quoi cela sert -il ? À faire apparaître un isomorphisme , ce qui permet d' utiliser sa réciproque ! Ainsi , sur le schéma précédent , il ***apparaît*** une application linéaire naturelle permettant d' aller de E 0 dans E ( sans pour autant que f soit inversible ) , en introduisant un supplémentaire F 0 de Im(f ) dans E 0

**15040**: ***Notons*** cette application : regardons ce que deviennent g f et f g. Soit un élément x E , que l' on décompose suivant la somme directe F Ker(f ) , alors iF E fe1 pIm(f ) kF 0 fe(xF ) iF E fe1 fe(xF ) De même , si x0 E 0 que l' on décompose sous la forme x0 xF 0 f ( x ) xF 0 fe(xF ) , alors : Finalement , on obtient : Figure 1.4 Utilisation de la factorisation g dépend du choix de F 0 ( et bien sûr de F )

**15044**: Notons cette application : ***regardons*** ce que deviennent g f et f g. Soit un élément x E , que l' on décompose suivant la somme directe F Ker(f ) , alors iF E fe1 pIm(f ) kF 0 fe(xF ) iF E fe1 fe(xF ) De même , si x0 E 0 que l' on décompose sous la forme x0 xF 0 f ( x ) xF 0 fe(xF ) , alors : Finalement , on obtient : Figure 1.4 Utilisation de la factorisation g dépend du choix de F 0 ( et bien sûr de F )

**15047**: Notons cette application : regardons ce que ***deviennent*** g f et f g. Soit un élément x E , que l' on décompose suivant la somme directe F Ker(f ) , alors iF E fe1 pIm(f ) kF 0 fe(xF ) iF E fe1 fe(xF ) De même , si x0 E 0 que l' on décompose sous la forme x0 xF 0 f ( x ) xF 0 fe(xF ) , alors : Finalement , on obtient : Figure 1.4 Utilisation de la factorisation g dépend du choix de F 0 ( et bien sûr de F )

**15062**: Notons cette application : regardons ce que deviennent g f et f g. Soit un élément x E , que l' on ***décompose*** suivant la somme directe F Ker(f ) , alors iF E fe1 pIm(f ) kF 0 fe(xF ) iF E fe1 fe(xF ) De même , si x0 E 0 que l' on décompose sous la forme x0 xF 0 f ( x ) xF 0 fe(xF ) , alors : Finalement , on obtient : Figure 1.4 Utilisation de la factorisation g dépend du choix de F 0 ( et bien sûr de F )

**15096**: Notons cette application : regardons ce que deviennent g f et f g. Soit un élément x E , que l' on décompose suivant la somme directe F Ker(f ) , alors iF E fe1 pIm(f ) kF 0 fe(xF ) iF E fe1 fe(xF ) De même , si x0 E 0 que l' on ***décompose*** sous la forme x0 xF 0 f ( x ) xF 0 fe(xF ) , alors : Finalement , on obtient : Figure 1.4 Utilisation de la factorisation g dépend du choix de F 0 ( et bien sûr de F )

**15117**: Notons cette application : regardons ce que deviennent g f et f g. Soit un élément x E , que l' on décompose suivant la somme directe F Ker(f ) , alors iF E fe1 pIm(f ) kF 0 fe(xF ) iF E fe1 fe(xF ) De même , si x0 E 0 que l' on décompose sous la forme x0 xF 0 f ( x ) xF 0 fe(xF ) , alors : Finalement , on ***obtient*** : Figure 1.4 Utilisation de la factorisation g dépend du choix de F 0 ( et bien sûr de F )

**15119**: Notons cette application : regardons ce que deviennent g f et f g. Soit un élément x E , que l' on décompose suivant la somme directe F Ker(f ) , alors iF E fe1 pIm(f ) kF 0 fe(xF ) iF E fe1 fe(xF ) De même , si x0 E 0 que l' on décompose sous la forme x0 xF 0 f ( x ) xF 0 fe(xF ) , alors : Finalement , on obtient : ***Figure*** 1.4 Utilisation de la factorisation g dépend du choix de F 0 ( et bien sûr de F )

**15126**: Notons cette application : regardons ce que deviennent g f et f g. Soit un élément x E , que l' on décompose suivant la somme directe F Ker(f ) , alors iF E fe1 pIm(f ) kF 0 fe(xF ) iF E fe1 fe(xF ) De même , si x0 E 0 que l' on décompose sous la forme x0 xF 0 f ( x ) xF 0 fe(xF ) , alors : Finalement , on obtient : Figure 1.4 Utilisation de la factorisation g ***dépend*** du choix de F 0 ( et bien sûr de F )

**15147**: Notons de plus , que si f ***est*** un isomorphisme , alors fe f et g f 1

**15163**: Notons que l' on ***a*** toujours les résultats suivants : fF est injective , et f est surjective On obtient donc , à l' aide de ce théorème , deux méthodes de démonstrations : 1

**15170**: Notons que l' on a toujours les résultats suivants : fF ***est*** injective , et f est surjective On obtient donc , à l' aide de ce théorème , deux méthodes de démonstrations : 1

**15175**: Notons que l' on a toujours les résultats suivants : fF est injective , et f ***est*** surjective On obtient donc , à l' aide de ce théorème , deux méthodes de démonstrations : 1

**15178**: Notons que l' on a toujours les résultats suivants : fF est injective , et f est surjective On ***obtient*** donc , à l' aide de ce théorème , deux méthodes de démonstrations : 1

**15241**: Quand le problème ***est*** simple lorsqu' une application linéaire est bijective , on peut toujours essayer de se ramener à cette situation avec le théorème de factorisation

**15247**: Quand le problème est simple lorsqu' une application linéaire ***est*** bijective , on peut toujours essayer de se ramener à cette situation avec le théorème de factorisation

**15251**: Quand le problème est simple lorsqu' une application linéaire est bijective , on ***peut*** toujours essayer de se ramener à cette situation avec le théorème de factorisation

**15282**: Si x00 Im(w ) , alors il ***existe*** un x E , tel que x00 w(x ) v u(x ) Im(v )

**15314**: ( Chemins naturels ) On ***a*** le diagramme 1.5 , page suivante

**15333**: ( Analyse ) En utilisant le théorème de factorisation , on ***peut*** construire un chemin naturel allant de E à E 0 ( en passant par E 00 )

**15361**: On ***obtient*** un candidat au rôle de u. Il s' écrit : ( Synthèse ) Il suffit alors de vérifier qu' il convient a , si x E , alors : or w(x ) Im(v ) par hypothèse 2

**15370**: On obtient un candidat au rôle de u. Il s' ***écrit*** : ( Synthèse ) Il suffit alors de vérifier qu' il convient a , si x E , alors : or w(x ) Im(v ) par hypothèse 2

**15376**: On obtient un candidat au rôle de u. Il s' écrit : ( Synthèse ) Il ***suffit*** alors de vérifier qu' il convient a , si x E , alors : or w(x ) Im(v ) par hypothèse 2

**15382**: On obtient un candidat au rôle de u. Il s' écrit : ( Synthèse ) Il suffit alors de vérifier qu' il ***convient*** a , si x E , alors : or w(x ) Im(v ) par hypothèse 2

**15383**: On obtient un candidat au rôle de u. Il s' écrit : ( Synthèse ) Il suffit alors de vérifier qu' il convient ***a*** , si x E , alors : or w(x ) Im(v ) par hypothèse 2

**15410**: ) Si v ***est*** inversible , alors u v 1 w. Dans le cas contraire , nous allons essayer de nous y ramener : ( Analyse ) Si u existe , alors b w v u donc w Pour pouvoir restreindre v à F 0 ( supplémentaire de Ker(v ) ) , nous allons imposer une condition supplémentaire à u permettant la co-restriction à F 0 de u : e inversible ! donc , dans ce cas , on a la condition nécessaire : ( Synthèse ) Soit u L ( E , E 0 ) défini par : ou , si l' on préfère : Alors , si x E , on a Le candidat ne est parfois pas celui dont on a besoin ! La vérification est indispensable ! b. La co-restriction est possible d' après l' hypothèse

**15436**: ) Si v est inversible , alors u v 1 w. Dans le cas contraire , nous allons essayer de nous y ramener : ( Analyse ) Si u ***existe*** , alors b w v u donc w Pour pouvoir restreindre v à F 0 ( supplémentaire de Ker(v ) ) , nous allons imposer une condition supplémentaire à u permettant la co-restriction à F 0 de u : e inversible ! donc , dans ce cas , on a la condition nécessaire : ( Synthèse ) Soit u L ( E , E 0 ) défini par : ou , si l' on préfère : Alors , si x E , on a Le candidat ne est parfois pas celui dont on a besoin ! La vérification est indispensable ! b. La co-restriction est possible d' après l' hypothèse

**15486**: ) Si v est inversible , alors u v 1 w. Dans le cas contraire , nous allons essayer de nous y ramener : ( Analyse ) Si u existe , alors b w v u donc w Pour pouvoir restreindre v à F 0 ( supplémentaire de Ker(v ) ) , nous allons imposer une condition supplémentaire à u permettant la co-restriction à F 0 de u : e inversible ! donc , dans ce cas , on ***a*** la condition nécessaire : ( Synthèse ) Soit u L ( E , E 0 ) défini par : ou , si l' on préfère : Alors , si x E , on a Le candidat ne est parfois pas celui dont on a besoin ! La vérification est indispensable ! b. La co-restriction est possible d' après l' hypothèse

**15511**: ) Si v est inversible , alors u v 1 w. Dans le cas contraire , nous allons essayer de nous y ramener : ( Analyse ) Si u existe , alors b w v u donc w Pour pouvoir restreindre v à F 0 ( supplémentaire de Ker(v ) ) , nous allons imposer une condition supplémentaire à u permettant la co-restriction à F 0 de u : e inversible ! donc , dans ce cas , on a la condition nécessaire : ( Synthèse ) Soit u L ( E , E 0 ) défini par : ou , si l' on ***préfère*** : Alors , si x E , on a Le candidat ne est parfois pas celui dont on a besoin ! La vérification est indispensable ! b. La co-restriction est possible d' après l' hypothèse

**15520**: ) Si v est inversible , alors u v 1 w. Dans le cas contraire , nous allons essayer de nous y ramener : ( Analyse ) Si u existe , alors b w v u donc w Pour pouvoir restreindre v à F 0 ( supplémentaire de Ker(v ) ) , nous allons imposer une condition supplémentaire à u permettant la co-restriction à F 0 de u : e inversible ! donc , dans ce cas , on a la condition nécessaire : ( Synthèse ) Soit u L ( E , E 0 ) défini par : ou , si l' on préfère : Alors , si x E , on ***a*** Le candidat ne est parfois pas celui dont on a besoin ! La vérification est indispensable ! b. La co-restriction est possible d' après l' hypothèse

**15524**: ) Si v est inversible , alors u v 1 w. Dans le cas contraire , nous allons essayer de nous y ramener : ( Analyse ) Si u existe , alors b w v u donc w Pour pouvoir restreindre v à F 0 ( supplémentaire de Ker(v ) ) , nous allons imposer une condition supplémentaire à u permettant la co-restriction à F 0 de u : e inversible ! donc , dans ce cas , on a la condition nécessaire : ( Synthèse ) Soit u L ( E , E 0 ) défini par : ou , si l' on préfère : Alors , si x E , on a Le candidat ne ***est*** parfois pas celui dont on a besoin ! La vérification est indispensable ! b. La co-restriction est possible d' après l' hypothèse

**15530**: ) Si v est inversible , alors u v 1 w. Dans le cas contraire , nous allons essayer de nous y ramener : ( Analyse ) Si u existe , alors b w v u donc w Pour pouvoir restreindre v à F 0 ( supplémentaire de Ker(v ) ) , nous allons imposer une condition supplémentaire à u permettant la co-restriction à F 0 de u : e inversible ! donc , dans ce cas , on a la condition nécessaire : ( Synthèse ) Soit u L ( E , E 0 ) défini par : ou , si l' on préfère : Alors , si x E , on a Le candidat ne est parfois pas celui dont on ***a*** besoin ! La vérification est indispensable ! b. La co-restriction est possible d' après l' hypothèse

**15535**: ) Si v est inversible , alors u v 1 w. Dans le cas contraire , nous allons essayer de nous y ramener : ( Analyse ) Si u existe , alors b w v u donc w Pour pouvoir restreindre v à F 0 ( supplémentaire de Ker(v ) ) , nous allons imposer une condition supplémentaire à u permettant la co-restriction à F 0 de u : e inversible ! donc , dans ce cas , on a la condition nécessaire : ( Synthèse ) Soit u L ( E , E 0 ) défini par : ou , si l' on préfère : Alors , si x E , on a Le candidat ne est parfois pas celui dont on a besoin ! La vérification ***est*** indispensable ! b. La co-restriction est possible d' après l' hypothèse

**15541**: ) Si v est inversible , alors u v 1 w. Dans le cas contraire , nous allons essayer de nous y ramener : ( Analyse ) Si u existe , alors b w v u donc w Pour pouvoir restreindre v à F 0 ( supplémentaire de Ker(v ) ) , nous allons imposer une condition supplémentaire à u permettant la co-restriction à F 0 de u : e inversible ! donc , dans ce cas , on a la condition nécessaire : ( Synthèse ) Soit u L ( E , E 0 ) défini par : ou , si l' on préfère : Alors , si x E , on a Le candidat ne est parfois pas celui dont on a besoin ! La vérification est indispensable ! b. La co-restriction ***est*** possible d' après l' hypothèse

**15608**: Cette démonstration ***est*** très proche de celle du relèvement linéaire

**15734**: 1.12.2 Soit E0 , E1 et E2 trois sous-espaces vectoriels d' un K - espace vectoriel E tels que : démontrer que E1 et E2 ***sont*** isomorphes

**15764**: 1.12.3 Soit E1 , E2 , E3 et E4 quatre sous-espaces vectoriels d' un K - espace vectoriel E tels que : démontrer que E3 et E4 ***sont*** isomorphes

**15789**: ( ***a*** ) Démontrer que ( b ) Donner une CNS pour que : ( c ) Donner une CNS pour que : 1.12.5 Soit E un K - espace vectoriel et soit f L ( E ) , démontrer que Étude du dual Soit E un K - espace vectoriel , on appelle dual de E et on note : Les éléments de E ? s' appellent des formes linéaires a

**15841**: ( a ) Démontrer que ( b ) Donner une CNS pour que : ( c ) Donner une CNS pour que : 1.12.5 Soit E un K - espace vectoriel et soit f L ( E ) , démontrer que Étude du dual Soit E un K - espace vectoriel , on ***appelle*** dual de E et on note : Les éléments de E ? s' appellent des formes linéaires a

**15847**: ( a ) Démontrer que ( b ) Donner une CNS pour que : ( c ) Donner une CNS pour que : 1.12.5 Soit E un K - espace vectoriel et soit f L ( E ) , démontrer que Étude du dual Soit E un K - espace vectoriel , on appelle dual de E et on ***note*** : Les éléments de E ? s' appellent des formes linéaires a

**15855**: ( a ) Démontrer que ( b ) Donner une CNS pour que : ( c ) Donner une CNS pour que : 1.12.5 Soit E un K - espace vectoriel et soit f L ( E ) , démontrer que Étude du dual Soit E un K - espace vectoriel , on appelle dual de E et on note : Les éléments de E ? s' ***appellent*** des formes linéaires a

**15859**: ( a ) Démontrer que ( b ) Donner une CNS pour que : ( c ) Donner une CNS pour que : 1.12.5 Soit E un K - espace vectoriel et soit f L ( E ) , démontrer que Étude du dual Soit E un K - espace vectoriel , on appelle dual de E et on note : Les éléments de E ? s' appellent des formes linéaires ***a***

**15861**: ***a.*** Le mot forme , désigne en général une application à valeurs dans le corps de base

**15866**: a. Le mot forme , ***désigne*** en général une application à valeurs dans le corps de base

**15901**: Alors E ? ***est*** un K - espace vectoriel

**15910**: Démonstration C' ***est*** un cas particulier de la propriété 1.23 , page 47

**15924**: Si E ***est*** de dimension finie , alors E ? est de dimension finie et dim E dim E ? En dimension finie , les deux espaces sont donc isomorphes

**15932**: Si E est de dimension finie , alors E ? ***est*** de dimension finie et dim E dim E ? En dimension finie , les deux espaces sont donc isomorphes

**15949**: Si E est de dimension finie , alors E ? est de dimension finie et dim E dim E ? En dimension finie , les deux espaces ***sont*** donc isomorphes

**15954**: C' ***est*** faux en dimension infinie

**15963**: Démonstration Si E ***est*** de dimension finie , c' est un cas particulier de la proposition 1.8 , page 58 : Si E ne est pas de dimension finie , voir l' exemple ci-dessous

**15969**: Démonstration Si E est de dimension finie , c' ***est*** un cas particulier de la proposition 1.8 , page 58 : Si E ne est pas de dimension finie , voir l' exemple ci-dessous

**15984**: Démonstration Si E est de dimension finie , c' est un cas particulier de la proposition 1.8 , page 58 : Si E ne ***est*** pas de dimension finie , voir l' exemple ci-dessous

**16018**: E Vect(x 7 xn , n N ) , sous-espace vectoriel du Q - espace vectoriel F ( Q , Q ) On ***a*** alors : E est dénombrable car : en bijection avec Qn1 donc Q , et une réunion dénombrable d' ensembles dénombrables est dénombrable

**16022**: E Vect(x 7 xn , n N ) , sous-espace vectoriel du Q - espace vectoriel F ( Q , Q ) On a alors : E ***est*** dénombrable car : en bijection avec Qn1 donc Q , et une réunion dénombrable d' ensembles dénombrables est dénombrable

**16040**: E Vect(x 7 xn , n N ) , sous-espace vectoriel du Q - espace vectoriel F ( Q , Q ) On a alors : E est dénombrable car : en bijection avec Qn1 donc Q , et une réunion dénombrable d' ensembles dénombrables ***est*** dénombrable

**16046**: E ? ne ***est*** pas dénombrable car : E ? isomorphe à ( donc en bijection avec ) QN par l' isomorphisme usuel : Or , QN ne est pas dénombrable ( voir le procédé diagonal de Cantor ) a

**16071**: E ? ne est pas dénombrable car : E ? isomorphe à ( donc en bijection avec ) QN par l' isomorphisme usuel : Or , QN ne ***est*** pas dénombrable ( voir le procédé diagonal de Cantor ) a

**16082**: E ? ne est pas dénombrable car : E ? isomorphe à ( donc en bijection avec ) QN par l' isomorphisme usuel : Or , QN ne est pas dénombrable ( voir le procédé diagonal de Cantor ) ***a***

**16088**: Remarquons que le dual ***est*** toujours plus gros que l' espace de départ ! C' est donc un isomorphisme non-géométrique

**16099**: Remarquons que le dual est toujours plus gros que l' espace de départ ! C' ***est*** donc un isomorphisme non-géométrique

**16105**: ***a.*** Ceci est aussi un exemple où L ( E , E 0 ) ne est pas isomorphe à L ( E 0 , E ) Si ( ei ) iI est une base de E , on peut définir la famille duale associée ( e?i ) iI ( E ? ) I par : Cette notation est très dangereuse ! En effet , si l' on change un des vecteurs ei , alors on change tous les vecteurs e?i

**16107**: a. Ceci ***est*** aussi un exemple où L ( E , E 0 ) ne est pas isomorphe à L ( E 0 , E ) Si ( ei ) iI est une base de E , on peut définir la famille duale associée ( e?i ) iI ( E ? ) I par : Cette notation est très dangereuse ! En effet , si l' on change un des vecteurs ei , alors on change tous les vecteurs e?i

**16120**: a. Ceci est aussi un exemple où L ( E , E 0 ) ne ***est*** pas isomorphe à L ( E 0 , E ) Si ( ei ) iI est une base de E , on peut définir la famille duale associée ( e?i ) iI ( E ? ) I par : Cette notation est très dangereuse ! En effet , si l' on change un des vecteurs ei , alors on change tous les vecteurs e?i

**16136**: a. Ceci est aussi un exemple où L ( E , E 0 ) ne est pas isomorphe à L ( E 0 , E ) Si ( ei ) iI ***est*** une base de E , on peut définir la famille duale associée ( e?i ) iI ( E ? ) I par : Cette notation est très dangereuse ! En effet , si l' on change un des vecteurs ei , alors on change tous les vecteurs e?i

**16143**: a. Ceci est aussi un exemple où L ( E , E 0 ) ne est pas isomorphe à L ( E 0 , E ) Si ( ei ) iI est une base de E , on ***peut*** définir la famille duale associée ( e?i ) iI ( E ? ) I par : Cette notation est très dangereuse ! En effet , si l' on change un des vecteurs ei , alors on change tous les vecteurs e?i

**16162**: a. Ceci est aussi un exemple où L ( E , E 0 ) ne est pas isomorphe à L ( E 0 , E ) Si ( ei ) iI est une base de E , on peut définir la famille duale associée ( e?i ) iI ( E ? ) I par : Cette notation ***est*** très dangereuse ! En effet , si l' on change un des vecteurs ei , alors on change tous les vecteurs e?i

**16172**: a. Ceci est aussi un exemple où L ( E , E 0 ) ne est pas isomorphe à L ( E 0 , E ) Si ( ei ) iI est une base de E , on peut définir la famille duale associée ( e?i ) iI ( E ? ) I par : Cette notation est très dangereuse ! En effet , si l' on ***change*** un des vecteurs ei , alors on change tous les vecteurs e?i

**16180**: a. Ceci est aussi un exemple où L ( E , E 0 ) ne est pas isomorphe à L ( E 0 , E ) Si ( ei ) iI est une base de E , on peut définir la famille duale associée ( e?i ) iI ( E ? ) I par : Cette notation est très dangereuse ! En effet , si l' on change un des vecteurs ei , alors on ***change*** tous les vecteurs e?i

**16207**: Soit E un K - espace vectoriel et soit ( ei ) iI une base de E. La famille duale associée ***est*** une partie libre Démonstration Soit ( i1 ,

**16257**: , p ) Kp Soit j 1 , p. Par définition de la famille duale , on ***a*** donc 1 p 0 , ce qui montre que la famille duale ( e?i ) iI est libre

**16265**: , p ) Kp Soit j 1 , p. Par définition de la famille duale , on a donc 1 p 0 , ce qui ***montre*** que la famille duale ( e?i ) iI est libre

**16274**: , p ) Kp Soit j 1 , p. Par définition de la famille duale , on a donc 1 p 0 , ce qui montre que la famille duale ( e?i ) iI ***est*** libre

**16287**: Si E ***est*** de dimension finie et si ( e1 ,

**16302**: , en ) ***est*** une base de E , alors la famille duale associée est une base de E ? ( dite base duale )

**16313**: , en ) est une base de E , alors la famille duale associée ***est*** une base de E ? ( dite base duale )

**16333**: Démonstration D' après la propriété précédente , c' ***est*** une famille libre à n éléments

**16359**: Or dim E ? dim E n ( propriété 1.39 , page ci - contre ) donc c' ***est*** bien une base de E ?

**16386**: Soit E un K - espace vectoriel et soit ( ei ) iI une base de E. Si E ***est*** de dimension infinie , alors la famille duale associée ne est jamais génératrice

**16397**: Soit E un K - espace vectoriel et soit ( ei ) iI une base de E. Si E est de dimension infinie , alors la famille duale associée ne ***est*** jamais génératrice

**16479**: , ip ( possible car E ***est*** de dimension infinie donc I est infini ) , on a Proposition 1.11 Base ante - duale Soit E un K - espace vectoriel de dimension finie n , ( 1 ,

**16485**: , ip ( possible car E est de dimension infinie donc I ***est*** infini ) , on a Proposition 1.11 Base ante - duale Soit E un K - espace vectoriel de dimension finie n , ( 1 ,

**16490**: , ip ( possible car E est de dimension infinie donc I est infini ) , on ***a*** Proposition 1.11 Base ante - duale Soit E un K - espace vectoriel de dimension finie n , ( 1 ,

**16526**: , n ) une base de E ? , alors il ***existe*** une unique Cette base est appelée base ante - duale de la base ( 1 ,

**16531**: , n ) une base de E ? , alors il existe une unique Cette base ***est*** appelée base ante - duale de la base ( 1 ,

**16559**: Démonstration Soit l' application définie par : Cette application ***a*** les propriétés suivantes : est linéaire

**16564**: Démonstration Soit l' application définie par : Cette application a les propriétés suivantes : ***est*** linéaire

**16567**: ***est*** injective

**16576**: Donc , comme E et Kn ***ont*** même dimension n , est un isomorphisme

**16581**: Donc , comme E et Kn ont même dimension n , ***est*** un isomorphisme

**16604**: Comme un isomorphisme ***envoie*** une base sur une base , si l' on pose : la famille obtenue convient , et c' est clairement la seule

**16614**: Comme un isomorphisme envoie une base sur une base , si l' on ***pose*** : la famille obtenue convient , et c' est clairement la seule

**16619**: Comme un isomorphisme envoie une base sur une base , si l' on pose : la famille obtenue ***convient*** , et c' est clairement la seule

**16623**: Comme un isomorphisme envoie une base sur une base , si l' on pose : la famille obtenue convient , et c' ***est*** clairement la seule

**16636**: Comment démontrer qu' une famille de formes linéaires ***est*** une base ? En utilisant la base ante - duale ( si l' on est capable de la trouver )

**16651**: Comment démontrer qu' une famille de formes linéaires est une base ? En utilisant la base ante - duale ( si l' on ***est*** capable de la trouver )

**16659**: On ***veut*** étudier ( 1 ,

**16681**: Si on ***considère*** la base ante - duale ( e1 ,

**16704**: , en ) , ( ou la famille que l' on ***imagine*** être la base ante - duale ) , c' est alors facile : soit ( 1 ,

**16714**: , en ) , ( ou la famille que l' on imagine être la base ante - duale ) , c' ***est*** alors facile : soit ( 1 ,

**16745**: ( ***a*** ) Calculer dim E. i. À quelle CNS la famille ( 1 ,

**16778**: Lorsque cette condition ne ***est*** pas vérifiée , exprimer 4 en fonction des trois autres

**16811**: Lorsque la fonction f C ( ***a*** , b , R ) , évaluer l' erreur de méthode

**16884**: 1.13.3 Trouver les formes linéaires définies sur E C 0 ( R , R ) telles que : 1.13.4 Soit E un K - espace vectoriel de dimension finie , soit E1 , E2 , ... , Ep des sous-espaces vectoriels de E , donner une CNS pour que : Hyperplans Soit E un K - espace vectoriel , on ***appelle*** hyperplan de E , tout sous-espace vectoriel H tel que : L' écriture : s' appelle équation de l' hyperplan H. Il ne y a pas unicité de l' équation , car , si convient , alors , quelque soit K ,

**16900**: 1.13.3 Trouver les formes linéaires définies sur E C 0 ( R , R ) telles que : 1.13.4 Soit E un K - espace vectoriel de dimension finie , soit E1 , E2 , ... , Ep des sous-espaces vectoriels de E , donner une CNS pour que : Hyperplans Soit E un K - espace vectoriel , on appelle hyperplan de E , tout sous-espace vectoriel H tel que : L' écriture : s' ***appelle*** équation de l' hyperplan H. Il ne y a pas unicité de l' équation , car , si convient , alors , quelque soit K ,

**16909**: 1.13.3 Trouver les formes linéaires définies sur E C 0 ( R , R ) telles que : 1.13.4 Soit E un K - espace vectoriel de dimension finie , soit E1 , E2 , ... , Ep des sous-espaces vectoriels de E , donner une CNS pour que : Hyperplans Soit E un K - espace vectoriel , on appelle hyperplan de E , tout sous-espace vectoriel H tel que : L' écriture : s' appelle équation de l' hyperplan H. Il ne y ***a*** pas unicité de l' équation , car , si convient , alors , quelque soit K ,

**16946**: Soit E un K - espace vectoriel , F un sous-espace vectoriel de E , on ***dit*** que F est de codimension finie , si F possède un supplémentaire de dimension finie

**16949**: Soit E un K - espace vectoriel , F un sous-espace vectoriel de E , on dit que F ***est*** de codimension finie , si F possède un supplémentaire de dimension finie

**16956**: Soit E un K - espace vectoriel , F un sous-espace vectoriel de E , on dit que F est de codimension finie , si F ***possède*** un supplémentaire de dimension finie

**16972**: La dimension commune de tous les supplémentaires de F ***est*** appelée codimension de F et notée ( si E F G ) : codim F dim G Si E est de dimension finie , tous les sous-espaces vectoriels de E sont de codimension finie et si F est un sous-espace vectoriel de E , alors : codim F dim E dim F Cette notion ne est donc pas intéressante en dimension finie , elle nous sera surtout utile en dimension 1

**16992**: La dimension commune de tous les supplémentaires de F est appelée codimension de F et notée ( si E F G ) : codim F dim G Si E ***est*** de dimension finie , tous les sous-espaces vectoriels de E sont de codimension finie et si F est un sous-espace vectoriel de E , alors : codim F dim E dim F Cette notion ne est donc pas intéressante en dimension finie , elle nous sera surtout utile en dimension 1

**17003**: La dimension commune de tous les supplémentaires de F est appelée codimension de F et notée ( si E F G ) : codim F dim G Si E est de dimension finie , tous les sous-espaces vectoriels de E ***sont*** de codimension finie et si F est un sous-espace vectoriel de E , alors : codim F dim E dim F Cette notion ne est donc pas intéressante en dimension finie , elle nous sera surtout utile en dimension 1

**17010**: La dimension commune de tous les supplémentaires de F est appelée codimension de F et notée ( si E F G ) : codim F dim G Si E est de dimension finie , tous les sous-espaces vectoriels de E sont de codimension finie et si F ***est*** un sous-espace vectoriel de E , alors : codim F dim E dim F Cette notion ne est donc pas intéressante en dimension finie , elle nous sera surtout utile en dimension 1

**17028**: La dimension commune de tous les supplémentaires de F est appelée codimension de F et notée ( si E F G ) : codim F dim G Si E est de dimension finie , tous les sous-espaces vectoriels de E sont de codimension finie et si F est un sous-espace vectoriel de E , alors : codim F dim E dim F Cette notion ne ***est*** donc pas intéressante en dimension finie , elle nous sera surtout utile en dimension 1

**17050**: Dans K3 , tout plan ***est*** un hyperplan , dans K2 , ce sont les droites ( ces sous-espaces vectoriels sont usuellement décrits par une équation )

**17058**: Dans K3 , tout plan est un hyperplan , dans K2 , ce ***sont*** les droites ( ces sous-espaces vectoriels sont usuellement décrits par une équation )

**17065**: Dans K3 , tout plan est un hyperplan , dans K2 , ce sont les droites ( ces sous-espaces vectoriels ***sont*** usuellement décrits par une équation )

**17086**: Dans E C 0 ( R , R ) , si ***a*** R , le sous-espace vectoriel défini par : est un hyperplan d' équation : f ( a ) 0

**17095**: Dans E C 0 ( R , R ) , si a R , le sous-espace vectoriel défini par : ***est*** un hyperplan d' équation : f ( a ) 0

**17103**: Dans E C 0 ( R , R ) , si a R , le sous-espace vectoriel défini par : est un hyperplan d' équation : f ( ***a*** ) 0

**17108**: Fa ***est*** de plus de codimension 1 , car : de dimension 1 en effet : et cette écriture est clairement unique

**17126**: Fa est de plus de codimension 1 , car : de dimension 1 en effet : et cette écriture ***est*** clairement unique

**17173**: Proposition 1.12 Caractérisation des hyperplans Soit E un K - espace vectoriel , H un sous-espace vectoriel de E , alors : H hyperplan de E codim H 1 Démonstration ( ) Soit ( x ) 0 une équation de H , comme ***est*** non nulle , on peut trouver un vecteur a E , tel ( ) Si E H K.a , a E 0E , alors , on peut prendre comme forme linéaire associée à H : Si E est de dimension finie , les hyperplans de E sont les sous-espaces vectoriels de dimension dim E1

**17178**: Proposition 1.12 Caractérisation des hyperplans Soit E un K - espace vectoriel , H un sous-espace vectoriel de E , alors : H hyperplan de E codim H 1 Démonstration ( ) Soit ( x ) 0 une équation de H , comme est non nulle , on ***peut*** trouver un vecteur a E , tel ( ) Si E H K.a , a E 0E , alors , on peut prendre comme forme linéaire associée à H : Si E est de dimension finie , les hyperplans de E sont les sous-espaces vectoriels de dimension dim E1

**17182**: Proposition 1.12 Caractérisation des hyperplans Soit E un K - espace vectoriel , H un sous-espace vectoriel de E , alors : H hyperplan de E codim H 1 Démonstration ( ) Soit ( x ) 0 une équation de H , comme est non nulle , on peut trouver un vecteur ***a*** E , tel ( ) Si E H K.a , a E 0E , alors , on peut prendre comme forme linéaire associée à H : Si E est de dimension finie , les hyperplans de E sont les sous-espaces vectoriels de dimension dim E1

**17193**: Proposition 1.12 Caractérisation des hyperplans Soit E un K - espace vectoriel , H un sous-espace vectoriel de E , alors : H hyperplan de E codim H 1 Démonstration ( ) Soit ( x ) 0 une équation de H , comme est non nulle , on peut trouver un vecteur a E , tel ( ) Si E H K.a , ***a*** E 0E , alors , on peut prendre comme forme linéaire associée à H : Si E est de dimension finie , les hyperplans de E sont les sous-espaces vectoriels de dimension dim E1

**17200**: Proposition 1.12 Caractérisation des hyperplans Soit E un K - espace vectoriel , H un sous-espace vectoriel de E , alors : H hyperplan de E codim H 1 Démonstration ( ) Soit ( x ) 0 une équation de H , comme est non nulle , on peut trouver un vecteur a E , tel ( ) Si E H K.a , a E 0E , alors , on ***peut*** prendre comme forme linéaire associée à H : Si E est de dimension finie , les hyperplans de E sont les sous-espaces vectoriels de dimension dim E1

**17211**: Proposition 1.12 Caractérisation des hyperplans Soit E un K - espace vectoriel , H un sous-espace vectoriel de E , alors : H hyperplan de E codim H 1 Démonstration ( ) Soit ( x ) 0 une équation de H , comme est non nulle , on peut trouver un vecteur a E , tel ( ) Si E H K.a , a E 0E , alors , on peut prendre comme forme linéaire associée à H : Si E ***est*** de dimension finie , les hyperplans de E sont les sous-espaces vectoriels de dimension dim E1

**17220**: Proposition 1.12 Caractérisation des hyperplans Soit E un K - espace vectoriel , H un sous-espace vectoriel de E , alors : H hyperplan de E codim H 1 Démonstration ( ) Soit ( x ) 0 une équation de H , comme est non nulle , on peut trouver un vecteur a E , tel ( ) Si E H K.a , a E 0E , alors , on peut prendre comme forme linéaire associée à H : Si E est de dimension finie , les hyperplans de E ***sont*** les sous-espaces vectoriels de dimension dim E1

**17249**: Dans R2 , une équation de la droite ( hyperplan ) engendrée par ( 1 , 2 ) ***est*** , par exemple : 2

**17283**: Dans R3 , une équation du plan ( hyperplan ) engendré par ( 1 , 1 , 0 ) et ( 1 , 0 , 2 ) ***est*** , par exemple : 3

**17378**: ( ) Par récurrence sur n. ( Initialisation ) : si n 1 et Ker ( ) Ker ( ) , si ***est*** nulle , l' est aussi

**17382**: ( ) Par récurrence sur n. ( Initialisation ) : si n 1 et Ker ( ) Ker ( ) , si est nulle , l' ***est*** aussi

**17386**: Si ***est*** non nulle , alors , il existe un vecteur a E , ( a ) 6 0

**17393**: Si est non nulle , alors , il ***existe*** un vecteur a E , ( a ) 6 0

**17396**: Si est non nulle , alors , il existe un vecteur ***a*** E , ( a ) 6 0

**17400**: Si est non nulle , alors , il existe un vecteur a E , ( ***a*** ) 6 0

**17408**: Alors : Il ***suffit*** de le vérifier pour x h .a , où h Ker ( ) et K. ( Hérédité ) : supposons le résultat vrai au rang p 1 , soit ( 1 ,

**17428**: Alors : Il suffit de le vérifier pour x h .a , où h Ker ( ) et K. ( Hérédité ) : ***supposons*** le résultat vrai au rang p 1 , soit ( 1 ,

**17460**: , p1 ) des formes linéaires de E et E ? telle que : Si p1 ***est*** nulle , c' est terminé

**17464**: , p1 ) des formes linéaires de E et E ? telle que : Si p1 est nulle , c' ***est*** terminé

**17467**: ***Supposons*** donc p1 non nulle et posons H Ker(p1 )

**17473**: Supposons donc p1 non nulle et ***posons*** H Ker(p1 )

**17479**: On ***a*** on peut donc appliquer l' hypothèse de récurrence qui nous donne : On a alors : donc , d' après l' initialisation : Dans R3 , les hyperplans sont des plans et on a la situation géométrique de la figure 1.7 , de la présente page

**17481**: On a on ***peut*** donc appliquer l' hypothèse de récurrence qui nous donne : On a alors : donc , d' après l' initialisation : Dans R3 , les hyperplans sont des plans et on a la situation géométrique de la figure 1.7 , de la présente page

**17490**: On a on peut donc appliquer l' hypothèse de récurrence qui nous ***donne*** : On a alors : donc , d' après l' initialisation : Dans R3 , les hyperplans sont des plans et on a la situation géométrique de la figure 1.7 , de la présente page

**17493**: On a on peut donc appliquer l' hypothèse de récurrence qui nous donne : On ***a*** alors : donc , d' après l' initialisation : Dans R3 , les hyperplans sont des plans et on a la situation géométrique de la figure 1.7 , de la présente page

**17508**: On a on peut donc appliquer l' hypothèse de récurrence qui nous donne : On a alors : donc , d' après l' initialisation : Dans R3 , les hyperplans ***sont*** des plans et on a la situation géométrique de la figure 1.7 , de la présente page

**17513**: On a on peut donc appliquer l' hypothèse de récurrence qui nous donne : On a alors : donc , d' après l' initialisation : Dans R3 , les hyperplans sont des plans et on ***a*** la situation géométrique de la figure 1.7 , de la présente page

**17540**: Notons H1 Ker(1 ) , H2 Ker(2 ) , si 1 et 2 ***sont*** indépendantes , les deux plans se coupent suivant la droite D. Soit K un plan contenant D ( comme sur le dessin ) , où K Ker ( ) , le théorème nous assure alors Le plan K a donc pour équation : Cette équation est définie à un coefficient de proportionnalité près , donc Figure 1.7 Hyperplans de R3 Le résultat de cette proposition est particulièrement intéressant en géométrie affine

**17547**: Notons H1 Ker(1 ) , H2 Ker(2 ) , si 1 et 2 sont indépendantes , les deux plans se ***coupent*** suivant la droite D. Soit K un plan contenant D ( comme sur le dessin ) , où K Ker ( ) , le théorème nous assure alors Le plan K a donc pour équation : Cette équation est définie à un coefficient de proportionnalité près , donc Figure 1.7 Hyperplans de R3 Le résultat de cette proposition est particulièrement intéressant en géométrie affine

**17574**: Notons H1 Ker(1 ) , H2 Ker(2 ) , si 1 et 2 sont indépendantes , les deux plans se coupent suivant la droite D. Soit K un plan contenant D ( comme sur le dessin ) , où K Ker ( ) , le théorème nous ***assure*** alors Le plan K a donc pour équation : Cette équation est définie à un coefficient de proportionnalité près , donc Figure 1.7 Hyperplans de R3 Le résultat de cette proposition est particulièrement intéressant en géométrie affine

**17579**: Notons H1 Ker(1 ) , H2 Ker(2 ) , si 1 et 2 sont indépendantes , les deux plans se coupent suivant la droite D. Soit K un plan contenant D ( comme sur le dessin ) , où K Ker ( ) , le théorème nous assure alors Le plan K ***a*** donc pour équation : Cette équation est définie à un coefficient de proportionnalité près , donc Figure 1.7 Hyperplans de R3 Le résultat de cette proposition est particulièrement intéressant en géométrie affine

**17586**: Notons H1 Ker(1 ) , H2 Ker(2 ) , si 1 et 2 sont indépendantes , les deux plans se coupent suivant la droite D. Soit K un plan contenant D ( comme sur le dessin ) , où K Ker ( ) , le théorème nous assure alors Le plan K a donc pour équation : Cette équation ***est*** définie à un coefficient de proportionnalité près , donc Figure 1.7 Hyperplans de R3 Le résultat de cette proposition est particulièrement intéressant en géométrie affine

**17606**: Notons H1 Ker(1 ) , H2 Ker(2 ) , si 1 et 2 sont indépendantes , les deux plans se coupent suivant la droite D. Soit K un plan contenant D ( comme sur le dessin ) , où K Ker ( ) , le théorème nous assure alors Le plan K a donc pour équation : Cette équation est définie à un coefficient de proportionnalité près , donc Figure 1.7 Hyperplans de R3 Le résultat de cette proposition ***est*** particulièrement intéressant en géométrie affine

**17620**: Ainsi si H1 , ... , Hp ***sont*** des hyperplans affines ( espaces affines ayant pour directions des hyperplans vectoriels ) d' intersection non vide ( soit A un point de l' intersection ) , d' équations : alors , pour tout hyperplan H d' équation ( AM ) 0 , contenant cette intersection , Soit V R3 , l' espace usuel muni de sa structure affine euclidienne usuelle

**17693**: Soit D une droite affine et A un point , ***cherchons*** les plans tangents à la sphère de centre A de rayon 1 , contenant D. Par exemple : Soit D la droite définie par : 4 x y z 0 , 2 x 5 y 3 z 4 0

**17734**: ***Cherchons*** le plan P contenant D tel que P soit à une distance 1 du point ( 1 , 1 , 1 )

**17772**: On va chercher le plan demandé sous la forme : Pour trouver , il ***suffit*** d' écrire d((1 , 1 , 1 ) , P ) 1

**17818**: Théorème 1.9 Mise en équation des sous-espaces de codimensions finies Soit E un K - espace vectoriel , soit E1 un sous-espace de E , alors , pour p N , on ***a*** Démonstration ( ) Si codim(E1 ) p , on peut , par définition trouver un supplémentaire F de dimension p , et une base de F : ( e1 ,

**17828**: Théorème 1.9 Mise en équation des sous-espaces de codimensions finies Soit E un K - espace vectoriel , soit E1 un sous-espace de E , alors , pour p N , on a Démonstration ( ) Si codim(E1 ) p , on ***peut*** , par définition trouver un supplémentaire F de dimension p , et une base de F : ( e1 ,

**17856**: ***Construisons*** alors , pour k 1 , p , la forme linéaire Alors , ( 1 ,

**17879**: , p ) ***sont*** indépendantes et ( ) Par récurrence sur p. ( Initialisation ) C' est la proposition caractérisant les hyperplans comme les espaces de codimension 1

**17892**: , p ) sont indépendantes et ( ) Par récurrence sur p. ( Initialisation ) C' ***est*** la proposition caractérisant les hyperplans comme les espaces de codimension 1

**17908**: ( Hérédité ) ***Supposons*** le résultat vrai au rang p et prenons p 1 formes linéaires indépendantes , D' après l' hypothèse de récurrence , nous savons que E2 est de codimension p. Soit F un supplémentaire de E2 dans E de dimension p. On peut donc trouver un vecteur a E2 E1

**17916**: ( Hérédité ) Supposons le résultat vrai au rang p et ***prenons*** p 1 formes linéaires indépendantes , D' après l' hypothèse de récurrence , nous savons que E2 est de codimension p. Soit F un supplémentaire de E2 dans E de dimension p. On peut donc trouver un vecteur a E2 E1

**17931**: ( Hérédité ) Supposons le résultat vrai au rang p et prenons p 1 formes linéaires indépendantes , D' après l' hypothèse de récurrence , nous ***savons*** que E2 est de codimension p. Soit F un supplémentaire de E2 dans E de dimension p. On peut donc trouver un vecteur a E2 E1

**17934**: ( Hérédité ) Supposons le résultat vrai au rang p et prenons p 1 formes linéaires indépendantes , D' après l' hypothèse de récurrence , nous savons que E2 ***est*** de codimension p. Soit F un supplémentaire de E2 dans E de dimension p. On peut donc trouver un vecteur a E2 E1

**17950**: ( Hérédité ) Supposons le résultat vrai au rang p et prenons p 1 formes linéaires indépendantes , D' après l' hypothèse de récurrence , nous savons que E2 est de codimension p. Soit F un supplémentaire de E2 dans E de dimension p. On ***peut*** donc trouver un vecteur a E2 E1

**17955**: ( Hérédité ) Supposons le résultat vrai au rang p et prenons p 1 formes linéaires indépendantes , D' après l' hypothèse de récurrence , nous savons que E2 est de codimension p. Soit F un supplémentaire de E2 dans E de dimension p. On peut donc trouver un vecteur ***a*** E2 E1

**17960**: Il ***reste*** à montrer que : de dimension p1 donc , en utilisant la première question de l' exercice 1.4.2 , page 32 , on obtient , puisque a E2 C' est ainsi que l' on retrouve que dans l' espace , les droites sont définies par 2 équations

**17984**: Il reste à montrer que : de dimension p1 donc , en utilisant la première question de l' exercice 1.4.2 , page 32 , on ***obtient*** , puisque a E2 C' est ainsi que l' on retrouve que dans l' espace , les droites sont définies par 2 équations

**17987**: Il reste à montrer que : de dimension p1 donc , en utilisant la première question de l' exercice 1.4.2 , page 32 , on obtient , puisque ***a*** E2 C' est ainsi que l' on retrouve que dans l' espace , les droites sont définies par 2 équations

**17990**: Il reste à montrer que : de dimension p1 donc , en utilisant la première question de l' exercice 1.4.2 , page 32 , on obtient , puisque a E2 C' ***est*** ainsi que l' on retrouve que dans l' espace , les droites sont définies par 2 équations

**17995**: Il reste à montrer que : de dimension p1 donc , en utilisant la première question de l' exercice 1.4.2 , page 32 , on obtient , puisque a E2 C' est ainsi que l' on ***retrouve*** que dans l' espace , les droites sont définies par 2 équations

**18003**: Il reste à montrer que : de dimension p1 donc , en utilisant la première question de l' exercice 1.4.2 , page 32 , on obtient , puisque a E2 C' est ainsi que l' on retrouve que dans l' espace , les droites ***sont*** définies par 2 équations

**18011**: Que se ***passe*** -t -il lorsque les formes linéaires ( et donc les équations ) ne sont pas indépendantes ? Il est immédiat que : En dimension finie , cela permet de calculer des dimensions

**18025**: Que se passe -t -il lorsque les formes linéaires ( et donc les équations ) ne ***sont*** pas indépendantes ? Il est immédiat que : En dimension finie , cela permet de calculer des dimensions

**18030**: Que se passe -t -il lorsque les formes linéaires ( et donc les équations ) ne sont pas indépendantes ? Il ***est*** immédiat que : En dimension finie , cela permet de calculer des dimensions

**18039**: Que se passe -t -il lorsque les formes linéaires ( et donc les équations ) ne sont pas indépendantes ? Il est immédiat que : En dimension finie , cela ***permet*** de calculer des dimensions

**18046**: On ***dit*** souvent que : la dimension de l' espace est le nombre de degrés de liberté le nombre d' équations indépendantes est le nombre de contraintes la dimension du sous-espace vectoriel est donc égale à nombre de degré de liberté nombre de contraintes

**18055**: On dit souvent que : la dimension de l' espace ***est*** le nombre de degrés de liberté le nombre d' équations indépendantes est le nombre de contraintes la dimension du sous-espace vectoriel est donc égale à nombre de degré de liberté nombre de contraintes

**18067**: On dit souvent que : la dimension de l' espace est le nombre de degrés de liberté le nombre d' équations indépendantes ***est*** le nombre de contraintes la dimension du sous-espace vectoriel est donc égale à nombre de degré de liberté nombre de contraintes

**18077**: On dit souvent que : la dimension de l' espace est le nombre de degrés de liberté le nombre d' équations indépendantes est le nombre de contraintes la dimension du sous-espace vectoriel ***est*** donc égale à nombre de degré de liberté nombre de contraintes

**18093**: Cette relation ne ***est*** valable qu' avec des contraintes linéaires ! Le théorème se généralise facilement à la situation affine

**18104**: Cette relation ne est valable qu' avec des contraintes linéaires ! Le théorème se ***généralise*** facilement à la situation affine

**18122**: Cependant , l' intersection d' hyperplans affines peut-être vide , aussi ***faut*** -il , avant toutes choses , s' assurer qu' elle ne l' est pas ! Puis , en s' appuyant sur un point trouvé de l' intersection , on est ramené au cas vectoriel

**18135**: Cependant , l' intersection d' hyperplans affines peut-être vide , aussi faut -il , avant toutes choses , s' assurer qu' elle ne l' ***est*** pas ! Puis , en s' appuyant sur un point trouvé de l' intersection , on est ramené au cas vectoriel

**18152**: Cependant , l' intersection d' hyperplans affines peut-être vide , aussi faut -il , avant toutes choses , s' assurer qu' elle ne l' est pas ! Puis , en s' appuyant sur un point trouvé de l' intersection , on ***est*** ramené au cas vectoriel

**18183**: , p ) ***sont*** des formes linéaires de E indépendantes , et si Hk est un hyperplan affine de direction Ker k pour tout k 1 ,

**18194**: , p ) sont des formes linéaires de E indépendantes , et si Hk ***est*** un hyperplan affine de direction Ker k pour tout k 1 ,

**18258**: , xp ) des vecteurs de E. Démontrer que : ( b ) Démontrer que C 0 ( R , R ) ***est*** isomorphe à un hyperplan de C 1 ( R , R )

**18294**: On ***dit*** que V sépare les vecteurs , si Démontrer que V sépare les vecteurs si , et seulement si , V E ?

**18297**: On dit que V ***sépare*** les vecteurs , si Démontrer que V sépare les vecteurs si , et seulement si , V E ?

**18305**: On dit que V sépare les vecteurs , si Démontrer que V ***sépare*** les vecteurs si , et seulement si , V E ?

**18328**: Pour ***A*** P(E ) , on appelle orthogonal ( direct ) de A et on note : Démontrer que : ( a ) A est un sous-espace vectoriel de E ?

**18333**: Pour A P(E ) , on ***appelle*** orthogonal ( direct ) de A et on note : Démontrer que : ( a ) A est un sous-espace vectoriel de E ?

**18342**: Pour A P(E ) , on appelle orthogonal ( direct ) de A et on ***note*** : Démontrer que : ( a ) A est un sous-espace vectoriel de E ?

**18350**: Pour A P(E ) , on appelle orthogonal ( direct ) de A et on note : Démontrer que : ( a ) ***A*** est un sous-espace vectoriel de E ?

**18351**: Pour A P(E ) , on appelle orthogonal ( direct ) de A et on note : Démontrer que : ( a ) A ***est*** un sous-espace vectoriel de E ?

**18365**: Dans la suite A et B ***sont*** des sous-espaces vectoriels de E. ( e ) Si E A B , alors A est isomorphe à B ?

**18381**: Dans la suite A et B sont des sous-espaces vectoriels de E. ( e ) Si E A B , alors A ***est*** isomorphe à B ?

**18392**: ( f ) Si E ***est*** de dimension finie , alors dim A codim A. 1.14.5 Soit E un K - espace vectoriel , et B P(E ? ) , on appelle orthogonal ( indirect ) de B et on note : Démontrer que : ( a ) B est un sous-espace vectoriel de E. Dans la suite , A et B sont des sous-espaces vectoriels de E ?

**18418**: ( f ) Si E est de dimension finie , alors dim A codim A. 1.14.5 Soit E un K - espace vectoriel , et B P(E ? ) , on ***appelle*** orthogonal ( indirect ) de B et on note : Démontrer que : ( a ) B est un sous-espace vectoriel de E. Dans la suite , A et B sont des sous-espaces vectoriels de E ?

**18427**: ( f ) Si E est de dimension finie , alors dim A codim A. 1.14.5 Soit E un K - espace vectoriel , et B P(E ? ) , on appelle orthogonal ( indirect ) de B et on ***note*** : Démontrer que : ( a ) B est un sous-espace vectoriel de E. Dans la suite , A et B sont des sous-espaces vectoriels de E ?

**18433**: ( f ) Si E est de dimension finie , alors dim A codim A. 1.14.5 Soit E un K - espace vectoriel , et B P(E ? ) , on appelle orthogonal ( indirect ) de B et on note : Démontrer que : ( ***a*** ) B est un sous-espace vectoriel de E. Dans la suite , A et B sont des sous-espaces vectoriels de E ?

**18436**: ( f ) Si E est de dimension finie , alors dim A codim A. 1.14.5 Soit E un K - espace vectoriel , et B P(E ? ) , on appelle orthogonal ( indirect ) de B et on note : Démontrer que : ( a ) B ***est*** un sous-espace vectoriel de E. Dans la suite , A et B sont des sous-espaces vectoriels de E ?

**18449**: ( f ) Si E est de dimension finie , alors dim A codim A. 1.14.5 Soit E un K - espace vectoriel , et B P(E ? ) , on appelle orthogonal ( indirect ) de B et on note : Démontrer que : ( a ) B est un sous-espace vectoriel de E. Dans la suite , A et B ***sont*** des sous-espaces vectoriels de E ?

**18462**: ( e ) Si E ***est*** de dimension finie , alors dim B codim B ( f ) En déduire que l' inclusion de la question ( d ) est une inégalité en dimension finie

**18486**: ( e ) Si E est de dimension finie , alors dim B codim B ( f ) En déduire que l' inclusion de la question ( d ) ***est*** une inégalité en dimension finie

**18510**: ( g ) Donner un contre-exemple à l' égalité dans la question ( d ) lorsque E ***est*** de dimension infinie

**18519**: ( h ) Si ***A*** est un sous-espace vectoriel de E , comparer : ( i ) Si B est un sous-espace vectoriel de E ? , comparer : 1.14.6 Soit E et E 0 deux K - espaces vectoriels et soit u L ( E , E 0 )

**18520**: ( h ) Si A ***est*** un sous-espace vectoriel de E , comparer : ( i ) Si B est un sous-espace vectoriel de E ? , comparer : 1.14.6 Soit E et E 0 deux K - espaces vectoriels et soit u L ( E , E 0 )

**18534**: ( h ) Si A est un sous-espace vectoriel de E , comparer : ( i ) Si B ***est*** un sous-espace vectoriel de E ? , comparer : 1.14.6 Soit E et E 0 deux K - espaces vectoriels et soit u L ( E , E 0 )

**18567**: On ***définit*** l' application transposée de u et on note : ( a ) Démontrer que t ( u v ) t u t v. ( b ) Démontrer que : ( c ) Démontrer que : Applications Systèmes linéaires Ce paragraphe , très simple , est fondamental ! Nous nous en servirons très souvent ! Soit E et E 0 deux K - espaces vectoriels , u L ( E , E 0 ) et e0 E 0 , on appelle système linéaire l' équation d' inconnue x E : L' ensemble : est appelé ensemble des solutions de ( S )

**18575**: On définit l' application transposée de u et on ***note*** : ( a ) Démontrer que t ( u v ) t u t v. ( b ) Démontrer que : ( c ) Démontrer que : Applications Systèmes linéaires Ce paragraphe , très simple , est fondamental ! Nous nous en servirons très souvent ! Soit E et E 0 deux K - espaces vectoriels , u L ( E , E 0 ) et e0 E 0 , on appelle système linéaire l' équation d' inconnue x E : L' ensemble : est appelé ensemble des solutions de ( S )

**18578**: On définit l' application transposée de u et on note : ( ***a*** ) Démontrer que t ( u v ) t u t v. ( b ) Démontrer que : ( c ) Démontrer que : Applications Systèmes linéaires Ce paragraphe , très simple , est fondamental ! Nous nous en servirons très souvent ! Soit E et E 0 deux K - espaces vectoriels , u L ( E , E 0 ) et e0 E 0 , on appelle système linéaire l' équation d' inconnue x E : L' ensemble : est appelé ensemble des solutions de ( S )

**18612**: On définit l' application transposée de u et on note : ( a ) Démontrer que t ( u v ) t u t v. ( b ) Démontrer que : ( c ) Démontrer que : Applications Systèmes linéaires Ce paragraphe , très simple , ***est*** fondamental ! Nous nous en servirons très souvent ! Soit E et E 0 deux K - espaces vectoriels , u L ( E , E 0 ) et e0 E 0 , on appelle système linéaire l' équation d' inconnue x E : L' ensemble : est appelé ensemble des solutions de ( S )

**18647**: On définit l' application transposée de u et on note : ( a ) Démontrer que t ( u v ) t u t v. ( b ) Démontrer que : ( c ) Démontrer que : Applications Systèmes linéaires Ce paragraphe , très simple , est fondamental ! Nous nous en servirons très souvent ! Soit E et E 0 deux K - espaces vectoriels , u L ( E , E 0 ) et e0 E 0 , on ***appelle*** système linéaire l' équation d' inconnue x E : L' ensemble : est appelé ensemble des solutions de ( S )

**18660**: On définit l' application transposée de u et on note : ( a ) Démontrer que t ( u v ) t u t v. ( b ) Démontrer que : ( c ) Démontrer que : Applications Systèmes linéaires Ce paragraphe , très simple , est fondamental ! Nous nous en servirons très souvent ! Soit E et E 0 deux K - espaces vectoriels , u L ( E , E 0 ) et e0 E 0 , on appelle système linéaire l' équation d' inconnue x E : L' ensemble : ***est*** appelé ensemble des solutions de ( S )

**18713**: Une condition nécessaire et suffisante pour que Sol(S ) 6 , appelée condition de compatibilité de ( S ) ***est*** : Démonstration Si Sol(S ) 6 , il existe x E tel que u(x ) e0 , en particulier e0 Im u. Si e0 Im u , il existe x E tel que e0 u(x ) , donc Sol(S ) 6

**18722**: Une condition nécessaire et suffisante pour que Sol(S ) 6 , appelée condition de compatibilité de ( S ) est : Démonstration Si Sol(S ) 6 , il ***existe*** x E tel que u(x ) e0 , en particulier e0 Im u. Si e0 Im u , il existe x E tel que e0 u(x ) , donc Sol(S ) 6

**18742**: Une condition nécessaire et suffisante pour que Sol(S ) 6 , appelée condition de compatibilité de ( S ) est : Démonstration Si Sol(S ) 6 , il existe x E tel que u(x ) e0 , en particulier e0 Im u. Si e0 Im u , il ***existe*** x E tel que e0 u(x ) , donc Sol(S ) 6

**18786**: Si e0 0E0 , le système ***est*** dit homogène

**18787**: Si e0 0E0 , le système est ***dit*** homogène

**18798**: Dans ce cas , Sol(S ) Ker(u ) ***est*** un sous-espace vectoriel de E En particulier , Sol(S ) 6

**18843**: Si e0 6 0E0 , le système : ***est*** dit système homogène associé de ( S )

**18844**: Si e0 6 0E0 , le système : est ***dit*** système homogène associé de ( S )

**18881**: Si x0 Sol(S ) ***existe*** alors : Sol(S ) est un espace affine de direction Ker(u )

**18886**: Si x0 Sol(S ) existe alors : Sol(S ) ***est*** un espace affine de direction Ker(u )

**18938**: Si e0 s' ***écrit*** comme une somme : Si pour tout k 1 , p , xk est une solution du système linéaire : alors une solution de ( S ) est Démonstration C' est immédiat par linéarité de u : donc x Sol(S )

**18952**: Si e0 s' écrit comme une somme : Si pour tout k 1 , p , xk ***est*** une solution du système linéaire : alors une solution de ( S ) est Démonstration C' est immédiat par linéarité de u : donc x Sol(S )

**18966**: Si e0 s' écrit comme une somme : Si pour tout k 1 , p , xk est une solution du système linéaire : alors une solution de ( S ) ***est*** Démonstration C' est immédiat par linéarité de u : donc x Sol(S )

**18969**: Si e0 s' écrit comme une somme : Si pour tout k 1 , p , xk est une solution du système linéaire : alors une solution de ( S ) est Démonstration C' ***est*** immédiat par linéarité de u : donc x Sol(S )

**19010**: , xp ) Kp : où les ai , j et les bi ***sont*** des scalaires 2

**19032**: Les équations différentielles linéaires du premier ordre : y 0 a(x ) y b(x ) , où ***a*** et b sont continues sur I 3

**19035**: Les équations différentielles linéaires du premier ordre : y 0 a(x ) y b(x ) , où a et b ***sont*** continues sur I 3

**19036**: Les équations différentielles linéaires du premier ordre : y 0 a(x ) y b(x ) , où a et b sont ***continues*** sur I 3

**19054**: Les équations différentielles linéaires du second ordre à coefficients constants : y 00 ***a*** y 0 b y c(x ) , où a et b sont des scalaires et c est continue sur I 4

**19063**: Les équations différentielles linéaires du second ordre à coefficients constants : y 00 a y 0 b y c(x ) , où ***a*** et b sont des scalaires et c est continue sur I 4

**19066**: Les équations différentielles linéaires du second ordre à coefficients constants : y 00 a y 0 b y c(x ) , où a et b ***sont*** des scalaires et c est continue sur I 4

**19071**: Les équations différentielles linéaires du second ordre à coefficients constants : y 00 a y 0 b y c(x ) , où a et b sont des scalaires et c ***est*** continue sur I 4

**19072**: Les équations différentielles linéaires du second ordre à coefficients constants : y 00 a y 0 b y c(x ) , où a et b sont des scalaires et c est ***continue*** sur I 4

**19096**: Les équations récurrentes : un1 an un bn , où ( an ) nN et ( bn ) nN ***sont*** dans KN 1.15.1 Soit l' équation récurrente : ( a ) Démontrer que c' est bien un système linéaire en précisant E , E 0 et u. ( b ) Justifier l' existence de solutions

**19106**: Les équations récurrentes : un1 an un bn , où ( an ) nN et ( bn ) nN sont dans KN 1.15.1 Soit l' équation récurrente : ( ***a*** ) Démontrer que c' est bien un système linéaire en précisant E , E 0 et u. ( b ) Justifier l' existence de solutions

**19111**: Les équations récurrentes : un1 an un bn , où ( an ) nN et ( bn ) nN sont dans KN 1.15.1 Soit l' équation récurrente : ( a ) Démontrer que c' ***est*** bien un système linéaire en précisant E , E 0 et u. ( b ) Justifier l' existence de solutions

**19174**: 1.15.2 Soit l' équation différentielle : ( ***a*** ) Démontrer que c' est bien un système linéaire en précisant E , E 0 et u. ( b ) Justifier l' existence de solutions

**19179**: 1.15.2 Soit l' équation différentielle : ( a ) Démontrer que c' ***est*** bien un système linéaire en précisant E , E 0 et u. ( b ) Justifier l' existence de solutions

**19252**: ( e ) Comparer aux solutions du système récurrent obtenu par discrétisation : Interpolation Proposition 1.14 Interpolation de Lagrange Soit f : I K , où I ***est*** un intervalle de R. Soit x1 xn des réels dans I , on appelle fonction polynomiale d' interpolation de Lagrange l' unique fonction polynomiale P de degré n , telle que Elle est égale à : Démonstration E f polynomiale de degré n Cet espace vectoriel est clairement de dimension n , de base La famille de E ? définie par : étant une base de E ? , on sait qu' il existe une base ante - duale ( Pk ) k1,n qui vérifie : Un calcul simple nous démontre que : On cherche ensuite une solution sous la forme : en évaluant sur les xj , on trouve l' unique solution de l' énoncé

**19266**: ( e ) Comparer aux solutions du système récurrent obtenu par discrétisation : Interpolation Proposition 1.14 Interpolation de Lagrange Soit f : I K , où I est un intervalle de R. Soit x1 xn des réels dans I , on ***appelle*** fonction polynomiale d' interpolation de Lagrange l' unique fonction polynomiale P de degré n , telle que Elle est égale à : Démonstration E f polynomiale de degré n Cet espace vectoriel est clairement de dimension n , de base La famille de E ? définie par : étant une base de E ? , on sait qu' il existe une base ante - duale ( Pk ) k1,n qui vérifie : Un calcul simple nous démontre que : On cherche ensuite une solution sous la forme : en évaluant sur les xj , on trouve l' unique solution de l' énoncé

**19285**: ( e ) Comparer aux solutions du système récurrent obtenu par discrétisation : Interpolation Proposition 1.14 Interpolation de Lagrange Soit f : I K , où I est un intervalle de R. Soit x1 xn des réels dans I , on appelle fonction polynomiale d' interpolation de Lagrange l' unique fonction polynomiale P de degré n , telle que Elle ***est*** égale à : Démonstration E f polynomiale de degré n Cet espace vectoriel est clairement de dimension n , de base La famille de E ? définie par : étant une base de E ? , on sait qu' il existe une base ante - duale ( Pk ) k1,n qui vérifie : Un calcul simple nous démontre que : On cherche ensuite une solution sous la forme : en évaluant sur les xj , on trouve l' unique solution de l' énoncé

**19299**: ( e ) Comparer aux solutions du système récurrent obtenu par discrétisation : Interpolation Proposition 1.14 Interpolation de Lagrange Soit f : I K , où I est un intervalle de R. Soit x1 xn des réels dans I , on appelle fonction polynomiale d' interpolation de Lagrange l' unique fonction polynomiale P de degré n , telle que Elle est égale à : Démonstration E f polynomiale de degré n Cet espace vectoriel ***est*** clairement de dimension n , de base La famille de E ? définie par : étant une base de E ? , on sait qu' il existe une base ante - duale ( Pk ) k1,n qui vérifie : Un calcul simple nous démontre que : On cherche ensuite une solution sous la forme : en évaluant sur les xj , on trouve l' unique solution de l' énoncé

**19323**: ( e ) Comparer aux solutions du système récurrent obtenu par discrétisation : Interpolation Proposition 1.14 Interpolation de Lagrange Soit f : I K , où I est un intervalle de R. Soit x1 xn des réels dans I , on appelle fonction polynomiale d' interpolation de Lagrange l' unique fonction polynomiale P de degré n , telle que Elle est égale à : Démonstration E f polynomiale de degré n Cet espace vectoriel est clairement de dimension n , de base La famille de E ? définie par : étant une base de E ? , on ***sait*** qu' il existe une base ante - duale ( Pk ) k1,n qui vérifie : Un calcul simple nous démontre que : On cherche ensuite une solution sous la forme : en évaluant sur les xj , on trouve l' unique solution de l' énoncé

**19326**: ( e ) Comparer aux solutions du système récurrent obtenu par discrétisation : Interpolation Proposition 1.14 Interpolation de Lagrange Soit f : I K , où I est un intervalle de R. Soit x1 xn des réels dans I , on appelle fonction polynomiale d' interpolation de Lagrange l' unique fonction polynomiale P de degré n , telle que Elle est égale à : Démonstration E f polynomiale de degré n Cet espace vectoriel est clairement de dimension n , de base La famille de E ? définie par : étant une base de E ? , on sait qu' il ***existe*** une base ante - duale ( Pk ) k1,n qui vérifie : Un calcul simple nous démontre que : On cherche ensuite une solution sous la forme : en évaluant sur les xj , on trouve l' unique solution de l' énoncé

**19337**: ( e ) Comparer aux solutions du système récurrent obtenu par discrétisation : Interpolation Proposition 1.14 Interpolation de Lagrange Soit f : I K , où I est un intervalle de R. Soit x1 xn des réels dans I , on appelle fonction polynomiale d' interpolation de Lagrange l' unique fonction polynomiale P de degré n , telle que Elle est égale à : Démonstration E f polynomiale de degré n Cet espace vectoriel est clairement de dimension n , de base La famille de E ? définie par : étant une base de E ? , on sait qu' il existe une base ante - duale ( Pk ) k1,n qui ***vérifie*** : Un calcul simple nous démontre que : On cherche ensuite une solution sous la forme : en évaluant sur les xj , on trouve l' unique solution de l' énoncé

**19343**: ( e ) Comparer aux solutions du système récurrent obtenu par discrétisation : Interpolation Proposition 1.14 Interpolation de Lagrange Soit f : I K , où I est un intervalle de R. Soit x1 xn des réels dans I , on appelle fonction polynomiale d' interpolation de Lagrange l' unique fonction polynomiale P de degré n , telle que Elle est égale à : Démonstration E f polynomiale de degré n Cet espace vectoriel est clairement de dimension n , de base La famille de E ? définie par : étant une base de E ? , on sait qu' il existe une base ante - duale ( Pk ) k1,n qui vérifie : Un calcul simple nous ***démontre*** que : On cherche ensuite une solution sous la forme : en évaluant sur les xj , on trouve l' unique solution de l' énoncé

**19347**: ( e ) Comparer aux solutions du système récurrent obtenu par discrétisation : Interpolation Proposition 1.14 Interpolation de Lagrange Soit f : I K , où I est un intervalle de R. Soit x1 xn des réels dans I , on appelle fonction polynomiale d' interpolation de Lagrange l' unique fonction polynomiale P de degré n , telle que Elle est égale à : Démonstration E f polynomiale de degré n Cet espace vectoriel est clairement de dimension n , de base La famille de E ? définie par : étant une base de E ? , on sait qu' il existe une base ante - duale ( Pk ) k1,n qui vérifie : Un calcul simple nous démontre que : On ***cherche*** ensuite une solution sous la forme : en évaluant sur les xj , on trouve l' unique solution de l' énoncé

**19362**: ( e ) Comparer aux solutions du système récurrent obtenu par discrétisation : Interpolation Proposition 1.14 Interpolation de Lagrange Soit f : I K , où I est un intervalle de R. Soit x1 xn des réels dans I , on appelle fonction polynomiale d' interpolation de Lagrange l' unique fonction polynomiale P de degré n , telle que Elle est égale à : Démonstration E f polynomiale de degré n Cet espace vectoriel est clairement de dimension n , de base La famille de E ? définie par : étant une base de E ? , on sait qu' il existe une base ante - duale ( Pk ) k1,n qui vérifie : Un calcul simple nous démontre que : On cherche ensuite une solution sous la forme : en évaluant sur les xj , on ***trouve*** l' unique solution de l' énoncé

**19386**: Soit la fonction sin sur l' intervalle 0 , 2 , pour un p N , ***prenons*** et regardons les interpolations pour différentes valeurs de p. Voir la session Wxmaxima 1.3 , de la présente 1.16.1 Redémontrer l' existence et l' unicité des fonctions polynomiales d' interpolation de Lagrange en utilisant un raisonnement sur les systèmes linéaires

**19388**: Soit la fonction sin sur l' intervalle 0 , 2 , pour un p N , prenons et ***regardons*** les interpolations pour différentes valeurs de p. Voir la session Wxmaxima 1.3 , de la présente 1.16.1 Redémontrer l' existence et l' unicité des fonctions polynomiales d' interpolation de Lagrange en utilisant un raisonnement sur les systèmes linéaires

**19469**: 1.16.2 Soit f : I K de classe C 1 , soit x1 xn des points de I. Démontrer l' existence et l' unicité d' une fonction polynomiale P de degré 2 n , telle que : 1.16.3 Soit f : ***a*** , b K , de classe C

**19483**: Pour p N , on ***pose*** Lp ( f ) la fonction polynomiale d' interpolation de Lagrange de f pour les points : On suppose que : Démontrer que : 1.16.4 Soit f : x 7 x , définie sur 1 , 1

**19502**: Pour p N , on pose Lp ( f ) la fonction polynomiale d' interpolation de Lagrange de f pour les points : On ***suppose*** que : Démontrer que : 1.16.4 Soit f : x 7 x , définie sur 1 , 1

**19530**: Pour p N , p 2 , on ***pose*** Lp ( f ) la fonction polynomiale d' interpolation de Lagrange de f pour les points : Démontrer que : Fonctions spline L' interpolation par les polynômes de Lagrange ne est pas toujours efficace pour être proche de la fonction de base

**19561**: Pour p N , p 2 , on pose Lp ( f ) la fonction polynomiale d' interpolation de Lagrange de f pour les points : Démontrer que : Fonctions spline L' interpolation par les polynômes de Lagrange ne ***est*** pas toujours efficace pour être proche de la fonction de base

**19586**: C' ***est*** pourquoi , il a fallu faire appel à d' autres classes de fonctions telles que : elles soient faciles à calculer elles approximent bien la fonction initiale elles soient insensibles à ce phénomène de divergence

**19609**: C' est pourquoi , il a fallu faire appel à d' autres classes de fonctions telles que : elles soient faciles à calculer elles ***approximent*** bien la fonction initiale elles soient insensibles à ce phénomène de divergence

**19624**: On ***appelle*** fonctions spline , des fonctions qui ont une classe fixée ( par exemple C 2 ) et qui sont polynomiales par morceaux

**19631**: On appelle fonctions spline , des fonctions qui ***ont*** une classe fixée ( par exemple C 2 ) et qui sont polynomiales par morceaux

**19643**: On appelle fonctions spline , des fonctions qui ont une classe fixée ( par exemple C 2 ) et qui ***sont*** polynomiales par morceaux

**19651**: Voici ce que ***donne*** l' approximation avec 30 points ( pour Lagrange , cela diverge )

**19662**: Voici ce que donne l' approximation avec 30 points ( pour Lagrange , cela ***diverge*** )

**19703**: , ep ) une base de E , alors : Autrement ***dit*** , en utilisant la dualité , pour tout k 1 , p , xk e?k ( x )

**19762**: , e0n ) des bases de E Autrement ***dit*** , en utilisant la dualité , pour tout ( i , j ) 1 , n 1 , p , ai , j ( e0i ) ? ( u(ej ) )

**19801**: Pour tout x E , on ***a*** donc : ai , j xj .e0i On appelle matrice à n lignes et p colonnes ( n , p ) ( N ) 2 , toute famille ( ai , j ) ( i , j)1,n1,p d' éléments d' un ensemble A représentée sous la forme d' un tableau à n lignes et p colonnes entouré par des crochets a : Les éléments ai , j de la matrice s' appellent coefficients de la matrice

**19810**: Pour tout x E , on a donc : ai , j xj .e0i On ***appelle*** matrice à n lignes et p colonnes ( n , p ) ( N ) 2 , toute famille ( ai , j ) ( i , j)1,n1,p d' éléments d' un ensemble A représentée sous la forme d' un tableau à n lignes et p colonnes entouré par des crochets a : Les éléments ai , j de la matrice s' appellent coefficients de la matrice

**19862**: Pour tout x E , on a donc : ai , j xj .e0i On appelle matrice à n lignes et p colonnes ( n , p ) ( N ) 2 , toute famille ( ai , j ) ( i , j)1,n1,p d' éléments d' un ensemble A représentée sous la forme d' un tableau à n lignes et p colonnes entouré par des crochets ***a*** : Les éléments ai , j de la matrice s' appellent coefficients de la matrice

**19873**: Pour tout x E , on a donc : ai , j xj .e0i On appelle matrice à n lignes et p colonnes ( n , p ) ( N ) 2 , toute famille ( ai , j ) ( i , j)1,n1,p d' éléments d' un ensemble A représentée sous la forme d' un tableau à n lignes et p colonnes entouré par des crochets a : Les éléments ai , j de la matrice s' ***appellent*** coefficients de la matrice

**19880**: On ***dit*** aussi que la matrice M est n p. L' ensemble des matrices à n lignes et p colonnes à coefficients dans A se note : Lorsque n p , on dit que la matrice M est une matrice carrée

**19886**: On dit aussi que la matrice M ***est*** n p. L' ensemble des matrices à n lignes et p colonnes à coefficients dans A se note : Lorsque n p , on dit que la matrice M est une matrice carrée

**19902**: On dit aussi que la matrice M est n p. L' ensemble des matrices à n lignes et p colonnes à coefficients dans ***A*** se note : Lorsque n p , on dit que la matrice M est une matrice carrée

**19904**: On dit aussi que la matrice M est n p. L' ensemble des matrices à n lignes et p colonnes à coefficients dans A se ***note*** : Lorsque n p , on dit que la matrice M est une matrice carrée

**19911**: On dit aussi que la matrice M est n p. L' ensemble des matrices à n lignes et p colonnes à coefficients dans A se note : Lorsque n p , on ***dit*** que la matrice M est une matrice carrée

**19916**: On dit aussi que la matrice M est n p. L' ensemble des matrices à n lignes et p colonnes à coefficients dans A se note : Lorsque n p , on dit que la matrice M ***est*** une matrice carrée

**19931**: L' ensemble des matrices carrées n n à coefficients dans ***A*** se note : a. On trouve aussi souvent des parenthèses ( ) , mais nous utiliserons les crochets pour différencier familles et matrices

**19933**: L' ensemble des matrices carrées n n à coefficients dans A se ***note*** : a. On trouve aussi souvent des parenthèses ( ) , mais nous utiliserons les crochets pour différencier familles et matrices

**19935**: L' ensemble des matrices carrées n n à coefficients dans A se note : ***a.*** On trouve aussi souvent des parenthèses ( ) , mais nous utiliserons les crochets pour différencier familles et matrices

**19937**: L' ensemble des matrices carrées n n à coefficients dans A se note : a. On ***trouve*** aussi souvent des parenthèses ( ) , mais nous utiliserons les crochets pour différencier familles et matrices

**19960**: Si E ***est*** un K - espace vectoriel de dimension finie p , si E ( e1 ,

**19982**: , ep ) ***est*** une base de E et si x est un vecteur de E , alors on appelle matrice de x dans la base E et on note : On dit que M est une matrice colonne

**19990**: , ep ) est une base de E et si x ***est*** un vecteur de E , alors on appelle matrice de x dans la base E et on note : On dit que M est une matrice colonne

**19998**: , ep ) est une base de E et si x est un vecteur de E , alors on ***appelle*** matrice de x dans la base E et on note : On dit que M est une matrice colonne

**20008**: , ep ) est une base de E et si x est un vecteur de E , alors on appelle matrice de x dans la base E et on ***note*** : On dit que M est une matrice colonne

**20011**: , ep ) est une base de E et si x est un vecteur de E , alors on appelle matrice de x dans la base E et on note : On ***dit*** que M est une matrice colonne

**20014**: , ep ) est une base de E et si x est un vecteur de E , alors on appelle matrice de x dans la base E et on note : On dit que M ***est*** une matrice colonne

**20026**: Si E et E 0 ***sont*** deux K - espaces vectoriels de dimension finie et f L ( E , E 0 ) , si E ( e1 ,

**20056**: , ep ) ***est*** une base de E , si E 0 ( e01 ,

**20074**: , e0n ) ***est*** une base de E 0 , alors on appelle matrice de f dans les bases E et E 0 et on note : Noter que n le nombre de lignes est la dimension de l' espace d' arrivée et p le nombre de colonnes est la dimension de l' espace de départ ! 3

**20083**: , e0n ) est une base de E 0 , alors on ***appelle*** matrice de f dans les bases E et E 0 et on note : Noter que n le nombre de lignes est la dimension de l' espace d' arrivée et p le nombre de colonnes est la dimension de l' espace de départ ! 3

**20096**: , e0n ) est une base de E 0 , alors on appelle matrice de f dans les bases E et E 0 et on ***note*** : Noter que n le nombre de lignes est la dimension de l' espace d' arrivée et p le nombre de colonnes est la dimension de l' espace de départ ! 3

**20105**: , e0n ) est une base de E 0 , alors on appelle matrice de f dans les bases E et E 0 et on note : Noter que n le nombre de lignes ***est*** la dimension de l' espace d' arrivée et p le nombre de colonnes est la dimension de l' espace de départ ! 3

**20119**: , e0n ) est une base de E 0 , alors on appelle matrice de f dans les bases E et E 0 et on note : Noter que n le nombre de lignes est la dimension de l' espace d' arrivée et p le nombre de colonnes ***est*** la dimension de l' espace de départ ! 3

**20133**: Réciproquement , si ***A*** ai , j ( i , j)1,n1,p Mn , p ( K ) , on peut lui associer canoniquement l' application linéaire f définie sur Kp ( de base canonique ( e1 ,

**20149**: Réciproquement , si A ai , j ( i , j)1,n1,p Mn , p ( K ) , on ***peut*** lui associer canoniquement l' application linéaire f définie sur Kp ( de base canonique ( e1 ,

**20207**: Dans le cas particulier d' un endomorphisme ( E E , E E 0 et f L ( E ) ) , on ***appelle*** matrice de f dans la base E et on note : La matrice MatE ( f ) est une matrice carrée ( nombre de lignesnombre de colonnes )

**20217**: Dans le cas particulier d' un endomorphisme ( E E , E E 0 et f L ( E ) ) , on appelle matrice de f dans la base E et on ***note*** : La matrice MatE ( f ) est une matrice carrée ( nombre de lignesnombre de colonnes )

**20225**: Dans le cas particulier d' un endomorphisme ( E E , E E 0 et f L ( E ) ) , on appelle matrice de f dans la base E et on note : La matrice MatE ( f ) ***est*** une matrice carrée ( nombre de lignesnombre de colonnes )

**20242**: Si E R2 et f ***est*** la symétrie par rapport à y x , parallèlement à y x et si E ( 1 , 0 ) , ( 0 , 1 ) , On définit les deux opérations suivantes sur Mn , p ( K ) : L' addition a : si A Mn , p ( K ) et si B Mn , p ( K ) , on définit A B Mn , p ( K ) par an , p bn , p La multiplication externe : si K et si A Mn , p ( K ) , on définit .A Mn , p ( K ) par Il faut que les dimensions des matrices A et B soient les mêmes ! L' élément neutre pour l' addition de Mn , p ( K ) est appelée la matrice nulle , notée 0n , p et définie par Cette matrice correspond à l' application linéaire nulle 0L ( E , E 0 )

**20271**: Si E R2 et f est la symétrie par rapport à y x , parallèlement à y x et si E ( 1 , 0 ) , ( 0 , 1 ) , On ***définit*** les deux opérations suivantes sur Mn , p ( K ) : L' addition a : si A Mn , p ( K ) et si B Mn , p ( K ) , on définit A B Mn , p ( K ) par an , p bn , p La multiplication externe : si K et si A Mn , p ( K ) , on définit .A Mn , p ( K ) par Il faut que les dimensions des matrices A et B soient les mêmes ! L' élément neutre pour l' addition de Mn , p ( K ) est appelée la matrice nulle , notée 0n , p et définie par Cette matrice correspond à l' application linéaire nulle 0L ( E , E 0 )

**20286**: Si E R2 et f est la symétrie par rapport à y x , parallèlement à y x et si E ( 1 , 0 ) , ( 0 , 1 ) , On définit les deux opérations suivantes sur Mn , p ( K ) : L' addition ***a*** : si A Mn , p ( K ) et si B Mn , p ( K ) , on définit A B Mn , p ( K ) par an , p bn , p La multiplication externe : si K et si A Mn , p ( K ) , on définit .A Mn , p ( K ) par Il faut que les dimensions des matrices A et B soient les mêmes ! L' élément neutre pour l' addition de Mn , p ( K ) est appelée la matrice nulle , notée 0n , p et définie par Cette matrice correspond à l' application linéaire nulle 0L ( E , E 0 )

**20289**: Si E R2 et f est la symétrie par rapport à y x , parallèlement à y x et si E ( 1 , 0 ) , ( 0 , 1 ) , On définit les deux opérations suivantes sur Mn , p ( K ) : L' addition a : si ***A*** Mn , p ( K ) et si B Mn , p ( K ) , on définit A B Mn , p ( K ) par an , p bn , p La multiplication externe : si K et si A Mn , p ( K ) , on définit .A Mn , p ( K ) par Il faut que les dimensions des matrices A et B soient les mêmes ! L' élément neutre pour l' addition de Mn , p ( K ) est appelée la matrice nulle , notée 0n , p et définie par Cette matrice correspond à l' application linéaire nulle 0L ( E , E 0 )

**20307**: Si E R2 et f est la symétrie par rapport à y x , parallèlement à y x et si E ( 1 , 0 ) , ( 0 , 1 ) , On définit les deux opérations suivantes sur Mn , p ( K ) : L' addition a : si A Mn , p ( K ) et si B Mn , p ( K ) , on ***définit*** A B Mn , p ( K ) par an , p bn , p La multiplication externe : si K et si A Mn , p ( K ) , on définit .A Mn , p ( K ) par Il faut que les dimensions des matrices A et B soient les mêmes ! L' élément neutre pour l' addition de Mn , p ( K ) est appelée la matrice nulle , notée 0n , p et définie par Cette matrice correspond à l' application linéaire nulle 0L ( E , E 0 )

**20331**: Si E R2 et f est la symétrie par rapport à y x , parallèlement à y x et si E ( 1 , 0 ) , ( 0 , 1 ) , On définit les deux opérations suivantes sur Mn , p ( K ) : L' addition a : si A Mn , p ( K ) et si B Mn , p ( K ) , on définit A B Mn , p ( K ) par an , p bn , p La multiplication externe : si K et si ***A*** Mn , p ( K ) , on définit .A Mn , p ( K ) par Il faut que les dimensions des matrices A et B soient les mêmes ! L' élément neutre pour l' addition de Mn , p ( K ) est appelée la matrice nulle , notée 0n , p et définie par Cette matrice correspond à l' application linéaire nulle 0L ( E , E 0 )

**20340**: Si E R2 et f est la symétrie par rapport à y x , parallèlement à y x et si E ( 1 , 0 ) , ( 0 , 1 ) , On définit les deux opérations suivantes sur Mn , p ( K ) : L' addition a : si A Mn , p ( K ) et si B Mn , p ( K ) , on définit A B Mn , p ( K ) par an , p bn , p La multiplication externe : si K et si A Mn , p ( K ) , on ***définit*** .A Mn , p ( K ) par Il faut que les dimensions des matrices A et B soient les mêmes ! L' élément neutre pour l' addition de Mn , p ( K ) est appelée la matrice nulle , notée 0n , p et définie par Cette matrice correspond à l' application linéaire nulle 0L ( E , E 0 )

**20350**: Si E R2 et f est la symétrie par rapport à y x , parallèlement à y x et si E ( 1 , 0 ) , ( 0 , 1 ) , On définit les deux opérations suivantes sur Mn , p ( K ) : L' addition a : si A Mn , p ( K ) et si B Mn , p ( K ) , on définit A B Mn , p ( K ) par an , p bn , p La multiplication externe : si K et si A Mn , p ( K ) , on définit .A Mn , p ( K ) par Il ***faut*** que les dimensions des matrices A et B soient les mêmes ! L' élément neutre pour l' addition de Mn , p ( K ) est appelée la matrice nulle , notée 0n , p et définie par Cette matrice correspond à l' application linéaire nulle 0L ( E , E 0 )

**20376**: Si E R2 et f est la symétrie par rapport à y x , parallèlement à y x et si E ( 1 , 0 ) , ( 0 , 1 ) , On définit les deux opérations suivantes sur Mn , p ( K ) : L' addition a : si A Mn , p ( K ) et si B Mn , p ( K ) , on définit A B Mn , p ( K ) par an , p bn , p La multiplication externe : si K et si A Mn , p ( K ) , on définit .A Mn , p ( K ) par Il faut que les dimensions des matrices A et B soient les mêmes ! L' élément neutre pour l' addition de Mn , p ( K ) ***est*** appelée la matrice nulle , notée 0n , p et définie par Cette matrice correspond à l' application linéaire nulle 0L ( E , E 0 )

**20391**: Si E R2 et f est la symétrie par rapport à y x , parallèlement à y x et si E ( 1 , 0 ) , ( 0 , 1 ) , On définit les deux opérations suivantes sur Mn , p ( K ) : L' addition a : si A Mn , p ( K ) et si B Mn , p ( K ) , on définit A B Mn , p ( K ) par an , p bn , p La multiplication externe : si K et si A Mn , p ( K ) , on définit .A Mn , p ( K ) par Il faut que les dimensions des matrices A et B soient les mêmes ! L' élément neutre pour l' addition de Mn , p ( K ) est appelée la matrice nulle , notée 0n , p et définie par Cette matrice ***correspond*** à l' application linéaire nulle 0L ( E , E 0 )

**20428**: Lorsque n p , on note Proposition 2.1 L' ensemble Mn , p ( K ) muni des deux opérations ci - dessus ***est*** un K - espace vectoriel isomorphe à L ( E , E 0 ) pour tous K - espaces vectoriels E et E 0 , toute base E de E et toute base E 0 de E 0 , via l' isomorphisme En particulier : dim Mn , p ( K ) n p Démonstration Il est immédiat que Mn , p ( K ) est un K - espace vectoriel : c' est le même espace que Kn Kp muni des opérations usuelles ( il est donc de dimension finie n p )

**20485**: Lorsque n p , on note Proposition 2.1 L' ensemble Mn , p ( K ) muni des deux opérations ci - dessus est un K - espace vectoriel isomorphe à L ( E , E 0 ) pour tous K - espaces vectoriels E et E 0 , toute base E de E et toute base E 0 de E 0 , via l' isomorphisme En particulier : dim Mn , p ( K ) n p Démonstration Il ***est*** immédiat que Mn , p ( K ) est un K - espace vectoriel : c' est le même espace que Kn Kp muni des opérations usuelles ( il est donc de dimension finie n p )

**20494**: Lorsque n p , on note Proposition 2.1 L' ensemble Mn , p ( K ) muni des deux opérations ci - dessus est un K - espace vectoriel isomorphe à L ( E , E 0 ) pour tous K - espaces vectoriels E et E 0 , toute base E de E et toute base E 0 de E 0 , via l' isomorphisme En particulier : dim Mn , p ( K ) n p Démonstration Il est immédiat que Mn , p ( K ) ***est*** un K - espace vectoriel : c' est le même espace que Kn Kp muni des opérations usuelles ( il est donc de dimension finie n p )

**20502**: Lorsque n p , on note Proposition 2.1 L' ensemble Mn , p ( K ) muni des deux opérations ci - dessus est un K - espace vectoriel isomorphe à L ( E , E 0 ) pour tous K - espaces vectoriels E et E 0 , toute base E de E et toute base E 0 de E 0 , via l' isomorphisme En particulier : dim Mn , p ( K ) n p Démonstration Il est immédiat que Mn , p ( K ) est un K - espace vectoriel : c' ***est*** le même espace que Kn Kp muni des opérations usuelles ( il est donc de dimension finie n p )

**20515**: Lorsque n p , on note Proposition 2.1 L' ensemble Mn , p ( K ) muni des deux opérations ci - dessus est un K - espace vectoriel isomorphe à L ( E , E 0 ) pour tous K - espaces vectoriels E et E 0 , toute base E de E et toute base E 0 de E 0 , via l' isomorphisme En particulier : dim Mn , p ( K ) n p Démonstration Il est immédiat que Mn , p ( K ) est un K - espace vectoriel : c' est le même espace que Kn Kp muni des opérations usuelles ( il ***est*** donc de dimension finie n p )

**20527**: La linéarité de ***est*** immédiate

**20530**: ***Démontrons*** que est injective

**20532**: Démontrons que ***est*** injective

**20540**: On ***a*** donc MatE , E 0 ( f ) 0n , p

**20554**: Autrement ***dit*** , pour tout i 1 , n et en notant E ( e1 ,

**20586**: , ep ) , les coefficients de u(ei ) dans la base E 0 ***sont*** tous nuls

**20591**: On ***a*** donc u 0L ( E , E 0 ) et on en déduit que Ker 0L ( E , E 0 ) donc est injective

**20604**: On a donc u 0L ( E , E 0 ) et on en ***déduit*** que Ker 0L ( E , E 0 ) donc est injective

**20615**: On a donc u 0L ( E , E 0 ) et on en déduit que Ker 0L ( E , E 0 ) donc ***est*** injective

**20618**: ***est*** linéaire et injective entre deux espaces de même dimension , elle est bijective

**20630**: est linéaire et injective entre deux espaces de même dimension , elle ***est*** bijective

**20634**: C' ***est*** donc un isomorphisme

**20643**: En particulier , on ***retrouve*** le fait que et la propriété de linéarité En dimension finie , connaître une application linéaire revient à connaître sa matrice dans des bases données

**20660**: En particulier , on retrouve le fait que et la propriété de linéarité En dimension finie , connaître une application linéaire ***revient*** à connaître sa matrice dans des bases données

**20684**: Ainsi , pour résoudre un problème d' algèbre linéaire en dimension finie , on ***peut*** travailler géométriquement avec les applications linéaires , ou bien algébriquement avec des matrices , en fonction de ce qui est le plus pertinent

**20704**: Ainsi , pour résoudre un problème d' algèbre linéaire en dimension finie , on peut travailler géométriquement avec les applications linéaires , ou bien algébriquement avec des matrices , en fonction de ce qui ***est*** le plus pertinent

**20710**: Il ***existe*** une base naturelle de Mn , p ( K ) , appelée base canonique , donnée par Autrement dit , le coefficient en ( i , j ) de Ek , est nul , sauf pour ( i , j ) ( k , ) en lequel il vaut 1

**20729**: Il existe une base naturelle de Mn , p ( K ) , appelée base canonique , donnée par Autrement ***dit*** , le coefficient en ( i , j ) de Ek , est nul , sauf pour ( i , j ) ( k , ) en lequel il vaut 1

**20742**: Il existe une base naturelle de Mn , p ( K ) , appelée base canonique , donnée par Autrement dit , le coefficient en ( i , j ) de Ek , ***est*** nul , sauf pour ( i , j ) ( k , ) en lequel il vaut 1

**20759**: Il existe une base naturelle de Mn , p ( K ) , appelée base canonique , donnée par Autrement dit , le coefficient en ( i , j ) de Ek , est nul , sauf pour ( i , j ) ( k , ) en lequel il ***vaut*** 1

**20780**: Soit A Mn , p ( K ) et B Mp , q ( K ) , on ***définit*** le produit de A par B comme la matrice A B Mn , q ( K ) définie par Voir la figure 2.1 , page ci - contre a

**20809**: Soit A Mn , p ( K ) et B Mp , q ( K ) , on définit le produit de A par B comme la matrice A B Mn , q ( K ) définie par Voir la figure 2.1 , page ci - contre ***a***

**20820**: a. Tiré de http : www.texample.nettikzexamplesmatrix - multiplication Il ***faut*** bien faire attention à ce que les dimensions des matrices soient compatibles : lorsque l' on veut faire le produit A B , le nombre de colonnes de A doit être égal au nombre de lignes de B. Soit E et E 0 deux K - espaces vectoriels de dimension finie ( avec p dim E et n dim E 0 ) , soit E une base de E et E 0 une base de E 0 , soit f L ( E , E 0 ) et soit x E. Alors Autrement dit , le produit matriciel MatE , E 0 ( f ) MatE ( x ) traduit le calcul de f ( x )

**20837**: a. Tiré de http : www.texample.nettikzexamplesmatrix - multiplication Il faut bien faire attention à ce que les dimensions des matrices soient compatibles : lorsque l' on ***veut*** faire le produit A B , le nombre de colonnes de A doit être égal au nombre de lignes de B. Soit E et E 0 deux K - espaces vectoriels de dimension finie ( avec p dim E et n dim E 0 ) , soit E une base de E et E 0 une base de E 0 , soit f L ( E , E 0 ) et soit x E. Alors Autrement dit , le produit matriciel MatE , E 0 ( f ) MatE ( x ) traduit le calcul de f ( x )

**20849**: a. Tiré de http : www.texample.nettikzexamplesmatrix - multiplication Il faut bien faire attention à ce que les dimensions des matrices soient compatibles : lorsque l' on veut faire le produit A B , le nombre de colonnes de ***A*** doit être égal au nombre de lignes de B. Soit E et E 0 deux K - espaces vectoriels de dimension finie ( avec p dim E et n dim E 0 ) , soit E une base de E et E 0 une base de E 0 , soit f L ( E , E 0 ) et soit x E. Alors Autrement dit , le produit matriciel MatE , E 0 ( f ) MatE ( x ) traduit le calcul de f ( x )

**20850**: a. Tiré de http : www.texample.nettikzexamplesmatrix - multiplication Il faut bien faire attention à ce que les dimensions des matrices soient compatibles : lorsque l' on veut faire le produit A B , le nombre de colonnes de A ***doit*** être égal au nombre de lignes de B. Soit E et E 0 deux K - espaces vectoriels de dimension finie ( avec p dim E et n dim E 0 ) , soit E une base de E et E 0 une base de E 0 , soit f L ( E , E 0 ) et soit x E. Alors Autrement dit , le produit matriciel MatE , E 0 ( f ) MatE ( x ) traduit le calcul de f ( x )

**20914**: a. Tiré de http : www.texample.nettikzexamplesmatrix - multiplication Il faut bien faire attention à ce que les dimensions des matrices soient compatibles : lorsque l' on veut faire le produit A B , le nombre de colonnes de A doit être égal au nombre de lignes de B. Soit E et E 0 deux K - espaces vectoriels de dimension finie ( avec p dim E et n dim E 0 ) , soit E une base de E et E 0 une base de E 0 , soit f L ( E , E 0 ) et soit x E. Alors Autrement ***dit*** , le produit matriciel MatE , E 0 ( f ) MatE ( x ) traduit le calcul de f ( x )

**20930**: a. Tiré de http : www.texample.nettikzexamplesmatrix - multiplication Il faut bien faire attention à ce que les dimensions des matrices soient compatibles : lorsque l' on veut faire le produit A B , le nombre de colonnes de A doit être égal au nombre de lignes de B. Soit E et E 0 deux K - espaces vectoriels de dimension finie ( avec p dim E et n dim E 0 ) , soit E une base de E et E 0 une base de E 0 , soit f L ( E , E 0 ) et soit x E. Alors Autrement dit , le produit matriciel MatE , E 0 ( f ) MatE ( x ) ***traduit*** le calcul de f ( x )

**20964**: Figure 2.1 Produit matriciel B : p lignes q colonnes A : n lignes p colonnes C A.B : n lignes q colonnes Démonstration On ***reprend*** les notations des points 1 et 2 de la définition 2.2 , page 90

**20981**: On ***a*** Autrement dit , ai , j xj .e0i j1 ai , j xj est le i - ième coefficient de f ( x ) dans la base E ( e1 ,

**20983**: On a Autrement ***dit*** , ai , j xj .e0i j1 ai , j xj est le i - ième coefficient de f ( x ) dans la base E ( e1 ,

**20991**: On a Autrement dit , ai , j xj .e0i j1 ***ai*** , j xj est le i - ième coefficient de f ( x ) dans la base E ( e1 ,

**20995**: On a Autrement dit , ai , j xj .e0i j1 ai , j xj ***est*** le i - ième coefficient de f ( x ) dans la base E ( e1 ,

**21021**: On ***a*** donc qui est par définition du produit matriciel le i - ième coefficient de la matrice colonne MatE , E 0 ( f ) MatE ( x ) , d' où le résultat

**21024**: On a donc qui ***est*** par définition du produit matriciel le i - ième coefficient de la matrice colonne MatE , E 0 ( f ) MatE ( x ) , d' où le résultat

**21098**: Proposition 2.2 Correspondance entre composition et produit matriciel Soit E , E 0 et E 00 des K - espaces vectoriels de dimension finie ( p dim E , n dim E 0 et q dim E 00 ) , soit Autrement ***dit*** , le produit matriciel MatE 0 , E 00 ( g ) MatE , E 0 ( f ) traduit le calcul de g f

**21118**: Proposition 2.2 Correspondance entre composition et produit matriciel Soit E , E 0 et E 00 des K - espaces vectoriels de dimension finie ( p dim E , n dim E 0 et q dim E 00 ) , soit Autrement dit , le produit matriciel MatE 0 , E 00 ( g ) MatE , E 0 ( f ) ***traduit*** le calcul de g f

**21157**: Soit j 1 , p. La j - ième colonne de MatE , E 00 ( g f ) ***est*** donnée par en remarquant que MatE ( ej ) Mp,1 ( K ) a des zéros partout sauf en position j et en utilisant plusieurs fois la propriété 2.1 , page 98

**21171**: Soit j 1 , p. La j - ième colonne de MatE , E 00 ( g f ) est donnée par en remarquant que MatE ( ej ) Mp,1 ( K ) ***a*** des zéros partout sauf en position j et en utilisant plusieurs fois la propriété 2.1 , page 98

**21221**: Finalement , les colonnes de MatE , E 00 ( g f ) et de MatE 0 , E 00 ( g ) MatE , E 0 ( f ) ***sont*** les mêmes , ces deux matrices sont donc égales

**21228**: Finalement , les colonnes de MatE , E 00 ( g f ) et de MatE 0 , E 00 ( g ) MatE , E 0 ( f ) sont les mêmes , ces deux matrices ***sont*** donc égales

**21237**: Lorsque p n , on ***appelle*** matrice identité d' ordre p et on note : La matrice identité Ip correspond l' endomorphisme idE , l' application identité de E ( avec p dim E ) , dans ne importe quelle base de E. Il est important de savoir calculer les produits de matrices et de savoir prouver que le résultat est correct ! Ainsi , si Ek , est de taille n p , et Er , s est de taille p q , alors : En effet : Les dimensions sont compatibles , on peut effectuer le produit et : Le produit est associatif : Le produit est distributif à gauche et à droite : Existence d' éléments neutres ( à gauche et à droite ) pour la multiplication : Existence d' éléments absorbants pour la multiplication : Démonstration Deux méthodes : on peut vérifier ces égalités en calculant tous ces produits en utilisant la définition du produit ( définition 2.4 , page 98 ) on peut également utiliser l' isomorphisme de la proposition 2.1 , page 96 , les égalités ci - dessus découlent alors des propriétés de la composition des applications linéaires ( la démonstration est laissée en exercice )

**21245**: Lorsque p n , on appelle matrice identité d' ordre p et on ***note*** : La matrice identité Ip correspond l' endomorphisme idE , l' application identité de E ( avec p dim E ) , dans ne importe quelle base de E. Il est important de savoir calculer les produits de matrices et de savoir prouver que le résultat est correct ! Ainsi , si Ek , est de taille n p , et Er , s est de taille p q , alors : En effet : Les dimensions sont compatibles , on peut effectuer le produit et : Le produit est associatif : Le produit est distributif à gauche et à droite : Existence d' éléments neutres ( à gauche et à droite ) pour la multiplication : Existence d' éléments absorbants pour la multiplication : Démonstration Deux méthodes : on peut vérifier ces égalités en calculant tous ces produits en utilisant la définition du produit ( définition 2.4 , page 98 ) on peut également utiliser l' isomorphisme de la proposition 2.1 , page 96 , les égalités ci - dessus découlent alors des propriétés de la composition des applications linéaires ( la démonstration est laissée en exercice )

**21251**: Lorsque p n , on appelle matrice identité d' ordre p et on note : La matrice identité Ip ***correspond*** l' endomorphisme idE , l' application identité de E ( avec p dim E ) , dans ne importe quelle base de E. Il est important de savoir calculer les produits de matrices et de savoir prouver que le résultat est correct ! Ainsi , si Ek , est de taille n p , et Er , s est de taille p q , alors : En effet : Les dimensions sont compatibles , on peut effectuer le produit et : Le produit est associatif : Le produit est distributif à gauche et à droite : Existence d' éléments neutres ( à gauche et à droite ) pour la multiplication : Existence d' éléments absorbants pour la multiplication : Démonstration Deux méthodes : on peut vérifier ces égalités en calculant tous ces produits en utilisant la définition du produit ( définition 2.4 , page 98 ) on peut également utiliser l' isomorphisme de la proposition 2.1 , page 96 , les égalités ci - dessus découlent alors des propriétés de la composition des applications linéaires ( la démonstration est laissée en exercice )

**21270**: Lorsque p n , on appelle matrice identité d' ordre p et on note : La matrice identité Ip correspond l' endomorphisme idE , l' application identité de E ( avec p dim E ) , dans ne ***importe*** quelle base de E. Il est important de savoir calculer les produits de matrices et de savoir prouver que le résultat est correct ! Ainsi , si Ek , est de taille n p , et Er , s est de taille p q , alors : En effet : Les dimensions sont compatibles , on peut effectuer le produit et : Le produit est associatif : Le produit est distributif à gauche et à droite : Existence d' éléments neutres ( à gauche et à droite ) pour la multiplication : Existence d' éléments absorbants pour la multiplication : Démonstration Deux méthodes : on peut vérifier ces égalités en calculant tous ces produits en utilisant la définition du produit ( définition 2.4 , page 98 ) on peut également utiliser l' isomorphisme de la proposition 2.1 , page 96 , les égalités ci - dessus découlent alors des propriétés de la composition des applications linéaires ( la démonstration est laissée en exercice )

**21276**: Lorsque p n , on appelle matrice identité d' ordre p et on note : La matrice identité Ip correspond l' endomorphisme idE , l' application identité de E ( avec p dim E ) , dans ne importe quelle base de E. Il ***est*** important de savoir calculer les produits de matrices et de savoir prouver que le résultat est correct ! Ainsi , si Ek , est de taille n p , et Er , s est de taille p q , alors : En effet : Les dimensions sont compatibles , on peut effectuer le produit et : Le produit est associatif : Le produit est distributif à gauche et à droite : Existence d' éléments neutres ( à gauche et à droite ) pour la multiplication : Existence d' éléments absorbants pour la multiplication : Démonstration Deux méthodes : on peut vérifier ces égalités en calculant tous ces produits en utilisant la définition du produit ( définition 2.4 , page 98 ) on peut également utiliser l' isomorphisme de la proposition 2.1 , page 96 , les égalités ci - dessus découlent alors des propriétés de la composition des applications linéaires ( la démonstration est laissée en exercice )

**21292**: Lorsque p n , on appelle matrice identité d' ordre p et on note : La matrice identité Ip correspond l' endomorphisme idE , l' application identité de E ( avec p dim E ) , dans ne importe quelle base de E. Il est important de savoir calculer les produits de matrices et de savoir prouver que le résultat ***est*** correct ! Ainsi , si Ek , est de taille n p , et Er , s est de taille p q , alors : En effet : Les dimensions sont compatibles , on peut effectuer le produit et : Le produit est associatif : Le produit est distributif à gauche et à droite : Existence d' éléments neutres ( à gauche et à droite ) pour la multiplication : Existence d' éléments absorbants pour la multiplication : Démonstration Deux méthodes : on peut vérifier ces égalités en calculant tous ces produits en utilisant la définition du produit ( définition 2.4 , page 98 ) on peut également utiliser l' isomorphisme de la proposition 2.1 , page 96 , les égalités ci - dessus découlent alors des propriétés de la composition des applications linéaires ( la démonstration est laissée en exercice )

**21300**: Lorsque p n , on appelle matrice identité d' ordre p et on note : La matrice identité Ip correspond l' endomorphisme idE , l' application identité de E ( avec p dim E ) , dans ne importe quelle base de E. Il est important de savoir calculer les produits de matrices et de savoir prouver que le résultat est correct ! Ainsi , si Ek , ***est*** de taille n p , et Er , s est de taille p q , alors : En effet : Les dimensions sont compatibles , on peut effectuer le produit et : Le produit est associatif : Le produit est distributif à gauche et à droite : Existence d' éléments neutres ( à gauche et à droite ) pour la multiplication : Existence d' éléments absorbants pour la multiplication : Démonstration Deux méthodes : on peut vérifier ces égalités en calculant tous ces produits en utilisant la définition du produit ( définition 2.4 , page 98 ) on peut également utiliser l' isomorphisme de la proposition 2.1 , page 96 , les égalités ci - dessus découlent alors des propriétés de la composition des applications linéaires ( la démonstration est laissée en exercice )

**21310**: Lorsque p n , on appelle matrice identité d' ordre p et on note : La matrice identité Ip correspond l' endomorphisme idE , l' application identité de E ( avec p dim E ) , dans ne importe quelle base de E. Il est important de savoir calculer les produits de matrices et de savoir prouver que le résultat est correct ! Ainsi , si Ek , est de taille n p , et Er , s ***est*** de taille p q , alors : En effet : Les dimensions sont compatibles , on peut effectuer le produit et : Le produit est associatif : Le produit est distributif à gauche et à droite : Existence d' éléments neutres ( à gauche et à droite ) pour la multiplication : Existence d' éléments absorbants pour la multiplication : Démonstration Deux méthodes : on peut vérifier ces égalités en calculant tous ces produits en utilisant la définition du produit ( définition 2.4 , page 98 ) on peut également utiliser l' isomorphisme de la proposition 2.1 , page 96 , les égalités ci - dessus découlent alors des propriétés de la composition des applications linéaires ( la démonstration est laissée en exercice )

**21323**: Lorsque p n , on appelle matrice identité d' ordre p et on note : La matrice identité Ip correspond l' endomorphisme idE , l' application identité de E ( avec p dim E ) , dans ne importe quelle base de E. Il est important de savoir calculer les produits de matrices et de savoir prouver que le résultat est correct ! Ainsi , si Ek , est de taille n p , et Er , s est de taille p q , alors : En effet : Les dimensions ***sont*** compatibles , on peut effectuer le produit et : Le produit est associatif : Le produit est distributif à gauche et à droite : Existence d' éléments neutres ( à gauche et à droite ) pour la multiplication : Existence d' éléments absorbants pour la multiplication : Démonstration Deux méthodes : on peut vérifier ces égalités en calculant tous ces produits en utilisant la définition du produit ( définition 2.4 , page 98 ) on peut également utiliser l' isomorphisme de la proposition 2.1 , page 96 , les égalités ci - dessus découlent alors des propriétés de la composition des applications linéaires ( la démonstration est laissée en exercice )

**21327**: Lorsque p n , on appelle matrice identité d' ordre p et on note : La matrice identité Ip correspond l' endomorphisme idE , l' application identité de E ( avec p dim E ) , dans ne importe quelle base de E. Il est important de savoir calculer les produits de matrices et de savoir prouver que le résultat est correct ! Ainsi , si Ek , est de taille n p , et Er , s est de taille p q , alors : En effet : Les dimensions sont compatibles , on ***peut*** effectuer le produit et : Le produit est associatif : Le produit est distributif à gauche et à droite : Existence d' éléments neutres ( à gauche et à droite ) pour la multiplication : Existence d' éléments absorbants pour la multiplication : Démonstration Deux méthodes : on peut vérifier ces égalités en calculant tous ces produits en utilisant la définition du produit ( définition 2.4 , page 98 ) on peut également utiliser l' isomorphisme de la proposition 2.1 , page 96 , les égalités ci - dessus découlent alors des propriétés de la composition des applications linéaires ( la démonstration est laissée en exercice )

**21335**: Lorsque p n , on appelle matrice identité d' ordre p et on note : La matrice identité Ip correspond l' endomorphisme idE , l' application identité de E ( avec p dim E ) , dans ne importe quelle base de E. Il est important de savoir calculer les produits de matrices et de savoir prouver que le résultat est correct ! Ainsi , si Ek , est de taille n p , et Er , s est de taille p q , alors : En effet : Les dimensions sont compatibles , on peut effectuer le produit et : Le produit ***est*** associatif : Le produit est distributif à gauche et à droite : Existence d' éléments neutres ( à gauche et à droite ) pour la multiplication : Existence d' éléments absorbants pour la multiplication : Démonstration Deux méthodes : on peut vérifier ces égalités en calculant tous ces produits en utilisant la définition du produit ( définition 2.4 , page 98 ) on peut également utiliser l' isomorphisme de la proposition 2.1 , page 96 , les égalités ci - dessus découlent alors des propriétés de la composition des applications linéaires ( la démonstration est laissée en exercice )

**21340**: Lorsque p n , on appelle matrice identité d' ordre p et on note : La matrice identité Ip correspond l' endomorphisme idE , l' application identité de E ( avec p dim E ) , dans ne importe quelle base de E. Il est important de savoir calculer les produits de matrices et de savoir prouver que le résultat est correct ! Ainsi , si Ek , est de taille n p , et Er , s est de taille p q , alors : En effet : Les dimensions sont compatibles , on peut effectuer le produit et : Le produit est associatif : Le produit ***est*** distributif à gauche et à droite : Existence d' éléments neutres ( à gauche et à droite ) pour la multiplication : Existence d' éléments absorbants pour la multiplication : Démonstration Deux méthodes : on peut vérifier ces égalités en calculant tous ces produits en utilisant la définition du produit ( définition 2.4 , page 98 ) on peut également utiliser l' isomorphisme de la proposition 2.1 , page 96 , les égalités ci - dessus découlent alors des propriétés de la composition des applications linéaires ( la démonstration est laissée en exercice )

**21376**: Lorsque p n , on appelle matrice identité d' ordre p et on note : La matrice identité Ip correspond l' endomorphisme idE , l' application identité de E ( avec p dim E ) , dans ne importe quelle base de E. Il est important de savoir calculer les produits de matrices et de savoir prouver que le résultat est correct ! Ainsi , si Ek , est de taille n p , et Er , s est de taille p q , alors : En effet : Les dimensions sont compatibles , on peut effectuer le produit et : Le produit est associatif : Le produit est distributif à gauche et à droite : Existence d' éléments neutres ( à gauche et à droite ) pour la multiplication : Existence d' éléments absorbants pour la multiplication : Démonstration Deux méthodes : on ***peut*** vérifier ces égalités en calculant tous ces produits en utilisant la définition du produit ( définition 2.4 , page 98 ) on peut également utiliser l' isomorphisme de la proposition 2.1 , page 96 , les égalités ci - dessus découlent alors des propriétés de la composition des applications linéaires ( la démonstration est laissée en exercice )

**21399**: Lorsque p n , on appelle matrice identité d' ordre p et on note : La matrice identité Ip correspond l' endomorphisme idE , l' application identité de E ( avec p dim E ) , dans ne importe quelle base de E. Il est important de savoir calculer les produits de matrices et de savoir prouver que le résultat est correct ! Ainsi , si Ek , est de taille n p , et Er , s est de taille p q , alors : En effet : Les dimensions sont compatibles , on peut effectuer le produit et : Le produit est associatif : Le produit est distributif à gauche et à droite : Existence d' éléments neutres ( à gauche et à droite ) pour la multiplication : Existence d' éléments absorbants pour la multiplication : Démonstration Deux méthodes : on peut vérifier ces égalités en calculant tous ces produits en utilisant la définition du produit ( définition 2.4 , page 98 ) on ***peut*** également utiliser l' isomorphisme de la proposition 2.1 , page 96 , les égalités ci - dessus découlent alors des propriétés de la composition des applications linéaires ( la démonstration est laissée en exercice )

**21417**: Lorsque p n , on appelle matrice identité d' ordre p et on note : La matrice identité Ip correspond l' endomorphisme idE , l' application identité de E ( avec p dim E ) , dans ne importe quelle base de E. Il est important de savoir calculer les produits de matrices et de savoir prouver que le résultat est correct ! Ainsi , si Ek , est de taille n p , et Er , s est de taille p q , alors : En effet : Les dimensions sont compatibles , on peut effectuer le produit et : Le produit est associatif : Le produit est distributif à gauche et à droite : Existence d' éléments neutres ( à gauche et à droite ) pour la multiplication : Existence d' éléments absorbants pour la multiplication : Démonstration Deux méthodes : on peut vérifier ces égalités en calculant tous ces produits en utilisant la définition du produit ( définition 2.4 , page 98 ) on peut également utiliser l' isomorphisme de la proposition 2.1 , page 96 , les égalités ci - dessus ***découlent*** alors des propriétés de la composition des applications linéaires ( la démonstration est laissée en exercice )

**21430**: Lorsque p n , on appelle matrice identité d' ordre p et on note : La matrice identité Ip correspond l' endomorphisme idE , l' application identité de E ( avec p dim E ) , dans ne importe quelle base de E. Il est important de savoir calculer les produits de matrices et de savoir prouver que le résultat est correct ! Ainsi , si Ek , est de taille n p , et Er , s est de taille p q , alors : En effet : Les dimensions sont compatibles , on peut effectuer le produit et : Le produit est associatif : Le produit est distributif à gauche et à droite : Existence d' éléments neutres ( à gauche et à droite ) pour la multiplication : Existence d' éléments absorbants pour la multiplication : Démonstration Deux méthodes : on peut vérifier ces égalités en calculant tous ces produits en utilisant la définition du produit ( définition 2.4 , page 98 ) on peut également utiliser l' isomorphisme de la proposition 2.1 , page 96 , les égalités ci - dessus découlent alors des propriétés de la composition des applications linéaires ( la démonstration ***est*** laissée en exercice )

**21447**: Pour toute matrice carrée A Mn ( K ) , on ***pose*** Si A et B sont deux matrices carrées qui commutent ( c' est - à - dire A.B B .A ) , alors on a la formule du binôme : A0 In et pour tout k N , Ak1 Ak A L' hypothèse A B B A est indispensable

**21452**: Pour toute matrice carrée A Mn ( K ) , on pose Si A et B ***sont*** deux matrices carrées qui commutent ( c' est - à - dire A.B B .A ) , alors on a la formule du binôme : A0 In et pour tout k N , Ak1 Ak A L' hypothèse A B B A est indispensable

**21457**: Pour toute matrice carrée A Mn ( K ) , on pose Si A et B sont deux matrices carrées qui ***commutent*** ( c' est - à - dire A.B B .A ) , alors on a la formule du binôme : A0 In et pour tout k N , Ak1 Ak A L' hypothèse A B B A est indispensable

**21472**: Pour toute matrice carrée A Mn ( K ) , on pose Si A et B sont deux matrices carrées qui commutent ( c' est - à - dire A.B B .A ) , alors on ***a*** la formule du binôme : A0 In et pour tout k N , Ak1 Ak A L' hypothèse A B B A est indispensable

**21491**: Pour toute matrice carrée A Mn ( K ) , on pose Si A et B sont deux matrices carrées qui commutent ( c' est - à - dire A.B B .A ) , alors on a la formule du binôme : A0 In et pour tout k N , Ak1 Ak A L' hypothèse ***A*** B B A est indispensable

**21495**: Pour toute matrice carrée A Mn ( K ) , on pose Si A et B sont deux matrices carrées qui commutent ( c' est - à - dire A.B B .A ) , alors on a la formule du binôme : A0 In et pour tout k N , Ak1 Ak A L' hypothèse A B B A ***est*** indispensable

**21552**: 2.1.4 Soit ( ***a*** , b ) R2 et soit A Mn ( R ) définie par : ai , j a si i j Trouver toutes les matrices qui commutent avec A , c' est - à - dire déterminer l' ensemble 2.1.5 On suppose que a , b et c sont trois nombrs complexes tels que a2 b2 c2 1

**21570**: 2.1.4 Soit ( a , b ) R2 et soit A Mn ( R ) définie par : ai , j ***a*** si i j Trouver toutes les matrices qui commutent avec A , c' est - à - dire déterminer l' ensemble 2.1.5 On suppose que a , b et c sont trois nombrs complexes tels que a2 b2 c2 1

**21579**: 2.1.4 Soit ( a , b ) R2 et soit A Mn ( R ) définie par : ai , j a si i j Trouver toutes les matrices qui ***commutent*** avec A , c' est - à - dire déterminer l' ensemble 2.1.5 On suppose que a , b et c sont trois nombrs complexes tels que a2 b2 c2 1

**21594**: 2.1.4 Soit ( a , b ) R2 et soit A Mn ( R ) définie par : ai , j a si i j Trouver toutes les matrices qui commutent avec A , c' est - à - dire déterminer l' ensemble 2.1.5 On ***suppose*** que a , b et c sont trois nombrs complexes tels que a2 b2 c2 1

**21596**: 2.1.4 Soit ( a , b ) R2 et soit A Mn ( R ) définie par : ai , j a si i j Trouver toutes les matrices qui commutent avec A , c' est - à - dire déterminer l' ensemble 2.1.5 On suppose que ***a*** , b et c sont trois nombrs complexes tels que a2 b2 c2 1

**21601**: 2.1.4 Soit ( a , b ) R2 et soit A Mn ( R ) définie par : ai , j a si i j Trouver toutes les matrices qui commutent avec A , c' est - à - dire déterminer l' ensemble 2.1.5 On suppose que a , b et c ***sont*** trois nombrs complexes tels que a2 b2 c2 1

**21613**: On ***pose*** : Démontrer que : 2.1.6 Soit A , B et C trois matrices de Mn ( R ) telles que : Démontrer que : Transposition Soit A un ensemble , ( n , p ) N 2 , alors l' application définie par : est appelée transposition

**21658**: On pose : Démontrer que : 2.1.6 Soit A , B et C trois matrices de Mn ( R ) telles que : Démontrer que : Transposition Soit A un ensemble , ( n , p ) N 2 , alors l' application définie par : ***est*** appelée transposition

**21666**: La matrice t M ***est*** appelée la transposée a de M

**21670**: La matrice t M est appelée la transposée ***a*** de M

**21674**: ***a.*** On rencontre également la notation M pour la transposée de M

**21676**: a. On ***rencontre*** également la notation M pour la transposée de M

**21689**: La transposition ***est*** une application linéaire : La transposition est involutive : La transposition est contravariante : Démonstration Les deux premiers points sont immédiats par définition

**21696**: La transposition est une application linéaire : La transposition ***est*** involutive : La transposition est contravariante : Démonstration Les deux premiers points sont immédiats par définition

**21701**: La transposition est une application linéaire : La transposition est involutive : La transposition ***est*** contravariante : Démonstration Les deux premiers points sont immédiats par définition

**21709**: La transposition est une application linéaire : La transposition est involutive : La transposition est contravariante : Démonstration Les deux premiers points ***sont*** immédiats par définition

**21779**: Il y deux manières de considérer les formes linéaires d' un K - espace vectoriel E de dimension finie : comme des applications linéaires de E dans K elles ***sont*** alors représentées par des matrices de M1,p ( K ) en fixant une base de E ( avec p dim E ) comme des vecteurs de E ? elles sont alors représentées dans une base de E ? par des matrices de Y a -t -il un lien entre ces deux représentations ? Soit E ( e1 ,

**21809**: Il y deux manières de considérer les formes linéaires d' un K - espace vectoriel E de dimension finie : comme des applications linéaires de E dans K elles sont alors représentées par des matrices de M1,p ( K ) en fixant une base de E ( avec p dim E ) comme des vecteurs de E ? elles ***sont*** alors représentées dans une base de E ? par des matrices de Y a -t -il un lien entre ces deux représentations ? Soit E ( e1 ,

**21823**: Il y deux manières de considérer les formes linéaires d' un K - espace vectoriel E de dimension finie : comme des applications linéaires de E dans K elles sont alors représentées par des matrices de M1,p ( K ) en fixant une base de E ( avec p dim E ) comme des vecteurs de E ? elles sont alors représentées dans une base de E ? par des matrices de Y ***a*** -t -il un lien entre ces deux représentations ? Soit E ( e1 ,

**21863**: , ep ) une base de E et E ? , en prenant ( 1 ) comme base de K , on ***a*** : On a donc , pour tout x E : Notation 2.1 Dans la suite de ce cours , on conviendra que les matrices à 1 ligne et 1 colonne seront notées comme des scalaires

**21866**: , ep ) une base de E et E ? , en prenant ( 1 ) comme base de K , on a : On ***a*** donc , pour tout x E : Notation 2.1 Dans la suite de ce cours , on conviendra que les matrices à 1 ligne et 1 colonne seront notées comme des scalaires

**21903**: Soit : On ***a*** donc , avec cet abus de notation , Soit E ? la base duale de E , notons : de sorte que : Finalement : Autrement dit , 2

**21930**: Soit : On a donc , avec cet abus de notation , Soit E ? la base duale de E , notons : de sorte que : Finalement : Autrement ***dit*** , 2

**21938**: En Wxmaxima , on ***obtient*** : Soit M Mp ( K )

**21948**: On ***dit*** que M Mp ( K ) est symétrique a si elle vérifie t M M

**21955**: On dit que M Mp ( K ) ***est*** symétrique a si elle vérifie t M M

**21957**: On dit que M Mp ( K ) est symétrique ***a*** si elle vérifie t M M

**21960**: On dit que M Mp ( K ) est symétrique a si elle ***vérifie*** t M M

**21975**: L' ensemble des matrices symétriques de Mp ( K ) ***est*** noté On dit que M Mp ( K ) est antisymétrique si elle vérifie t M M

**21978**: L' ensemble des matrices symétriques de Mp ( K ) est noté On ***dit*** que M Mp ( K ) est antisymétrique si elle vérifie t M M

**21985**: L' ensemble des matrices symétriques de Mp ( K ) est noté On dit que M Mp ( K ) ***est*** antisymétrique si elle vérifie t M M

**21989**: L' ensemble des matrices symétriques de Mp ( K ) est noté On dit que M Mp ( K ) est antisymétrique si elle ***vérifie*** t M M

**22004**: L' ensemble des matrices antisymétriques de Mp ( K ) ***est*** noté a. Les matrices M vérifiant t M M sont nécessairement carrées ! ( a ) Démontrer que Sp ( K ) et Ap ( K ) sont des sous-espaces vectoriels de Mp ( K )

**22006**: L' ensemble des matrices antisymétriques de Mp ( K ) est noté ***a.*** Les matrices M vérifiant t M M sont nécessairement carrées ! ( a ) Démontrer que Sp ( K ) et Ap ( K ) sont des sous-espaces vectoriels de Mp ( K )

**22014**: L' ensemble des matrices antisymétriques de Mp ( K ) est noté a. Les matrices M vérifiant t M M ***sont*** nécessairement carrées ! ( a ) Démontrer que Sp ( K ) et Ap ( K ) sont des sous-espaces vectoriels de Mp ( K )

**22019**: L' ensemble des matrices antisymétriques de Mp ( K ) est noté a. Les matrices M vérifiant t M M sont nécessairement carrées ! ( ***a*** ) Démontrer que Sp ( K ) et Ap ( K ) sont des sous-espaces vectoriels de Mp ( K )

**22032**: L' ensemble des matrices antisymétriques de Mp ( K ) est noté a. Les matrices M vérifiant t M M sont nécessairement carrées ! ( a ) Démontrer que Sp ( K ) et Ap ( K ) ***sont*** des sous-espaces vectoriels de Mp ( K )

**22073**: Matrices diagonales , matrices triangulaires On ***dit*** que A est diagonale si tous ses coefficients non-diagonaux sont nuls , c' est - à - dire : Autrement dit : On note Dp ( K ) l' ensemble des matrices diagonales de Mp ( K )

**22076**: Matrices diagonales , matrices triangulaires On dit que A ***est*** diagonale si tous ses coefficients non-diagonaux sont nuls , c' est - à - dire : Autrement dit : On note Dp ( K ) l' ensemble des matrices diagonales de Mp ( K )

**22083**: Matrices diagonales , matrices triangulaires On dit que A est diagonale si tous ses coefficients non-diagonaux ***sont*** nuls , c' est - à - dire : Autrement dit : On note Dp ( K ) l' ensemble des matrices diagonales de Mp ( K )

**22094**: Matrices diagonales , matrices triangulaires On dit que A est diagonale si tous ses coefficients non-diagonaux sont nuls , c' est - à - dire : Autrement ***dit*** : On note Dp ( K ) l' ensemble des matrices diagonales de Mp ( K )

**22097**: Matrices diagonales , matrices triangulaires On dit que A est diagonale si tous ses coefficients non-diagonaux sont nuls , c' est - à - dire : Autrement dit : On ***note*** Dp ( K ) l' ensemble des matrices diagonales de Mp ( K )

**22114**: On ***dit*** que A est triangulaire supérieure si tous ses coefficients au - dessous de sa diagonale sont nuls , c' est - à - dire : Autrement dit : p ( K ) l' ensemble des matrices triangulaires supérieures de Mp ( K )

**22117**: On dit que A ***est*** triangulaire supérieure si tous ses coefficients au - dessous de sa diagonale sont nuls , c' est - à - dire : Autrement dit : p ( K ) l' ensemble des matrices triangulaires supérieures de Mp ( K )

**22130**: On dit que A est triangulaire supérieure si tous ses coefficients au - dessous de sa diagonale ***sont*** nuls , c' est - à - dire : Autrement dit : p ( K ) l' ensemble des matrices triangulaires supérieures de Mp ( K )

**22141**: On dit que A est triangulaire supérieure si tous ses coefficients au - dessous de sa diagonale sont nuls , c' est - à - dire : Autrement ***dit*** : p ( K ) l' ensemble des matrices triangulaires supérieures de Mp ( K )

**22160**: On ***dit*** que A est triangulaire inférieur si tous ses coefficients au-dessus de sa diagonale sont nuls , c' est - à - dire : Autrement dit : p ( K ) l' ensemble des matrices triangulaires inférieures de Mp ( K )

**22163**: On dit que A ***est*** triangulaire inférieur si tous ses coefficients au-dessus de sa diagonale sont nuls , c' est - à - dire : Autrement dit : p ( K ) l' ensemble des matrices triangulaires inférieures de Mp ( K )

**22174**: On dit que A est triangulaire inférieur si tous ses coefficients au-dessus de sa diagonale ***sont*** nuls , c' est - à - dire : Autrement dit : p ( K ) l' ensemble des matrices triangulaires inférieures de Mp ( K )

**22185**: On dit que A est triangulaire inférieur si tous ses coefficients au-dessus de sa diagonale sont nuls , c' est - à - dire : Autrement ***dit*** : p ( K ) l' ensemble des matrices triangulaires inférieures de Mp ( K )

**22204**: Il ***est*** indispensable que A soit une matrice carrée ! ( a ) Démontrer que Dp ( K ) , T p ( K ) et Tp ( K ) sont des sous-espaces vectoriels de Mp ( K )

**22214**: Il est indispensable que A soit une matrice carrée ! ( ***a*** ) Démontrer que Dp ( K ) , T p ( K ) et Tp ( K ) sont des sous-espaces vectoriels de Mp ( K )

**22233**: Il est indispensable que A soit une matrice carrée ! ( a ) Démontrer que Dp ( K ) , T p ( K ) et Tp ( K ) ***sont*** des sous-espaces vectoriels de Mp ( K )

**22244**: Quelles ***sont*** leurs dimension respectives ? ( b ) Démontrer que ces trois espaces sont stables par produit et exprimer simplement les coefficients diagonaux du produit

**22257**: Quelles sont leurs dimension respectives ? ( b ) Démontrer que ces trois espaces ***sont*** stables par produit et exprimer simplement les coefficients diagonaux du produit

**22275**: Trace d' une matrice Soit ***a*** A ai , j ( i , j)1,n Mp ( K )

**22290**: On ***définit*** la trace de A , notée trace(A ) , comme la somme des éléments diagonaux de A : Il est indispensable que A soit une matrice carrée ! 1

**22310**: On définit la trace de A , notée trace(A ) , comme la somme des éléments diagonaux de A : Il ***est*** indispensable que A soit une matrice carrée ! 1

**22325**: A 7 trace(A ) ***est*** une forme linéaire sur Mp ( K )

**22339**: La trace ***est*** invariante par transposition : Mp ( K ) , trace(t A ) trace(A ) 3

**22358**: La trace ***est*** invariante par commutation de deux matrices a : Mais il est faux de penser que trace(A B C ) trace(A C B ) On peut commuter deux matrices , mais on ne peux pas changer ne importe quel ordre ! Démonstration En exercice

**22365**: La trace est invariante par commutation de deux matrices ***a*** : Mais il est faux de penser que trace(A B C ) trace(A C B ) On peut commuter deux matrices , mais on ne peux pas changer ne importe quel ordre ! Démonstration En exercice

**22369**: La trace est invariante par commutation de deux matrices a : Mais il ***est*** faux de penser que trace(A B C ) trace(A C B ) On peut commuter deux matrices , mais on ne peux pas changer ne importe quel ordre ! Démonstration En exercice

**22383**: La trace est invariante par commutation de deux matrices a : Mais il est faux de penser que trace(A B C ) trace(A C B ) On ***peut*** commuter deux matrices , mais on ne peux pas changer ne importe quel ordre ! Démonstration En exercice

**22391**: La trace est invariante par commutation de deux matrices a : Mais il est faux de penser que trace(A B C ) trace(A C B ) On peut commuter deux matrices , mais on ne ***peux*** pas changer ne importe quel ordre ! Démonstration En exercice

**22395**: La trace est invariante par commutation de deux matrices a : Mais il est faux de penser que trace(A B C ) trace(A C B ) On peut commuter deux matrices , mais on ne peux pas changer ne ***importe*** quel ordre ! Démonstration En exercice

**22406**: Matrices inversibles Soit ***a*** A Mp ( K )

**22414**: On ***dit*** que A est inversible si : On dit alors que B est l' inverse de A et on note GLp ( K ) l' ensemble des matrices inversibles de Mp ( K ) Il est indispensable que A soit une matrice carrée ! Si il existe , l' inverse est unique

**22417**: On dit que A ***est*** inversible si : On dit alors que B est l' inverse de A et on note GLp ( K ) l' ensemble des matrices inversibles de Mp ( K ) Il est indispensable que A soit une matrice carrée ! Si il existe , l' inverse est unique

**22422**: On dit que A est inversible si : On ***dit*** alors que B est l' inverse de A et on note GLp ( K ) l' ensemble des matrices inversibles de Mp ( K ) Il est indispensable que A soit une matrice carrée ! Si il existe , l' inverse est unique

**22426**: On dit que A est inversible si : On dit alors que B ***est*** l' inverse de A et on note GLp ( K ) l' ensemble des matrices inversibles de Mp ( K ) Il est indispensable que A soit une matrice carrée ! Si il existe , l' inverse est unique

**22449**: On dit que A est inversible si : On dit alors que B est l' inverse de A et on note GLp ( K ) l' ensemble des matrices inversibles de Mp ( K ) Il ***est*** indispensable que A soit une matrice carrée ! Si il existe , l' inverse est unique

**22460**: On dit que A est inversible si : On dit alors que B est l' inverse de A et on note GLp ( K ) l' ensemble des matrices inversibles de Mp ( K ) Il est indispensable que A soit une matrice carrée ! Si il ***existe*** , l' inverse est unique

**22464**: On dit que A est inversible si : On dit alors que B est l' inverse de A et on note GLp ( K ) l' ensemble des matrices inversibles de Mp ( K ) Il est indispensable que A soit une matrice carrée ! Si il existe , l' inverse ***est*** unique

**22475**: En effet , si B et B 0 ***sont*** deux inverses de A , on a : Soit A Dp ( K ) une matrice diagonale : Alors A est inversible si , et seulement si , pour tout k 1 , p , k 6 0

**22482**: En effet , si B et B 0 sont deux inverses de A , on ***a*** : Soit A Dp ( K ) une matrice diagonale : Alors A est inversible si , et seulement si , pour tout k 1 , p , k 6 0

**22496**: En effet , si B et B 0 sont deux inverses de A , on a : Soit A Dp ( K ) une matrice diagonale : Alors A ***est*** inversible si , et seulement si , pour tout k 1 , p , k 6 0

**22517**: Si c' ***est*** le cas , on a Proposition 2.3 Lien entre automorphismes et matrices inversibles Soit E un K - espace vectoriel de dimension finie , soit E une base de E et soit f L ( E )

**22522**: Si c' est le cas , on ***a*** Proposition 2.3 Lien entre automorphismes et matrices inversibles Soit E un K - espace vectoriel de dimension finie , soit E une base de E et soit f L ( E )

**22558**: Alors f ***est*** inversible si , et seulement si , la matrice MatE ( f ) est inversible

**22572**: Alors f est inversible si , et seulement si , la matrice MatE ( f ) ***est*** inversible

**22577**: Si c' ***est*** le cas : Démonstration Supposons f inversible

**22582**: Si c' est le cas : Démonstration ***Supposons*** f inversible

**22587**: On ***a*** ce qui démontre que MatE ( f ) est inversible et MatE ( f 1 ) MatE ( f ) 1

**22590**: On a ce qui ***démontre*** que MatE ( f ) est inversible et MatE ( f 1 ) MatE ( f ) 1

**22596**: On a ce qui démontre que MatE ( f ) ***est*** inversible et MatE ( f 1 ) MatE ( f ) 1

**22610**: ***Supposons*** MatE ( f ) inversible

**22627**: D' après la proposition 2.1 , page 96 , il ***existe*** un unique g L ( E ) tel MatE ( f ) MatE ( g ) MatE ( g ) MatE ( f ) Ip ce qui démontre que f est inversible

**22655**: D' après la proposition 2.1 , page 96 , il existe un unique g L ( E ) tel MatE ( f ) MatE ( g ) MatE ( g ) MatE ( f ) Ip ce qui ***démontre*** que f est inversible

**22658**: D' après la proposition 2.1 , page 96 , il existe un unique g L ( E ) tel MatE ( f ) MatE ( g ) MatE ( g ) MatE ( f ) Ip ce qui démontre que f ***est*** inversible

**22666**: Ainsi , les matrices inversibles ***représentent*** les automorphismes

**22674**: À noter qu' il ***suffit*** de vérifier l' inversibilité matriciel dans une seule base

**22707**: Alors : Démonstration La relation A A1 A1 A ***démontre*** que A1 est inversible et que son inverse est A. et de même ( B 1 A1 ) ( A B ) Ip , d' où le résultat

**22710**: Alors : Démonstration La relation A A1 A1 A démontre que A1 ***est*** inversible et que son inverse est A. et de même ( B 1 A1 ) ( A B ) Ip , d' où le résultat

**22716**: Alors : Démonstration La relation A A1 A1 A démontre que A1 est inversible et que son inverse ***est*** A. et de même ( B 1 A1 ) ( A B ) Ip , d' où le résultat

**22765**: Si A et B ***sont*** inversibles , il est faux en général que A B est inversible

**22769**: Si A et B sont inversibles , il ***est*** faux en général que A B est inversible

**22776**: Si A et B sont inversibles , il est faux en général que A B ***est*** inversible

**22785**: Par exemple A A 0p ne ***est*** jamais inversible

**22797**: En particulier , GLp ( K ) ne ***est*** pas un sous-espace vectoriel de Mp ( K )

**22809**: Il ***est*** cependant stable par Changement de bases K(matrice de passage ) Soit E un K - espace vectoriel de dimension finie et E ( e1 ,

**22871**: , xr ) une famille de r vecteurs de E. On ***appelle*** matrice de la famille X ( x1 ,

**22892**: , xr ) dans la base E et on ***note*** : MatE ( X ) MatE où ai , j est le i - ième coefficient de xj dans la base E : Autrement dit , par dualité , pour tout ( i , j ) 1 , n 1 , r , ai , j e?i ( xj )

**22900**: , xr ) dans la base E et on note : MatE ( X ) MatE où ***ai*** , j est le i - ième coefficient de xj dans la base E : Autrement dit , par dualité , pour tout ( i , j ) 1 , n 1 , r , ai , j e?i ( xj )

**22903**: , xr ) dans la base E et on note : MatE ( X ) MatE où ai , j ***est*** le i - ième coefficient de xj dans la base E : Autrement dit , par dualité , pour tout ( i , j ) 1 , n 1 , r , ai , j e?i ( xj )

**22917**: , xr ) dans la base E et on note : MatE ( X ) MatE où ai , j est le i - ième coefficient de xj dans la base E : Autrement ***dit*** , par dualité , pour tout ( i , j ) 1 , n 1 , r , ai , j e?i ( xj )

**22948**: Si E ***est*** un K - espace vectoriel de dimension finie , E une base de E et x E , alors : MatE ( x ) MatE ( x ) 2

**22984**: Si E et E 0 ***sont*** des K - espaces vectoriels de dimension finie , E ( e1 ,

**23054**: On ***appelle*** matrice de passage de E à B et on note : C' est donc la matrice de la nouvelle base B exprimée dans l' ancienne base E

**23064**: On appelle matrice de passage de E à B et on ***note*** : C' est donc la matrice de la nouvelle base B exprimée dans l' ancienne base E

**23067**: On appelle matrice de passage de E à B et on note : C' ***est*** donc la matrice de la nouvelle base B exprimée dans l' ancienne base E

**23084**: On ***a*** ( attention à l' ordre des bases ! ) : PE MatB , E ( idE ) Soit E un K - espace vectoriel de dimension finie , E et B deux bases de E. Alors la matrice de passage PE est inversible et Démonstration PB MatB , E ( idE ) MatE , B ( idE ) MatB , B ( idE idE ) MatB , B ( idE ) Ip et de même PB PE Ip , d' où le résultat

**23126**: On a ( attention à l' ordre des bases ! ) : PE MatB , E ( idE ) Soit E un K - espace vectoriel de dimension finie , E et B deux bases de E. Alors la matrice de passage PE ***est*** inversible et Démonstration PB MatB , E ( idE ) MatE , B ( idE ) MatB , B ( idE idE ) MatB , B ( idE ) Ip et de même PB PE Ip , d' où le résultat

**23212**: Si E ***est*** un K - espace vectoriel de dimension finie avec p dim E , si E est une base de E , alors une famille X ( x1 ,

**23228**: Si E est un K - espace vectoriel de dimension finie avec p dim E , si E ***est*** une base de E , alors une famille X ( x1 ,

**23252**: , xp ) de p vecteurs de E ***est*** une base de E si , et seulement si , MatE ( X ) est inversible

**23267**: , xp ) de p vecteurs de E est une base de E si , et seulement si , MatE ( X ) ***est*** inversible

**23272**: Si c' ***est*** le cas , MatE ( X ) PE

**23319**: Proposition 2.4 Changement de base pour les vecteurs Soit E un K - espace vectoriel de dimension finie , E et B deux bases de E , x E. Alors : MatE ( x ) PE Autrement ***dit*** , en multipliant à gauche par PE , on obtient les anciennes coordonnées en fonction des nouvelles coordonnées

**23329**: Proposition 2.4 Changement de base pour les vecteurs Soit E un K - espace vectoriel de dimension finie , E et B deux bases de E , x E. Alors : MatE ( x ) PE Autrement dit , en multipliant à gauche par PE , on ***obtient*** les anciennes coordonnées en fonction des nouvelles coordonnées

**23431**: Alors ( voir la figure 2.2 , page suivante ) : Dans le cas particulier où f ***est*** un endomorphisme de E ( E 0 E , E 0 E , B 0 B et f L ( E ) ) , on a MatB ( f ) PE Démonstration L' égalité f idE idE 0 f donne c' est - à - dire d' où le résultat

**23457**: Alors ( voir la figure 2.2 , page suivante ) : Dans le cas particulier où f est un endomorphisme de E ( E 0 E , E 0 E , B 0 B et f L ( E ) ) , on ***a*** MatB ( f ) PE Démonstration L' égalité f idE idE 0 f donne c' est - à - dire d' où le résultat

**23471**: Alors ( voir la figure 2.2 , page suivante ) : Dans le cas particulier où f est un endomorphisme de E ( E 0 E , E 0 E , B 0 B et f L ( E ) ) , on a MatB ( f ) PE Démonstration L' égalité f idE idE 0 f ***donne*** c' est - à - dire d' où le résultat

**23496**: Figure 2.2 Changement de base pour les applications linéaires Le noyau de A ***est*** le sous-espace vectoriel de Mp,1 ( K ) défini par : L' image de A est le sous-espace vectoriel de Mn,1 ( K ) défini par : Le rang de A , noté rang(A ) , est la dimension de Im(A ) : rang(A ) dim Im(A ) a. On notera aussi Ker A , Im A et rang A. Soit A Mn , p ( K )

**23512**: Figure 2.2 Changement de base pour les applications linéaires Le noyau de A est le sous-espace vectoriel de Mp,1 ( K ) défini par : L' image de A ***est*** le sous-espace vectoriel de Mn,1 ( K ) défini par : Le rang de A , noté rang(A ) , est la dimension de Im(A ) : rang(A ) dim Im(A ) a. On notera aussi Ker A , Im A et rang A. Soit A Mn , p ( K )

**23533**: Figure 2.2 Changement de base pour les applications linéaires Le noyau de A est le sous-espace vectoriel de Mp,1 ( K ) défini par : L' image de A est le sous-espace vectoriel de Mn,1 ( K ) défini par : Le rang de A , noté rang(A ) , ***est*** la dimension de Im(A ) : rang(A ) dim Im(A ) a. On notera aussi Ker A , Im A et rang A. Soit A Mn , p ( K )

**23545**: Figure 2.2 Changement de base pour les applications linéaires Le noyau de A est le sous-espace vectoriel de Mp,1 ( K ) défini par : L' image de A est le sous-espace vectoriel de Mn,1 ( K ) défini par : Le rang de A , noté rang(A ) , est la dimension de Im(A ) : rang(A ) dim Im(A ) ***a.*** On notera aussi Ker A , Im A et rang A. Soit A Mn , p ( K )

**23597**: , Cp les colonnes de A ( vues comme des matrices colonnes de En particulier , rang A min(n , p ) car Im ***A*** est un sous-espace vectoriel de Mn,1 ( K ) engendré par les p colonnes de A. Soit E et E 0 deux K - espaces vectoriels de dimension finie , soit E une base de E , soit E 0 une base de E 0 1

**23598**: , Cp les colonnes de A ( vues comme des matrices colonnes de En particulier , rang A min(n , p ) car Im A ***est*** un sous-espace vectoriel de Mn,1 ( K ) engendré par les p colonnes de A. Soit E et E 0 deux K - espaces vectoriels de dimension finie , soit E une base de E , soit E 0 une base de E 0 1

**23718**: Pour tout y E 0 , y Im f si , et seulement si , MatE 0 ( y ) Im A. En particulier , dim Im f dim Im A c' est - à - dire rang f rang A Si ***A*** Mn , p ( K ) , on a le théorème du rang ( voir le théorème 1.4 , page 57 ) : rang A p dim Ker A De plus , si A Mp ( K ) ( matrice carrée ) , les propositions suivantes sont équivalentes : 1

**23727**: Pour tout y E 0 , y Im f si , et seulement si , MatE 0 ( y ) Im A. En particulier , dim Im f dim Im A c' est - à - dire rang f rang A Si A Mn , p ( K ) , on ***a*** le théorème du rang ( voir le théorème 1.4 , page 57 ) : rang A p dim Ker A De plus , si A Mp ( K ) ( matrice carrée ) , les propositions suivantes sont équivalentes : 1

**23752**: Pour tout y E 0 , y Im f si , et seulement si , MatE 0 ( y ) Im A. En particulier , dim Im f dim Im A c' est - à - dire rang f rang A Si A Mn , p ( K ) , on a le théorème du rang ( voir le théorème 1.4 , page 57 ) : rang A p dim Ker A De plus , si ***A*** Mp ( K ) ( matrice carrée ) , les propositions suivantes sont équivalentes : 1

**23765**: Pour tout y E 0 , y Im f si , et seulement si , MatE 0 ( y ) Im A. En particulier , dim Im f dim Im A c' est - à - dire rang f rang A Si A Mn , p ( K ) , on a le théorème du rang ( voir le théorème 1.4 , page 57 ) : rang A p dim Ker A De plus , si A Mp ( K ) ( matrice carrée ) , les propositions suivantes ***sont*** équivalentes : 1

**23771**: A ***est*** inversible 4

**23787**: , Cp ) de ***A*** forment une base de Mp,1 ( K )

**23788**: , Cp ) de A ***forment*** une base de Mp,1 ( K )

**23803**: La multiplication par une matrice inversible ***conserve*** le rang : si A Mn , p ( K ) , alors P GLp ( K ) , Q GLn ( K ) , rang(A P ) rang(Q A ) rang A 2.4.1 Soit A Mn ( K ) , existe -t -il B Mn ( K ) telle que : 2.4.2 Soit A et B dans Mn ( R ) , telles que : Démontrer que A ou B est la matrice nulle

**23808**: La multiplication par une matrice inversible conserve le rang : si ***A*** Mn , p ( K ) , alors P GLp ( K ) , Q GLn ( K ) , rang(A P ) rang(Q A ) rang A 2.4.1 Soit A Mn ( K ) , existe -t -il B Mn ( K ) telle que : 2.4.2 Soit A et B dans Mn ( R ) , telles que : Démontrer que A ou B est la matrice nulle

**23845**: La multiplication par une matrice inversible conserve le rang : si A Mn , p ( K ) , alors P GLp ( K ) , Q GLn ( K ) , rang(A P ) rang(Q A ) rang A 2.4.1 Soit A Mn ( K ) , ***existe*** -t -il B Mn ( K ) telle que : 2.4.2 Soit A et B dans Mn ( R ) , telles que : Démontrer que A ou B est la matrice nulle

**23875**: La multiplication par une matrice inversible conserve le rang : si A Mn , p ( K ) , alors P GLp ( K ) , Q GLn ( K ) , rang(A P ) rang(Q A ) rang A 2.4.1 Soit A Mn ( K ) , existe -t -il B Mn ( K ) telle que : 2.4.2 Soit A et B dans Mn ( R ) , telles que : Démontrer que A ou B ***est*** la matrice nulle

**23890**: 2.4.3 Démontrer que toute matrice de Mn ( R ) ***est*** somme de deux matrices inversibles

**23907**: 2.4.4 Démontrer que toute matrice de Mn ( R ) ***est*** limite d' une suite de matrices inversibles

**23954**: 2.4.5 Soit A et B dans Mn , p ( K ) , démontrer que : rang(A ) rang(B ) Relations d' équivalence et matrices Relations d' équivalence Soit E un ensemble non vide , R une relation ***a*** sur E. On dit que : R est réflexive si : R est symétrique si : R est transitive si : On dit que R est une relation d' équivalence sur E si R est réflexive , symétrique et transitive

**23958**: 2.4.5 Soit A et B dans Mn , p ( K ) , démontrer que : rang(A ) rang(B ) Relations d' équivalence et matrices Relations d' équivalence Soit E un ensemble non vide , R une relation a sur E. On ***dit*** que : R est réflexive si : R est symétrique si : R est transitive si : On dit que R est une relation d' équivalence sur E si R est réflexive , symétrique et transitive

**23962**: 2.4.5 Soit A et B dans Mn , p ( K ) , démontrer que : rang(A ) rang(B ) Relations d' équivalence et matrices Relations d' équivalence Soit E un ensemble non vide , R une relation a sur E. On dit que : R ***est*** réflexive si : R est symétrique si : R est transitive si : On dit que R est une relation d' équivalence sur E si R est réflexive , symétrique et transitive

**23967**: 2.4.5 Soit A et B dans Mn , p ( K ) , démontrer que : rang(A ) rang(B ) Relations d' équivalence et matrices Relations d' équivalence Soit E un ensemble non vide , R une relation a sur E. On dit que : R est réflexive si : R ***est*** symétrique si : R est transitive si : On dit que R est une relation d' équivalence sur E si R est réflexive , symétrique et transitive

**23972**: 2.4.5 Soit A et B dans Mn , p ( K ) , démontrer que : rang(A ) rang(B ) Relations d' équivalence et matrices Relations d' équivalence Soit E un ensemble non vide , R une relation a sur E. On dit que : R est réflexive si : R est symétrique si : R ***est*** transitive si : On dit que R est une relation d' équivalence sur E si R est réflexive , symétrique et transitive

**23977**: 2.4.5 Soit A et B dans Mn , p ( K ) , démontrer que : rang(A ) rang(B ) Relations d' équivalence et matrices Relations d' équivalence Soit E un ensemble non vide , R une relation a sur E. On dit que : R est réflexive si : R est symétrique si : R est transitive si : On ***dit*** que R est une relation d' équivalence sur E si R est réflexive , symétrique et transitive

**23980**: 2.4.5 Soit A et B dans Mn , p ( K ) , démontrer que : rang(A ) rang(B ) Relations d' équivalence et matrices Relations d' équivalence Soit E un ensemble non vide , R une relation a sur E. On dit que : R est réflexive si : R est symétrique si : R est transitive si : On dit que R ***est*** une relation d' équivalence sur E si R est réflexive , symétrique et transitive

**23989**: 2.4.5 Soit A et B dans Mn , p ( K ) , démontrer que : rang(A ) rang(B ) Relations d' équivalence et matrices Relations d' équivalence Soit E un ensemble non vide , R une relation a sur E. On dit que : R est réflexive si : R est symétrique si : R est transitive si : On dit que R est une relation d' équivalence sur E si R ***est*** réflexive , symétrique et transitive

**23996**: ***a.*** c' est - à - dire la donnée d' un sous-ensemble A de E E , où on définit pour tout ( x , y ) E 2 , xRy ( x , y ) A 1

**24015**: a. c' est - à - dire la donnée d' un sous-ensemble A de E E , où on ***définit*** pour tout ( x , y ) E 2 , xRy ( x , y ) A 1

**24040**: L' égalité sur un ensemble ***est*** toujours une relation d' équivalence ! 2

**24052**: avoir même parité ***est*** une relation d' équivalence sur N ou Z. 3

**24066**: avoir même dimension ***est*** une relation d' équivalence sur l' ensemble des sous-espaces vectoriels d' un même espace vectoriel E de dimension finie

**24093**: être en bijection avec ***est*** une relation d' équivalence sur l' ensemble des sous-ensembles d' un ensemble E. 5

**24111**: être parallèle ***est*** une relation d' équivalence sur l' ensemble des droites du plan

**24134**: Pour tout n N , avoir le même ***reste*** dans la division euclidienne par n est une relation d' équivalence sur Z. Soit E un ensemble non vide , on appelle partition de E , la donnée d' une famille ( Ei ) iI de sous-ensembles de E tels que : tous les Ei sont non vides : ils sont disjoints deux-à-deux : ils recouvrent E : 1

**24141**: Pour tout n N , avoir le même reste dans la division euclidienne par n ***est*** une relation d' équivalence sur Z. Soit E un ensemble non vide , on appelle partition de E , la donnée d' une famille ( Ei ) iI de sous-ensembles de E tels que : tous les Ei sont non vides : ils sont disjoints deux-à-deux : ils recouvrent E : 1

**24156**: Pour tout n N , avoir le même reste dans la division euclidienne par n est une relation d' équivalence sur Z. Soit E un ensemble non vide , on ***appelle*** partition de E , la donnée d' une famille ( Ei ) iI de sous-ensembles de E tels que : tous les Ei sont non vides : ils sont disjoints deux-à-deux : ils recouvrent E : 1

**24180**: Pour tout n N , avoir le même reste dans la division euclidienne par n est une relation d' équivalence sur Z. Soit E un ensemble non vide , on appelle partition de E , la donnée d' une famille ( Ei ) iI de sous-ensembles de E tels que : tous les Ei ***sont*** non vides : ils sont disjoints deux-à-deux : ils recouvrent E : 1

**24185**: Pour tout n N , avoir le même reste dans la division euclidienne par n est une relation d' équivalence sur Z. Soit E un ensemble non vide , on appelle partition de E , la donnée d' une famille ( Ei ) iI de sous-ensembles de E tels que : tous les Ei sont non vides : ils ***sont*** disjoints deux-à-deux : ils recouvrent E : 1

**24190**: Pour tout n N , avoir le même reste dans la division euclidienne par n est une relation d' équivalence sur Z. Soit E un ensemble non vide , on appelle partition de E , la donnée d' une famille ( Ei ) iI de sous-ensembles de E tels que : tous les Ei sont non vides : ils sont disjoints deux-à-deux : ils ***recouvrent*** E : 1

**24202**: L' ensemble des singletons d' un ensemble ***est*** une partition de cet ensemble

**24236**: L' ensemble 2 N 2 n , n N des nombres pairs et l' ensemble 12 N 2 n1 , n N des nombres impairs ***forment*** une partition de N. 3

**24254**: Plus généralement , si n N , n 2 , on ***peut*** s' intéresser aux ensembles : ( Ek ) k0,n1 est une partition de N Soit ( Ei ) iI une partition d' un ensemble E non vide et soit R la relation sur E définie par : ( autrement dit , x et y sont en relation lorsque x et y appartiennent à un même Ei )

**24264**: Plus généralement , si n N , n 2 , on peut s' intéresser aux ensembles : ( Ek ) k0,n1 ***est*** une partition de N Soit ( Ei ) iI une partition d' un ensemble E non vide et soit R la relation sur E définie par : ( autrement dit , x et y sont en relation lorsque x et y appartiennent à un même Ei )

**24294**: Plus généralement , si n N , n 2 , on peut s' intéresser aux ensembles : ( Ek ) k0,n1 est une partition de N Soit ( Ei ) iI une partition d' un ensemble E non vide et soit R la relation sur E définie par : ( autrement ***dit*** , x et y sont en relation lorsque x et y appartiennent à un même Ei )

**24299**: Plus généralement , si n N , n 2 , on peut s' intéresser aux ensembles : ( Ek ) k0,n1 est une partition de N Soit ( Ei ) iI une partition d' un ensemble E non vide et soit R la relation sur E définie par : ( autrement dit , x et y ***sont*** en relation lorsque x et y appartiennent à un même Ei )

**24306**: Plus généralement , si n N , n 2 , on peut s' intéresser aux ensembles : ( Ek ) k0,n1 est une partition de N Soit ( Ei ) iI une partition d' un ensemble E non vide et soit R la relation sur E définie par : ( autrement dit , x et y sont en relation lorsque x et y ***appartiennent*** à un même Ei )

**24315**: Alors R ***est*** une relation d' équivalence sur E. Démonstration C' est immédiat , une fois qu' on a remarqué que pour tout x E , il existe i I tel que x Ei car la partition ( Ei ) iI recouvre E. Soit R une relation d' équivalence sur un ensemble E non vide et soit x E. La classe d' équivalence de x modulo R est le sous-ensemble de E défini par : Classe(x , R ) y E , xRy Un élément d' une classe d' équivalence est appelé un représentant de cette classe d' équivalence

**24324**: Alors R est une relation d' équivalence sur E. Démonstration C' ***est*** immédiat , une fois qu' on a remarqué que pour tout x E , il existe i I tel que x Ei car la partition ( Ei ) iI recouvre E. Soit R une relation d' équivalence sur un ensemble E non vide et soit x E. La classe d' équivalence de x modulo R est le sous-ensemble de E défini par : Classe(x , R ) y E , xRy Un élément d' une classe d' équivalence est appelé un représentant de cette classe d' équivalence

**24340**: Alors R est une relation d' équivalence sur E. Démonstration C' est immédiat , une fois qu' on a remarqué que pour tout x E , il ***existe*** i I tel que x Ei car la partition ( Ei ) iI recouvre E. Soit R une relation d' équivalence sur un ensemble E non vide et soit x E. La classe d' équivalence de x modulo R est le sous-ensemble de E défini par : Classe(x , R ) y E , xRy Un élément d' une classe d' équivalence est appelé un représentant de cette classe d' équivalence

**24354**: Alors R est une relation d' équivalence sur E. Démonstration C' est immédiat , une fois qu' on a remarqué que pour tout x E , il existe i I tel que x Ei car la partition ( Ei ) iI ***recouvre*** E. Soit R une relation d' équivalence sur un ensemble E non vide et soit x E. La classe d' équivalence de x modulo R est le sous-ensemble de E défini par : Classe(x , R ) y E , xRy Un élément d' une classe d' équivalence est appelé un représentant de cette classe d' équivalence

**24380**: Alors R est une relation d' équivalence sur E. Démonstration C' est immédiat , une fois qu' on a remarqué que pour tout x E , il existe i I tel que x Ei car la partition ( Ei ) iI recouvre E. Soit R une relation d' équivalence sur un ensemble E non vide et soit x E. La classe d' équivalence de x modulo R ***est*** le sous-ensemble de E défini par : Classe(x , R ) y E , xRy Un élément d' une classe d' équivalence est appelé un représentant de cette classe d' équivalence

**24403**: Alors R est une relation d' équivalence sur E. Démonstration C' est immédiat , une fois qu' on a remarqué que pour tout x E , il existe i I tel que x Ei car la partition ( Ei ) iI recouvre E. Soit R une relation d' équivalence sur un ensemble E non vide et soit x E. La classe d' équivalence de x modulo R est le sous-ensemble de E défini par : Classe(x , R ) y E , xRy Un élément d' une classe d' équivalence ***est*** appelé un représentant de cette classe d' équivalence

**24419**: Pour tout x E , on ***a*** toujours x Classe(x , R ) car xRx

**24437**: En particulier , une classe d' équivalence ne ***est*** jamais vide

**24468**: On ***a*** donc xRz d' où zRx

**24496**: On ***a*** donc Classe(x , R ) Classe(y , R )

**24511**: De même , on ***démontre*** que Classe(x , R ) Classe(y , R ) , donc Classe(x , R ) Classe(y , R )

**24550**: Comme y Classe(y , R ) , on ***a*** y Classe(x , R ) donc xRy

**24577**: Alors les classes d' équivalence ***forment*** une partition de E. Plus formellement , si on définit la famille ( Ei ) iI des classes d' équivalence , donnée par I Classe(x , R ) , x E et pour tout i I , Ei i alors ( Ei ) iI est une partition de E. Démonstration On a vu que pour tout i I , Ei est non vide

**24587**: Alors les classes d' équivalence forment une partition de E. Plus formellement , si on ***définit*** la famille ( Ei ) iI des classes d' équivalence , donnée par I Classe(x , R ) , x E et pour tout i I , Ei i alors ( Ei ) iI est une partition de E. Démonstration On a vu que pour tout i I , Ei est non vide

**24622**: Alors les classes d' équivalence forment une partition de E. Plus formellement , si on définit la famille ( Ei ) iI des classes d' équivalence , donnée par I Classe(x , R ) , x E et pour tout i I , Ei i alors ( Ei ) iI ***est*** une partition de E. Démonstration On a vu que pour tout i I , Ei est non vide

**24638**: Alors les classes d' équivalence forment une partition de E. Plus formellement , si on définit la famille ( Ei ) iI des classes d' équivalence , donnée par I Classe(x , R ) , x E et pour tout i I , Ei i alors ( Ei ) iI est une partition de E. Démonstration On a vu que pour tout i I , Ei ***est*** non vide

**24643**: ( ***remarque*** 2.15 , page précédente )

**24656**: Pour tout x E , on ***a*** x Classe(x , R ) donc il existe i E tel que x Ei

**24664**: Pour tout x E , on a x Classe(x , R ) donc il ***existe*** i E tel que x Ei

**24674**: On en ***déduit*** que ( Ei ) iI recouvre E Soit ( i , j ) I 2 tel que i 6 j. Supposons que Ei Ej 6

**24680**: On en déduit que ( Ei ) iI ***recouvre*** E Soit ( i , j ) I 2 tel que i 6 j. Supposons que Ei Ej 6

**24695**: On en déduit que ( Ei ) iI recouvre E Soit ( i , j ) I 2 tel que i 6 j. ***Supposons*** que Ei Ej 6

**24702**: Il ***existe*** donc z Ei Ej

**24713**: Comme z Ei , on ***a*** xi Rz où xi est un représentant de E1 , donc Classe(z , R ) Ei ( propriété 2.10 , page précédente )

**24718**: Comme z Ei , on a xi Rz où xi ***est*** un représentant de E1 , donc Classe(z , R ) Ei ( propriété 2.10 , page précédente )

**24742**: De même , on ***a*** Classe(z , R ) Ej

**24750**: On ***a*** donc Ei Ej , ce qui contredit i 6 j donc Ei Ej

**24757**: On a donc Ei Ej , ce qui ***contredit*** i 6 j donc Ei Ej

**24767**: 2.5.1 On ***définit*** sur C la relation : ( a ) Démontrer que c' est une relation d' équivalence

**24774**: 2.5.1 On définit sur C la relation : ( ***a*** ) Démontrer que c' est une relation d' équivalence

**24779**: 2.5.1 On définit sur C la relation : ( a ) Démontrer que c' ***est*** une relation d' équivalence

**24821**: 2.5.2 Soit E un ensemble non vide et A E , on ***définit*** une relation sur P(E ) par : ( a ) Démontrer que c' est une relation d' équivalence

**24830**: 2.5.2 Soit E un ensemble non vide et A E , on définit une relation sur P(E ) par : ( ***a*** ) Démontrer que c' est une relation d' équivalence

**24835**: 2.5.2 Soit E un ensemble non vide et A E , on définit une relation sur P(E ) par : ( a ) Démontrer que c' ***est*** une relation d' équivalence

**24852**: ( b ) Démontrer que l' ensemble des classes d' équivalence ***est*** en bijection avec P(A )

**24860**: On ***définit*** de même la relation sur P(E ) par : ( c ) C' est une relation d' équivalence , trouver une bijection de l' ensemble des classes d' équivalence avec un ensemble connu

**24874**: On définit de même la relation sur P(E ) par : ( c ) C' ***est*** une relation d' équivalence , trouver une bijection de l' ensemble des classes d' équivalence avec un ensemble connu

**24904**: 2.5.3 Soit E un ensemble non vide , on ***définit*** la relation sur P(E ) par : f F ( X , Y ) , injective Cette relation est-elle réflexive ? Symétrique ? Transitive ? 2.5.4 Soit E un ensemble non vide muni d' une relation R réflexive et transitive

**24947**: On ***définit*** les deux relations suivantes : xRy ou yRx Les relations S et T sont - elles réflexives ? Symétriques ? Transitives ? 2.5.5 Soit E un ensemble non vide muni d' une relation d' équivalence R , on pose : Classe(a , R ) Soit A une partie de E. ( a ) Démontrer que A s(A )

**24961**: On définit les deux relations suivantes : xRy ou yRx Les relations S et T ***sont*** - elles réflexives ? Symétriques ? Transitives ? 2.5.5 Soit E un ensemble non vide muni d' une relation d' équivalence R , on pose : Classe(a , R ) Soit A une partie de E. ( a ) Démontrer que A s(A )

**24986**: On définit les deux relations suivantes : xRy ou yRx Les relations S et T sont - elles réflexives ? Symétriques ? Transitives ? 2.5.5 Soit E un ensemble non vide muni d' une relation d' équivalence R , on ***pose*** : Classe(a , R ) Soit A une partie de E. ( a ) Démontrer que A s(A )

**24999**: On définit les deux relations suivantes : xRy ou yRx Les relations S et T sont - elles réflexives ? Symétriques ? Transitives ? 2.5.5 Soit E un ensemble non vide muni d' une relation d' équivalence R , on pose : Classe(a , R ) Soit A une partie de E. ( ***a*** ) Démontrer que A s(A )

**25061**: Deux matrices M et N de Mn , p ( K ) ***sont*** dites équivalentes si : Si c' est le cas , on note Cela définit une relation sur Mn , p ( K ) appelée équivalence

**25062**: Deux matrices M et N de Mn , p ( K ) sont ***dites*** équivalentes si : Si c' est le cas , on note Cela définit une relation sur Mn , p ( K ) appelée équivalence

**25068**: Deux matrices M et N de Mn , p ( K ) sont dites équivalentes si : Si c' ***est*** le cas , on note Cela définit une relation sur Mn , p ( K ) appelée équivalence

**25075**: Deux matrices M et N de Mn , p ( K ) sont dites équivalentes si : Si c' est le cas , on note Cela ***définit*** une relation sur Mn , p ( K ) appelée équivalence

**25100**: Deux matrices M et N de Mp ( K ) ***sont*** dites semblables si : Si c' est le cas , on note Cela définit une relation sur Mp ( K ) appelée similitude

**25101**: Deux matrices M et N de Mp ( K ) sont ***dites*** semblables si : Si c' est le cas , on note Cela définit une relation sur Mp ( K ) appelée similitude

**25107**: Deux matrices M et N de Mp ( K ) sont dites semblables si : Si c' ***est*** le cas , on note Cela définit une relation sur Mp ( K ) appelée similitude

**25114**: Deux matrices M et N de Mp ( K ) sont dites semblables si : Si c' est le cas , on note Cela ***définit*** une relation sur Mp ( K ) appelée similitude

**25150**: D' après la formule de changement de base ( proposition 2.5 , page 111 ) : deux matrices de Mn , p ( K ) ***sont*** équivalentes si , et seulement si , elles représentent la même application deux matrices de Mp ( K ) sont semblables si , et seulement si , elles représentent le même endomorphisme

**25159**: D' après la formule de changement de base ( proposition 2.5 , page 111 ) : deux matrices de Mn , p ( K ) sont équivalentes si , et seulement si , elles ***représentent*** la même application deux matrices de Mp ( K ) sont semblables si , et seulement si , elles représentent le même endomorphisme

**25170**: D' après la formule de changement de base ( proposition 2.5 , page 111 ) : deux matrices de Mn , p ( K ) sont équivalentes si , et seulement si , elles représentent la même application deux matrices de Mp ( K ) ***sont*** semblables si , et seulement si , elles représentent le même endomorphisme

**25179**: D' après la formule de changement de base ( proposition 2.5 , page 111 ) : deux matrices de Mn , p ( K ) sont équivalentes si , et seulement si , elles représentent la même application deux matrices de Mp ( K ) sont semblables si , et seulement si , elles ***représentent*** le même endomorphisme

**25188**: L' équivalence ***est*** une relation d' équivalence sur Mn , p ( K )

**25205**: La similitude ***est*** une relation d' équivalence sur Mp ( K )

**25273**: Notation 2.2 Nous noterons Jn , p , r la matrice de Mn , p ( K ) définie par : Proposition 2.6 Caractérisation des matrices équivalentes Deux matrices de Mn , p ( K ) ***sont*** équivalentes si , et seulement si , elles ont même rang

**25282**: Notation 2.2 Nous noterons Jn , p , r la matrice de Mn , p ( K ) définie par : Proposition 2.6 Caractérisation des matrices équivalentes Deux matrices de Mn , p ( K ) sont équivalentes si , et seulement si , elles ***ont*** même rang

**25307**: Supposons - les équivalentes , il ***existe*** P GLn ( K ) et Q GLp ( K ) telles que N P M Q. En notant uX l' application linéaire canoniquement associée à une matrice X , on a uN uP uM uQ La conservation du rang en découle , car uP et uQ sont inversibles ( voir la remarque 2.14 , page 115 )

**25339**: Supposons - les équivalentes , il existe P GLn ( K ) et Q GLp ( K ) telles que N P M Q. En notant uX l' application linéaire canoniquement associée à une matrice X , on ***a*** uN uP uM uQ La conservation du rang en découle , car uP et uQ sont inversibles ( voir la remarque 2.14 , page 115 )

**25355**: Supposons - les équivalentes , il existe P GLn ( K ) et Q GLp ( K ) telles que N P M Q. En notant uX l' application linéaire canoniquement associée à une matrice X , on a uN uP uM uQ La conservation du rang en découle , car uP et uQ ***sont*** inversibles ( voir la remarque 2.14 , page 115 )

**25367**: ***Supposons*** que rang N rang M

**25374**: ***Posons*** r rang(M ) rang(N )

**25386**: La factorisation de uM nous ***donne*** l' existence d' un supplémentaire E1 de Ker(uM ) isomorphe à Im(uM )

**25438**: , ep ) de Ker(uM ) ce qui nous ***donne*** une base E ( e1 ,

**25470**: , uM ( er ) ) ***est*** une partie libre de Kn que l' on complète en une base E 0 de Kn

**25489**: On ***a*** alors , par construction des bases E et E 0 : De même , on démontre que N Jn , p , r

**25505**: On a alors , par construction des bases E et E 0 : De même , on ***démontre*** que N Jn , p , r

**25515**: Puisque ***est*** une relation d' équivalence , on a donc M N

**25522**: Puisque est une relation d' équivalence , on ***a*** donc M N

**25534**: On a donc démontré qu' il y ***a*** min(n , p ) 1 classe d' équivalence pour la relation d' équivalence dans Mn , p ( K ) , chaque classe d' équivalence est l' ensemble des matrices de rang r 0 , min(n , p ) et un représentant de chaque classe est Jn , p , r

**25560**: On a donc démontré qu' il y a min(n , p ) 1 classe d' équivalence pour la relation d' équivalence dans Mn , p ( K ) , chaque classe d' équivalence ***est*** l' ensemble des matrices de rang r 0 , min(n , p ) et un représentant de chaque classe est Jn , p , r

**25580**: On a donc démontré qu' il y a min(n , p ) 1 classe d' équivalence pour la relation d' équivalence dans Mn , p ( K ) , chaque classe d' équivalence est l' ensemble des matrices de rang r 0 , min(n , p ) et un représentant de chaque classe ***est*** Jn , p , r

**25588**: Il ***est*** beaucoup plus difficile de caractériser les classes d' équivalence pour la similitude ( voir la partie sur les classes de similitude dans chapitre 4 sur la réduction des endormophismes )

**25621**: ( ***a*** ) Démontrer que si A B , alors rang(A ) rang(B ) et trace(A ) trace(B )

**25660**: ( b ) Trouver un exemple où rang(A ) rang(B ) et trace(A ) trace(B ) mais A et B ***sont*** pas semblables

**25674**: ( c ) Trouver un exemple où A et B ***sont*** équivalentes mais ne sont pas semblables

**25678**: ( c ) Trouver un exemple où A et B sont équivalentes mais ne ***sont*** pas semblables

**25702**: ( ***a*** ) Justifier que la trace de MatE ( f ) ne dépend pas de la base E de E choisie

**25714**: ( a ) Justifier que la trace de MatE ( f ) ne ***dépend*** pas de la base E de E choisie

**25725**: On ***définit*** alors la trace de f , notée trace(f ) , comme la valeur de trace(MatE ( f ) ) dans ne importe quelle base E de E. ( b ) Soit p un projecteur de E. Démontrer que trace(p ) rang(p )

**25747**: On définit alors la trace de f , notée trace(f ) , comme la valeur de trace(MatE ( f ) ) dans ne ***importe*** quelle base E de E. ( b ) Soit p un projecteur de E. Démontrer que trace(p ) rang(p )

**25797**: 2.6.3 Démontrer que A Mn , p ( K ) , rang(t A ) rang(A ) 2.6.4 Démontrer que toute matrice de Mp ( K ) non inversible ***est*** équivalente à une matrice B telle qu' il existe k N tel que B k 0p ( matrice dite nilpotente )

**25806**: 2.6.3 Démontrer que A Mn , p ( K ) , rang(t A ) rang(A ) 2.6.4 Démontrer que toute matrice de Mp ( K ) non inversible est équivalente à une matrice B telle qu' il ***existe*** k N tel que B k 0p ( matrice dite nilpotente )

**25844**: Systèmes linéaires Algorithme du pivot de Gauss Notation 2.3 Soit ( k , ) 1 , p , k 6 et K , on ***appelle*** matrice de transvection la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : Démonstration Un calcul direct donne Notation 2.4 Soit k 1 , p , K , on appelle matrice de dilatation la matrice de Mp ( K ) définie par : à la k - ième place Cette matrice est inversible et son inverse est : Démonstration Un calcul immédiat démontre que Notation 2.5 Soit une permutation a de 1 , p , on appelle matrice de permutation la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : a. C' est - à - dire une bijection de de 1 , p dans 1 , p. Démonstration Si et 0 sont deux permutations de 1 , p alors on remarque que : On prend alors 0 1 en remarquant que si id1,p , alors P Ip

**25860**: Systèmes linéaires Algorithme du pivot de Gauss Notation 2.3 Soit ( k , ) 1 , p , k 6 et K , on appelle matrice de transvection la matrice de Mp ( K ) définie par : Cette matrice ***est*** inversible et son inverse est : Démonstration Un calcul direct donne Notation 2.4 Soit k 1 , p , K , on appelle matrice de dilatation la matrice de Mp ( K ) définie par : à la k - ième place Cette matrice est inversible et son inverse est : Démonstration Un calcul immédiat démontre que Notation 2.5 Soit une permutation a de 1 , p , on appelle matrice de permutation la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : a. C' est - à - dire une bijection de de 1 , p dans 1 , p. Démonstration Si et 0 sont deux permutations de 1 , p alors on remarque que : On prend alors 0 1 en remarquant que si id1,p , alors P Ip

**25865**: Systèmes linéaires Algorithme du pivot de Gauss Notation 2.3 Soit ( k , ) 1 , p , k 6 et K , on appelle matrice de transvection la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse ***est*** : Démonstration Un calcul direct donne Notation 2.4 Soit k 1 , p , K , on appelle matrice de dilatation la matrice de Mp ( K ) définie par : à la k - ième place Cette matrice est inversible et son inverse est : Démonstration Un calcul immédiat démontre que Notation 2.5 Soit une permutation a de 1 , p , on appelle matrice de permutation la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : a. C' est - à - dire une bijection de de 1 , p dans 1 , p. Démonstration Si et 0 sont deux permutations de 1 , p alors on remarque que : On prend alors 0 1 en remarquant que si id1,p , alors P Ip

**25871**: Systèmes linéaires Algorithme du pivot de Gauss Notation 2.3 Soit ( k , ) 1 , p , k 6 et K , on appelle matrice de transvection la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : Démonstration Un calcul direct ***donne*** Notation 2.4 Soit k 1 , p , K , on appelle matrice de dilatation la matrice de Mp ( K ) définie par : à la k - ième place Cette matrice est inversible et son inverse est : Démonstration Un calcul immédiat démontre que Notation 2.5 Soit une permutation a de 1 , p , on appelle matrice de permutation la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : a. C' est - à - dire une bijection de de 1 , p dans 1 , p. Démonstration Si et 0 sont deux permutations de 1 , p alors on remarque que : On prend alors 0 1 en remarquant que si id1,p , alors P Ip

**25883**: Systèmes linéaires Algorithme du pivot de Gauss Notation 2.3 Soit ( k , ) 1 , p , k 6 et K , on appelle matrice de transvection la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : Démonstration Un calcul direct donne Notation 2.4 Soit k 1 , p , K , on ***appelle*** matrice de dilatation la matrice de Mp ( K ) définie par : à la k - ième place Cette matrice est inversible et son inverse est : Démonstration Un calcul immédiat démontre que Notation 2.5 Soit une permutation a de 1 , p , on appelle matrice de permutation la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : a. C' est - à - dire une bijection de de 1 , p dans 1 , p. Démonstration Si et 0 sont deux permutations de 1 , p alors on remarque que : On prend alors 0 1 en remarquant que si id1,p , alors P Ip

**25905**: Systèmes linéaires Algorithme du pivot de Gauss Notation 2.3 Soit ( k , ) 1 , p , k 6 et K , on appelle matrice de transvection la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : Démonstration Un calcul direct donne Notation 2.4 Soit k 1 , p , K , on appelle matrice de dilatation la matrice de Mp ( K ) définie par : à la k - ième place Cette matrice ***est*** inversible et son inverse est : Démonstration Un calcul immédiat démontre que Notation 2.5 Soit une permutation a de 1 , p , on appelle matrice de permutation la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : a. C' est - à - dire une bijection de de 1 , p dans 1 , p. Démonstration Si et 0 sont deux permutations de 1 , p alors on remarque que : On prend alors 0 1 en remarquant que si id1,p , alors P Ip

**25910**: Systèmes linéaires Algorithme du pivot de Gauss Notation 2.3 Soit ( k , ) 1 , p , k 6 et K , on appelle matrice de transvection la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : Démonstration Un calcul direct donne Notation 2.4 Soit k 1 , p , K , on appelle matrice de dilatation la matrice de Mp ( K ) définie par : à la k - ième place Cette matrice est inversible et son inverse ***est*** : Démonstration Un calcul immédiat démontre que Notation 2.5 Soit une permutation a de 1 , p , on appelle matrice de permutation la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : a. C' est - à - dire une bijection de de 1 , p dans 1 , p. Démonstration Si et 0 sont deux permutations de 1 , p alors on remarque que : On prend alors 0 1 en remarquant que si id1,p , alors P Ip

**25916**: Systèmes linéaires Algorithme du pivot de Gauss Notation 2.3 Soit ( k , ) 1 , p , k 6 et K , on appelle matrice de transvection la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : Démonstration Un calcul direct donne Notation 2.4 Soit k 1 , p , K , on appelle matrice de dilatation la matrice de Mp ( K ) définie par : à la k - ième place Cette matrice est inversible et son inverse est : Démonstration Un calcul immédiat ***démontre*** que Notation 2.5 Soit une permutation a de 1 , p , on appelle matrice de permutation la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : a. C' est - à - dire une bijection de de 1 , p dans 1 , p. Démonstration Si et 0 sont deux permutations de 1 , p alors on remarque que : On prend alors 0 1 en remarquant que si id1,p , alors P Ip

**25923**: Systèmes linéaires Algorithme du pivot de Gauss Notation 2.3 Soit ( k , ) 1 , p , k 6 et K , on appelle matrice de transvection la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : Démonstration Un calcul direct donne Notation 2.4 Soit k 1 , p , K , on appelle matrice de dilatation la matrice de Mp ( K ) définie par : à la k - ième place Cette matrice est inversible et son inverse est : Démonstration Un calcul immédiat démontre que Notation 2.5 Soit une permutation ***a*** de 1 , p , on appelle matrice de permutation la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : a. C' est - à - dire une bijection de de 1 , p dans 1 , p. Démonstration Si et 0 sont deux permutations de 1 , p alors on remarque que : On prend alors 0 1 en remarquant que si id1,p , alors P Ip

**25930**: Systèmes linéaires Algorithme du pivot de Gauss Notation 2.3 Soit ( k , ) 1 , p , k 6 et K , on appelle matrice de transvection la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : Démonstration Un calcul direct donne Notation 2.4 Soit k 1 , p , K , on appelle matrice de dilatation la matrice de Mp ( K ) définie par : à la k - ième place Cette matrice est inversible et son inverse est : Démonstration Un calcul immédiat démontre que Notation 2.5 Soit une permutation a de 1 , p , on ***appelle*** matrice de permutation la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : a. C' est - à - dire une bijection de de 1 , p dans 1 , p. Démonstration Si et 0 sont deux permutations de 1 , p alors on remarque que : On prend alors 0 1 en remarquant que si id1,p , alors P Ip

**25946**: Systèmes linéaires Algorithme du pivot de Gauss Notation 2.3 Soit ( k , ) 1 , p , k 6 et K , on appelle matrice de transvection la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : Démonstration Un calcul direct donne Notation 2.4 Soit k 1 , p , K , on appelle matrice de dilatation la matrice de Mp ( K ) définie par : à la k - ième place Cette matrice est inversible et son inverse est : Démonstration Un calcul immédiat démontre que Notation 2.5 Soit une permutation a de 1 , p , on appelle matrice de permutation la matrice de Mp ( K ) définie par : Cette matrice ***est*** inversible et son inverse est : a. C' est - à - dire une bijection de de 1 , p dans 1 , p. Démonstration Si et 0 sont deux permutations de 1 , p alors on remarque que : On prend alors 0 1 en remarquant que si id1,p , alors P Ip

**25951**: Systèmes linéaires Algorithme du pivot de Gauss Notation 2.3 Soit ( k , ) 1 , p , k 6 et K , on appelle matrice de transvection la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : Démonstration Un calcul direct donne Notation 2.4 Soit k 1 , p , K , on appelle matrice de dilatation la matrice de Mp ( K ) définie par : à la k - ième place Cette matrice est inversible et son inverse est : Démonstration Un calcul immédiat démontre que Notation 2.5 Soit une permutation a de 1 , p , on appelle matrice de permutation la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse ***est*** : a. C' est - à - dire une bijection de de 1 , p dans 1 , p. Démonstration Si et 0 sont deux permutations de 1 , p alors on remarque que : On prend alors 0 1 en remarquant que si id1,p , alors P Ip

**25953**: Systèmes linéaires Algorithme du pivot de Gauss Notation 2.3 Soit ( k , ) 1 , p , k 6 et K , on appelle matrice de transvection la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : Démonstration Un calcul direct donne Notation 2.4 Soit k 1 , p , K , on appelle matrice de dilatation la matrice de Mp ( K ) définie par : à la k - ième place Cette matrice est inversible et son inverse est : Démonstration Un calcul immédiat démontre que Notation 2.5 Soit une permutation a de 1 , p , on appelle matrice de permutation la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : ***a.*** C' est - à - dire une bijection de de 1 , p dans 1 , p. Démonstration Si et 0 sont deux permutations de 1 , p alors on remarque que : On prend alors 0 1 en remarquant que si id1,p , alors P Ip

**25975**: Systèmes linéaires Algorithme du pivot de Gauss Notation 2.3 Soit ( k , ) 1 , p , k 6 et K , on appelle matrice de transvection la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : Démonstration Un calcul direct donne Notation 2.4 Soit k 1 , p , K , on appelle matrice de dilatation la matrice de Mp ( K ) définie par : à la k - ième place Cette matrice est inversible et son inverse est : Démonstration Un calcul immédiat démontre que Notation 2.5 Soit une permutation a de 1 , p , on appelle matrice de permutation la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : a. C' est - à - dire une bijection de de 1 , p dans 1 , p. Démonstration Si et 0 ***sont*** deux permutations de 1 , p alors on remarque que : On prend alors 0 1 en remarquant que si id1,p , alors P Ip

**25984**: Systèmes linéaires Algorithme du pivot de Gauss Notation 2.3 Soit ( k , ) 1 , p , k 6 et K , on appelle matrice de transvection la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : Démonstration Un calcul direct donne Notation 2.4 Soit k 1 , p , K , on appelle matrice de dilatation la matrice de Mp ( K ) définie par : à la k - ième place Cette matrice est inversible et son inverse est : Démonstration Un calcul immédiat démontre que Notation 2.5 Soit une permutation a de 1 , p , on appelle matrice de permutation la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : a. C' est - à - dire une bijection de de 1 , p dans 1 , p. Démonstration Si et 0 sont deux permutations de 1 , p alors on ***remarque*** que : On prend alors 0 1 en remarquant que si id1,p , alors P Ip

**25988**: Systèmes linéaires Algorithme du pivot de Gauss Notation 2.3 Soit ( k , ) 1 , p , k 6 et K , on appelle matrice de transvection la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : Démonstration Un calcul direct donne Notation 2.4 Soit k 1 , p , K , on appelle matrice de dilatation la matrice de Mp ( K ) définie par : à la k - ième place Cette matrice est inversible et son inverse est : Démonstration Un calcul immédiat démontre que Notation 2.5 Soit une permutation a de 1 , p , on appelle matrice de permutation la matrice de Mp ( K ) définie par : Cette matrice est inversible et son inverse est : a. C' est - à - dire une bijection de de 1 , p dans 1 , p. Démonstration Si et 0 sont deux permutations de 1 , p alors on remarque que : On ***prend*** alors 0 1 en remarquant que si id1,p , alors P Ip

**26071**: Transvection Cela ***revient*** donc à faire la transformation ( dite opération élémentaire ) : ( on remplace la k - ième ligne par la k - ième ligne à laquelle on a ajouté fois la -ième ligne )

**26085**: Transvection Cela revient donc à faire la transformation ( dite opération élémentaire ) : ( on ***remplace*** la k - ième ligne par la k - ième ligne à laquelle on a ajouté fois la -ième ligne )

**26112**: Dilatation Cela ***revient*** donc à faire la transformation ( dite opération élémentaire ) : ( on remplace la k - ième ligne par fois la k - ième ligne )

**26126**: Dilatation Cela revient donc à faire la transformation ( dite opération élémentaire ) : ( on ***remplace*** la k - ième ligne par fois la k - ième ligne )

**26145**: Permutation Cela ***revient*** donc à faire la transformation ( dite opération élémentaire ) : Soit A ai , j ( i , j)1,n1,p Mn , p ( K )

**26226**: Transvection Cela ***revient*** donc à faire la transformation ( dite opération élémentaire ) : ( on remplace la -ième colonne par la -ième colonne à laquelle on a ajouté fois la k - ième colonne )

**26240**: Transvection Cela revient donc à faire la transformation ( dite opération élémentaire ) : ( on ***remplace*** la -ième colonne par la -ième colonne à laquelle on a ajouté fois la k - ième colonne )

**26265**: Dilatation Cela ***revient*** donc à faire la transformation ( dite opération élémentaire ) : ( on remplace la k - ième colonne par fois la k - ième colonne )

**26279**: Dilatation Cela revient donc à faire la transformation ( dite opération élémentaire ) : ( on ***remplace*** la k - ième colonne par fois la k - ième colonne )

**26298**: Permutation Cela ***revient*** donc à faire la transformation ( dite opération élémentaire ) : Lorsqu' on utilise des opérations élémentaires , c' est généralement pour faire apparaître le plus de 0 possibles dans la matrice

**26312**: Permutation Cela revient donc à faire la transformation ( dite opération élémentaire ) : Lorsqu' on ***utilise*** des opérations élémentaires , c' est généralement pour faire apparaître le plus de 0 possibles dans la matrice

**26318**: Permutation Cela revient donc à faire la transformation ( dite opération élémentaire ) : Lorsqu' on utilise des opérations élémentaires , c' ***est*** généralement pour faire apparaître le plus de 0 possibles dans la matrice

**26333**: Il ***est*** alors indispensable de présenter les calculs de manière lisible par le lecteur

**26349**: Le principe ***est*** : 1

**26354**: on ***encadre*** le pivot ( terme dont on se sert pour faire apparaître les 0 ) 2

**26362**: on encadre le pivot ( terme dont on se ***sert*** pour faire apparaître les 0 ) 2

**26372**: on ***signale*** la ou les opérations élémentaires effectuées sous la matrice

**26409**: Soit la matrice : Théorème 2.1 du pivot généralisé de Gauss Soit A Mn , p ( K ) , de rang r , alors il ***existe*** des matrices de transvection - dilatation - permutation de Mn ( K ) , notées R1 ,

**26467**: , Ss telles Démonstration ( algorithme du pivot de Gauss généralisé ) La démonstration ***est*** essentiellement l' algorithme du pivot de Gauss généralisé

**26478**: Si ***A*** 0n , p , alors il ne y a rien à démontrer ( on a A Jn , p,0 0n , p avec r 0 )

**26487**: Si A 0n , p , alors il ne y ***a*** rien à démontrer ( on a A Jn , p,0 0n , p avec r 0 )

**26493**: Si A 0n , p , alors il ne y a rien à démontrer ( on ***a*** A Jn , p,0 0n , p avec r 0 )

**26506**: ***Supposons*** A 6 0n , p

**26516**: Il ***existe*** alors un élément non nul de A , qu' on place en position ( 1 , 1 ) par permutation , puis qu' on transforme en 1 par dilatation

**26527**: Il existe alors un élément non nul de A , qu' on ***place*** en position ( 1 , 1 ) par permutation , puis qu' on transforme en 1 par dilatation

**26541**: Il existe alors un élément non nul de A , qu' on place en position ( 1 , 1 ) par permutation , puis qu' on ***transforme*** en 1 par dilatation

**26550**: Ce coefficient s' ***appelle*** le pivot

**26563**: Par transvection sur les lignes , on ***annule*** tous les coefficients en-dessous du pivot et par transvection sur les colonnes , on annule tous les coefficients à droite du pivot

**26578**: Par transvection sur les lignes , on annule tous les coefficients en-dessous du pivot et par transvection sur les colonnes , on ***annule*** tous les coefficients à droite du pivot

**26590**: On ***obtient*** donc une matrice de la forme ( voir la partie suivante sur les matrices - blocs ) : Si A0 est vide ( n 1 ou p 1 ) ou si A0 0n1,p1 , on s' arrête

**26611**: On obtient donc une matrice de la forme ( voir la partie suivante sur les matrices - blocs ) : Si A0 ***est*** vide ( n 1 ou p 1 ) ou si A0 0n1,p1 , on s' arrête

**26627**: On obtient donc une matrice de la forme ( voir la partie suivante sur les matrices - blocs ) : Si A0 est vide ( n 1 ou p 1 ) ou si A0 0n1,p1 , on s' ***arrête***

**26631**: Sinon on ***recommence*** l' algorithme avec A0 à la place de A À la fin , on obtient une matrice Jn , p , r , obtenue par opérations élémentaires sur les lignes et les colonnes d' où le résultat

**26646**: Sinon on recommence l' algorithme avec A0 à la place de A À la fin , on ***obtient*** une matrice Jn , p , r , obtenue par opérations élémentaires sur les lignes et les colonnes d' où le résultat

**26671**: On ***a*** nécessairement r rang(A ) , car le rang est invariant par multiplication par une matrice inversible ( remarque 2.14 , page 115 )

**26680**: On a nécessairement r rang(A ) , car le rang ***est*** invariant par multiplication par une matrice inversible ( remarque 2.14 , page 115 )

**26689**: On a nécessairement r rang(A ) , car le rang est invariant par multiplication par une matrice inversible ( ***remarque*** 2.14 , page 115 )

**26697**: Autrement ***dit*** , toute matrice de Mn , p ( K ) de rang r est équivalente à Jn , p , r

**26711**: Autrement dit , toute matrice de Mn , p ( K ) de rang r ***est*** équivalente à Jn , p , r

**26721**: On ***retrouve*** ainsi le résultat de la proposition 2.6 , page 120

**26738**: En utilisant Wxmaxima : On ***peut*** aussi travailler à la main

**26747**: Que se ***passe*** -t -il lorsque , par exemple , la première colonne est remplie de 0 ? On commence par permuter les colonnes ! Ici , nous allons écrire les matrices nous - mêmes ... Toute matrice de permutation peut s' exprimer comme produit de matrices de transvection - dilatation

**26758**: Que se passe -t -il lorsque , par exemple , la première colonne ***est*** remplie de 0 ? On commence par permuter les colonnes ! Ici , nous allons écrire les matrices nous - mêmes ... Toute matrice de permutation peut s' exprimer comme produit de matrices de transvection - dilatation

**26764**: Que se passe -t -il lorsque , par exemple , la première colonne est remplie de 0 ? On ***commence*** par permuter les colonnes ! Ici , nous allons écrire les matrices nous - mêmes ... Toute matrice de permutation peut s' exprimer comme produit de matrices de transvection - dilatation

**26785**: Que se passe -t -il lorsque , par exemple , la première colonne est remplie de 0 ? On commence par permuter les colonnes ! Ici , nous allons écrire les matrices nous - mêmes ... Toute matrice de permutation ***peut*** s' exprimer comme produit de matrices de transvection - dilatation

**26806**: Toute permutation de 1 , p ***peut*** s' exprimer comme une composée de transpositions ( une permutation qui échange seulement deux éléments )

**26818**: Toute permutation de 1 , p peut s' exprimer comme une composée de transpositions ( une permutation qui ***échange*** seulement deux éléments )

**26825**: Ceci ***peut*** se démontrer par récurrence sur p. Initialisation p 1 , c' est évident

**26837**: Ceci peut se démontrer par récurrence sur p. Initialisation p 1 , c' ***est*** évident

**26844**: Hérédité Soit p N. ***Supposons*** le résultat vrai au rang p. Soit une permutation de 1 , p 1

**26873**: transposition k , p1 de 1 , p 1 définie par : On ***pose*** alors 0 k , p1

**26881**: On ***applique*** l' hypothèse de récurrence à 0 1,p 2

**26904**: On ***écrit*** où les j sont des transpositions de 1 , p. On a alors 3

**26908**: On écrit où les j ***sont*** des transpositions de 1 , p. On a alors 3

**26916**: On écrit où les j sont des transpositions de 1 , p. On ***a*** alors 3

**26921**: Il ***suffit*** donc de démontrer le résultat pour une transposition

**26937**: Par décalage d' indices , il ***suffit*** de démontrer cela pour la transposition qui échange 1 et 2

**26951**: On ***peut*** procéder de la manière suivante : Soit A GLp ( K ) , alors il existe des matrices de transvection - dilatation - permutation de Mp ( K ) , notées R1 ,

**26967**: On peut procéder de la manière suivante : Soit A GLp ( K ) , alors il ***existe*** des matrices de transvection - dilatation - permutation de Mp ( K ) , notées R1 ,

**27017**: , Ss telles que : Autrement ***dit*** , quand A est inversible , on peut se contenter de travailler soit uniquement sur les lignes , soit uniquement sur les colonnes

**27021**: , Ss telles que : Autrement dit , quand A ***est*** inversible , on peut se contenter de travailler soit uniquement sur les lignes , soit uniquement sur les colonnes

**27025**: , Ss telles que : Autrement dit , quand A est inversible , on ***peut*** se contenter de travailler soit uniquement sur les lignes , soit uniquement sur les colonnes

**27055**: Démonstration En reprenant le résultat du théorème 2.1 , page 127 , il ***existe*** un produit de matrices de transvection - dilatationpermutation de Mn ( K ) , noté R et un produit de matrices de transvection - dilatation - permutation de Mp ( K ) , noté S tels que : car rang(A ) p ( car A est inversible ) et Jp , p , p Ip

**27101**: Démonstration En reprenant le résultat du théorème 2.1 , page 127 , il existe un produit de matrices de transvection - dilatationpermutation de Mn ( K ) , noté R et un produit de matrices de transvection - dilatation - permutation de Mp ( K ) , noté S tels que : car rang(A ) p ( car A ***est*** inversible ) et Jp , p , p Ip

**27126**: En multipliant à gauche par S et à droite par S 1 , on ***obtient*** ( S R ) A Ip

**27153**: Lorsque la matrice A ***est*** dans GLp ( K ) , on peut se limiter à au plus une dilatation de type Dp ( ) ( et ne faire donc autrement que des transvections )

**27161**: Lorsque la matrice A est dans GLp ( K ) , on ***peut*** se limiter à au plus une dilatation de type Dp ( ) ( et ne faire donc autrement que des transvections )

**27189**: En effet , il ***suffit*** de savoir le faire sur une matrice 2 2 ( a 6 0 et b 6 0 ) : Il est important de noter que : 1

**27200**: En effet , il suffit de savoir le faire sur une matrice 2 2 ( ***a*** 6 0 et b 6 0 ) : Il est important de noter que : 1

**27210**: En effet , il suffit de savoir le faire sur une matrice 2 2 ( a 6 0 et b 6 0 ) : Il ***est*** important de noter que : 1

**27221**: Lorsque l' on ***calcule*** un rang : on peut travailler à la fois sur les lignes et les colonnes

**27226**: Lorsque l' on calcule un rang : on ***peut*** travailler à la fois sur les lignes et les colonnes

**27243**: Lorsque l' on ***résout*** un système d' équations linéaires , on ne travaille que sur les lignes

**27252**: Lorsque l' on résout un système d' équations linéaires , on ne ***travaille*** que sur les lignes

**27263**: Lorsque l' on ***calcule*** un noyau , on ne travaille que sur les lignes

**27269**: Lorsque l' on calcule un noyau , on ne ***travaille*** que sur les lignes

**27280**: Lorsque l' on ***calcule*** une image , on ne travaille que sur les colonnes

**27286**: Lorsque l' on calcule une image , on ne ***travaille*** que sur les colonnes

**27298**: En effet , les opérations élémentaires ***correspondent*** à des multiplications par des matrices inversibles

**27311**: Donc , lorsqu' on ***multiplie*** : à gauche , on ne change pas le rang , ni le noyau ( manipulation sur les lignes ) à droite , on ne change pas le rang , ni l' image ( manipulation sur les colonnes )

**27318**: Donc , lorsqu' on multiplie : à gauche , on ne ***change*** pas le rang , ni le noyau ( manipulation sur les lignes ) à droite , on ne change pas le rang , ni l' image ( manipulation sur les colonnes )

**27337**: Donc , lorsqu' on multiplie : à gauche , on ne change pas le rang , ni le noyau ( manipulation sur les lignes ) à droite , on ne ***change*** pas le rang , ni l' image ( manipulation sur les colonnes )

**27358**: 2.7.1 Soit la matrice : ( ***a*** ) Calculer son rang r. ( b ) Trouver deux matrices P et Q inversibles telles que : 2.7.2 Déterminer les a K tels que la matrice : soit inversible Démontrer que A est inversible et calculer son inverse

**27380**: 2.7.1 Soit la matrice : ( a ) Calculer son rang r. ( b ) Trouver deux matrices P et Q inversibles telles que : 2.7.2 Déterminer les ***a*** K tels que la matrice : soit inversible Démontrer que A est inversible et calculer son inverse

**27392**: 2.7.1 Soit la matrice : ( a ) Calculer son rang r. ( b ) Trouver deux matrices P et Q inversibles telles que : 2.7.2 Déterminer les a K tels que la matrice : soit inversible Démontrer que A ***est*** inversible et calculer son inverse

**27405**: 2.7.4 Soit la matrice : ( ***a*** ) Déterminer le rang de A. ( b ) Déterminer une base de l' image de A. ( c ) Donner des équations de Im(A )

**27444**: ( d ) Déterminer une base du noyau de A. ( ***a*** ) Démontrer que est un isomorphisme si , et seulement si , A est inversible

**27448**: ( d ) Déterminer une base du noyau de A. ( a ) Démontrer que ***est*** un isomorphisme si , et seulement si , A est inversible

**27458**: ( d ) Déterminer une base du noyau de A. ( a ) Démontrer que est un isomorphisme si , et seulement si , A ***est*** inversible

**27493**: ( b ) Calculer le rang de en fonction de celui de A. 2.7.6 Soit A une matrice de Mn ( R ) tri-diagonale ( c' est - à - dire vérifiant ***ai*** , j 0 dès que i j 2 )

**27505**: On ***suppose*** de plus que Démontrer qu' il existe une matrice diagonale D inversible , telle que D1 A D Sn ( R )

**27512**: On suppose de plus que Démontrer qu' il ***existe*** une matrice diagonale D inversible , telle que D1 A D Sn ( R )

**27540**: n ( K ) , soit 0 , démontrer qu' il ***existe*** une matrice P Dn ( K ) GLn ( K ) telle que : Systèmes linéaires On a déjà vu au chapitre précédent ( section 1.4.1 , page 78 ) qu' en toute généralité , un système linéaire est une équation de la forme : où u L ( E , E 0 ) et b E 0 sont fixés et x est l' inconnue ( E et E 0 sont des K - espaces vectoriels )

**27579**: n ( K ) , soit 0 , démontrer qu' il existe une matrice P Dn ( K ) GLn ( K ) telle que : Systèmes linéaires On a déjà vu au chapitre précédent ( section 1.4.1 , page 78 ) qu' en toute généralité , un système linéaire ***est*** une équation de la forme : où u L ( E , E 0 ) et b E 0 sont fixés et x est l' inconnue ( E et E 0 sont des K - espaces vectoriels )

**27599**: n ( K ) , soit 0 , démontrer qu' il existe une matrice P Dn ( K ) GLn ( K ) telle que : Systèmes linéaires On a déjà vu au chapitre précédent ( section 1.4.1 , page 78 ) qu' en toute généralité , un système linéaire est une équation de la forme : où u L ( E , E 0 ) et b E 0 ***sont*** fixés et x est l' inconnue ( E et E 0 sont des K - espaces vectoriels )

**27603**: n ( K ) , soit 0 , démontrer qu' il existe une matrice P Dn ( K ) GLn ( K ) telle que : Systèmes linéaires On a déjà vu au chapitre précédent ( section 1.4.1 , page 78 ) qu' en toute généralité , un système linéaire est une équation de la forme : où u L ( E , E 0 ) et b E 0 sont fixés et x ***est*** l' inconnue ( E et E 0 sont des K - espaces vectoriels )

**27611**: n ( K ) , soit 0 , démontrer qu' il existe une matrice P Dn ( K ) GLn ( K ) telle que : Systèmes linéaires On a déjà vu au chapitre précédent ( section 1.4.1 , page 78 ) qu' en toute généralité , un système linéaire est une équation de la forme : où u L ( E , E 0 ) et b E 0 sont fixés et x est l' inconnue ( E et E 0 ***sont*** des K - espaces vectoriels )

**27636**: Dans le cas de la dimension finie et une fois des bases fixées , ce système linéaire ***est*** équivalent à un système de n équations à n inconnues , que l' on peut écrire sous forme matricielle : où A est la matrice n p de u , X la matrice p 1 de x et B la matrice n 1 de b ( avec p dim E et Lorsque l' on a un système de n équations à p inconnues , la condition de compatibilité ( c' est - à - dire la condition pour que le système admette des solutions ) s' écrit ( voir la partie suivante sur les matrices - blocs ) : rang(A ) rang A B ) ce qui se vérifie facilement à l' aide d' une méthode de pivot sur les lignes , où l' on ne prend jamais le pivot sur la colonne constituée des éléments de B. La matrice A B Mn , p1 ( K ) s' appelle la matrice augmentée du système A X B. Un système de n équations à n inconnues : est dit de Cramer , lorsque A est inversible

**27637**: Dans le cas de la dimension finie et une fois des bases fixées , ce système linéaire est ***équivalent*** à un système de n équations à n inconnues , que l' on peut écrire sous forme matricielle : où A est la matrice n p de u , X la matrice p 1 de x et B la matrice n 1 de b ( avec p dim E et Lorsque l' on a un système de n équations à p inconnues , la condition de compatibilité ( c' est - à - dire la condition pour que le système admette des solutions ) s' écrit ( voir la partie suivante sur les matrices - blocs ) : rang(A ) rang A B ) ce qui se vérifie facilement à l' aide d' une méthode de pivot sur les lignes , où l' on ne prend jamais le pivot sur la colonne constituée des éléments de B. La matrice A B Mn , p1 ( K ) s' appelle la matrice augmentée du système A X B. Un système de n équations à n inconnues : est dit de Cramer , lorsque A est inversible

**27651**: Dans le cas de la dimension finie et une fois des bases fixées , ce système linéaire est équivalent à un système de n équations à n inconnues , que l' on ***peut*** écrire sous forme matricielle : où A est la matrice n p de u , X la matrice p 1 de x et B la matrice n 1 de b ( avec p dim E et Lorsque l' on a un système de n équations à p inconnues , la condition de compatibilité ( c' est - à - dire la condition pour que le système admette des solutions ) s' écrit ( voir la partie suivante sur les matrices - blocs ) : rang(A ) rang A B ) ce qui se vérifie facilement à l' aide d' une méthode de pivot sur les lignes , où l' on ne prend jamais le pivot sur la colonne constituée des éléments de B. La matrice A B Mn , p1 ( K ) s' appelle la matrice augmentée du système A X B. Un système de n équations à n inconnues : est dit de Cramer , lorsque A est inversible

**27659**: Dans le cas de la dimension finie et une fois des bases fixées , ce système linéaire est équivalent à un système de n équations à n inconnues , que l' on peut écrire sous forme matricielle : où A ***est*** la matrice n p de u , X la matrice p 1 de x et B la matrice n 1 de b ( avec p dim E et Lorsque l' on a un système de n équations à p inconnues , la condition de compatibilité ( c' est - à - dire la condition pour que le système admette des solutions ) s' écrit ( voir la partie suivante sur les matrices - blocs ) : rang(A ) rang A B ) ce qui se vérifie facilement à l' aide d' une méthode de pivot sur les lignes , où l' on ne prend jamais le pivot sur la colonne constituée des éléments de B. La matrice A B Mn , p1 ( K ) s' appelle la matrice augmentée du système A X B. Un système de n équations à n inconnues : est dit de Cramer , lorsque A est inversible

**27691**: Dans le cas de la dimension finie et une fois des bases fixées , ce système linéaire est équivalent à un système de n équations à n inconnues , que l' on peut écrire sous forme matricielle : où A est la matrice n p de u , X la matrice p 1 de x et B la matrice n 1 de b ( avec p dim E et Lorsque l' on ***a*** un système de n équations à p inconnues , la condition de compatibilité ( c' est - à - dire la condition pour que le système admette des solutions ) s' écrit ( voir la partie suivante sur les matrices - blocs ) : rang(A ) rang A B ) ce qui se vérifie facilement à l' aide d' une méthode de pivot sur les lignes , où l' on ne prend jamais le pivot sur la colonne constituée des éléments de B. La matrice A B Mn , p1 ( K ) s' appelle la matrice augmentée du système A X B. Un système de n équations à n inconnues : est dit de Cramer , lorsque A est inversible

**27723**: Dans le cas de la dimension finie et une fois des bases fixées , ce système linéaire est équivalent à un système de n équations à n inconnues , que l' on peut écrire sous forme matricielle : où A est la matrice n p de u , X la matrice p 1 de x et B la matrice n 1 de b ( avec p dim E et Lorsque l' on a un système de n équations à p inconnues , la condition de compatibilité ( c' est - à - dire la condition pour que le système admette des solutions ) s' ***écrit*** ( voir la partie suivante sur les matrices - blocs ) : rang(A ) rang A B ) ce qui se vérifie facilement à l' aide d' une méthode de pivot sur les lignes , où l' on ne prend jamais le pivot sur la colonne constituée des éléments de B. La matrice A B Mn , p1 ( K ) s' appelle la matrice augmentée du système A X B. Un système de n équations à n inconnues : est dit de Cramer , lorsque A est inversible

**27745**: Dans le cas de la dimension finie et une fois des bases fixées , ce système linéaire est équivalent à un système de n équations à n inconnues , que l' on peut écrire sous forme matricielle : où A est la matrice n p de u , X la matrice p 1 de x et B la matrice n 1 de b ( avec p dim E et Lorsque l' on a un système de n équations à p inconnues , la condition de compatibilité ( c' est - à - dire la condition pour que le système admette des solutions ) s' écrit ( voir la partie suivante sur les matrices - blocs ) : rang(A ) rang A B ) ce qui se ***vérifie*** facilement à l' aide d' une méthode de pivot sur les lignes , où l' on ne prend jamais le pivot sur la colonne constituée des éléments de B. La matrice A B Mn , p1 ( K ) s' appelle la matrice augmentée du système A X B. Un système de n équations à n inconnues : est dit de Cramer , lorsque A est inversible

**27763**: Dans le cas de la dimension finie et une fois des bases fixées , ce système linéaire est équivalent à un système de n équations à n inconnues , que l' on peut écrire sous forme matricielle : où A est la matrice n p de u , X la matrice p 1 de x et B la matrice n 1 de b ( avec p dim E et Lorsque l' on a un système de n équations à p inconnues , la condition de compatibilité ( c' est - à - dire la condition pour que le système admette des solutions ) s' écrit ( voir la partie suivante sur les matrices - blocs ) : rang(A ) rang A B ) ce qui se vérifie facilement à l' aide d' une méthode de pivot sur les lignes , où l' on ne ***prend*** jamais le pivot sur la colonne constituée des éléments de B. La matrice A B Mn , p1 ( K ) s' appelle la matrice augmentée du système A X B. Un système de n équations à n inconnues : est dit de Cramer , lorsque A est inversible

**27786**: Dans le cas de la dimension finie et une fois des bases fixées , ce système linéaire est équivalent à un système de n équations à n inconnues , que l' on peut écrire sous forme matricielle : où A est la matrice n p de u , X la matrice p 1 de x et B la matrice n 1 de b ( avec p dim E et Lorsque l' on a un système de n équations à p inconnues , la condition de compatibilité ( c' est - à - dire la condition pour que le système admette des solutions ) s' écrit ( voir la partie suivante sur les matrices - blocs ) : rang(A ) rang A B ) ce qui se vérifie facilement à l' aide d' une méthode de pivot sur les lignes , où l' on ne prend jamais le pivot sur la colonne constituée des éléments de B. La matrice A B Mn , p1 ( K ) s' ***appelle*** la matrice augmentée du système A X B. Un système de n équations à n inconnues : est dit de Cramer , lorsque A est inversible

**27804**: Dans le cas de la dimension finie et une fois des bases fixées , ce système linéaire est équivalent à un système de n équations à n inconnues , que l' on peut écrire sous forme matricielle : où A est la matrice n p de u , X la matrice p 1 de x et B la matrice n 1 de b ( avec p dim E et Lorsque l' on a un système de n équations à p inconnues , la condition de compatibilité ( c' est - à - dire la condition pour que le système admette des solutions ) s' écrit ( voir la partie suivante sur les matrices - blocs ) : rang(A ) rang A B ) ce qui se vérifie facilement à l' aide d' une méthode de pivot sur les lignes , où l' on ne prend jamais le pivot sur la colonne constituée des éléments de B. La matrice A B Mn , p1 ( K ) s' appelle la matrice augmentée du système A X B. Un système de n équations à n inconnues : ***est*** dit de Cramer , lorsque A est inversible

**27805**: Dans le cas de la dimension finie et une fois des bases fixées , ce système linéaire est équivalent à un système de n équations à n inconnues , que l' on peut écrire sous forme matricielle : où A est la matrice n p de u , X la matrice p 1 de x et B la matrice n 1 de b ( avec p dim E et Lorsque l' on a un système de n équations à p inconnues , la condition de compatibilité ( c' est - à - dire la condition pour que le système admette des solutions ) s' écrit ( voir la partie suivante sur les matrices - blocs ) : rang(A ) rang A B ) ce qui se vérifie facilement à l' aide d' une méthode de pivot sur les lignes , où l' on ne prend jamais le pivot sur la colonne constituée des éléments de B. La matrice A B Mn , p1 ( K ) s' appelle la matrice augmentée du système A X B. Un système de n équations à n inconnues : est ***dit*** de Cramer , lorsque A est inversible

**27811**: Dans le cas de la dimension finie et une fois des bases fixées , ce système linéaire est équivalent à un système de n équations à n inconnues , que l' on peut écrire sous forme matricielle : où A est la matrice n p de u , X la matrice p 1 de x et B la matrice n 1 de b ( avec p dim E et Lorsque l' on a un système de n équations à p inconnues , la condition de compatibilité ( c' est - à - dire la condition pour que le système admette des solutions ) s' écrit ( voir la partie suivante sur les matrices - blocs ) : rang(A ) rang A B ) ce qui se vérifie facilement à l' aide d' une méthode de pivot sur les lignes , où l' on ne prend jamais le pivot sur la colonne constituée des éléments de B. La matrice A B Mn , p1 ( K ) s' appelle la matrice augmentée du système A X B. Un système de n équations à n inconnues : est dit de Cramer , lorsque A ***est*** inversible

**27820**: En ce cas , il y ***a*** existence et unicité de la solution , donnée par Pour résoudre un système de Cramer , on ne calcule jamais l' inverse de la matrice A , on utilise l' algorithme du pivot de Gauss ! Soit à résoudre le système : Appliquons l' algorithme du pivot de Gauss à la matrice augmentée A B : La deuxième ligne permet de discerner trois cas : 1

**27839**: En ce cas , il y a existence et unicité de la solution , donnée par Pour résoudre un système de Cramer , on ne ***calcule*** jamais l' inverse de la matrice A , on utilise l' algorithme du pivot de Gauss ! Soit à résoudre le système : Appliquons l' algorithme du pivot de Gauss à la matrice augmentée A B : La deuxième ligne permet de discerner trois cas : 1

**27849**: En ce cas , il y a existence et unicité de la solution , donnée par Pour résoudre un système de Cramer , on ne calcule jamais l' inverse de la matrice A , on ***utilise*** l' algorithme du pivot de Gauss ! Soit à résoudre le système : Appliquons l' algorithme du pivot de Gauss à la matrice augmentée A B : La deuxième ligne permet de discerner trois cas : 1

**27863**: En ce cas , il y a existence et unicité de la solution , donnée par Pour résoudre un système de Cramer , on ne calcule jamais l' inverse de la matrice A , on utilise l' algorithme du pivot de Gauss ! Soit à résoudre le système : ***Appliquons*** l' algorithme du pivot de Gauss à la matrice augmentée A B : La deuxième ligne permet de discerner trois cas : 1

**27880**: En ce cas , il y a existence et unicité de la solution , donnée par Pour résoudre un système de Cramer , on ne calcule jamais l' inverse de la matrice A , on utilise l' algorithme du pivot de Gauss ! Soit à résoudre le système : Appliquons l' algorithme du pivot de Gauss à la matrice augmentée A B : La deuxième ligne ***permet*** de discerner trois cas : 1

**27892**: 1 , le système ***est*** incompatible car rang(A ) 2 et rangA B 3 : 2

**27909**: 6 , le système ***est*** compatible car rang(A ) rangA B 2 : Comme on a travaillé sur les lignes , les solutions du système sont les solutions du système réduit , soit : 3

**27930**: 6 , le système est compatible car rang(A ) rangA B 2 : Comme on a travaillé sur les lignes , les solutions du système ***sont*** les solutions du système réduit , soit : 3

**27948**: 6 1 , 6 , le système ***est*** de Cramer ( solution unique ) , le système réduit s' écrit : On voit que le cas 6 ne s' obtient pas par continuité du cas de Cramer

**27960**: 6 1 , 6 , le système est de Cramer ( solution unique ) , le système réduit s' ***écrit*** : On voit que le cas 6 ne s' obtient pas par continuité du cas de Cramer

**27963**: 6 1 , 6 , le système est de Cramer ( solution unique ) , le système réduit s' écrit : On ***voit*** que le cas 6 ne s' obtient pas par continuité du cas de Cramer

**27970**: 6 1 , 6 , le système est de Cramer ( solution unique ) , le système réduit s' écrit : On voit que le cas 6 ne s' ***obtient*** pas par continuité du cas de Cramer

**27984**: Donc : l' ordinateur ne ***fait*** pas apparaître le cas ! ! ! Si on a vraiment besoin de calculer l' inverse d' une matrice A GLp ( K ) inversible , on peut appliquer l' algorithme du pivot de Gauss généralisée à la matrice augmentée A Ip , avec des opérations sur les lignes

**27994**: Donc : l' ordinateur ne fait pas apparaître le cas ! ! ! Si on ***a*** vraiment besoin de calculer l' inverse d' une matrice A GLp ( K ) inversible , on peut appliquer l' algorithme du pivot de Gauss généralisée à la matrice augmentée A Ip , avec des opérations sur les lignes

**28012**: Donc : l' ordinateur ne fait pas apparaître le cas ! ! ! Si on a vraiment besoin de calculer l' inverse d' une matrice A GLp ( K ) inversible , on ***peut*** appliquer l' algorithme du pivot de Gauss généralisée à la matrice augmentée A Ip , avec des opérations sur les lignes

**28036**: On ***obtient*** à la fin une matrice augmentée de la forme En effet , si on note E la matrice correspondante aux opérations élémentaires sur les lignes lors de l' algorithme , on a E A Ip , d' où E Ip A1

**28068**: On obtient à la fin une matrice augmentée de la forme En effet , si on note E la matrice correspondante aux opérations élémentaires sur les lignes lors de l' algorithme , on ***a*** E A Ip , d' où E Ip A1

**28080**: Autrement ***dit*** , en effectuant les mêmes opérations sur Ip , on obtient A1

**28091**: Autrement dit , en effectuant les mêmes opérations sur Ip , on ***obtient*** A1

**28097**: 2.8.1 Soit ( ***a*** , b , c ) C2

**28116**: Résoudre le système et donner une condition nécessaire et suffisante sur ***a*** , b et c pour que les solutions soient réelles

**28141**: 2.8.2 Inverser la matrice 2.8.3 Résoudre le système Matrices - blocs Notation 2.6 ***signifie*** que : mi , j ai , j mi , j cin1 , j i , j bi , jp1 On parle alors de matrice - blocs

**28147**: 2.8.2 Inverser la matrice 2.8.3 Résoudre le système Matrices - blocs Notation 2.6 signifie que : mi , j ***ai*** , j mi , j cin1 , j i , j bi , jp1 On parle alors de matrice - blocs

**28163**: 2.8.2 Inverser la matrice 2.8.3 Résoudre le système Matrices - blocs Notation 2.6 signifie que : mi , j ai , j mi , j cin1 , j i , j bi , jp1 On ***parle*** alors de matrice - blocs

**28171**: On ***peut*** généraliser cette notation à un nombre de blocs plus grand

**28199**: La matrice Jn , p , r ( voir la notation 2.2 , page 119 ) ***peut*** s' écrire sous la forme d' une matrice - blocs : 0nr , r 0nr , pr Multiplication d' une matrice - bloc par un scalaire

**28228**: Si ***A*** Mn1 , p1 ( K ) , B Mn1 , p2 ( K ) , C Les sommes entre matrices - blocs incompatibles ne peuvent s' exprimer en termes de blocs

**28253**: Si A Mn1 , p1 ( K ) , B Mn1 , p2 ( K ) , C Les sommes entre matrices - blocs incompatibles ne ***peuvent*** s' exprimer en termes de blocs

**28262**: Produit ***a*** entre matrices - blocs compatibles

**28270**: Si ***A*** Mn1 , p1 ( K ) , B Mn1 , p2 ( K ) , C Mn2 , p1 ( K ) et A EB G A F B H C ED G C F D H Même remarque que précédemment , lorsque les dimensions ne sont pas compatibles

**28309**: Si A Mn1 , p1 ( K ) , B Mn1 , p2 ( K ) , C Mn2 , p1 ( K ) et A EB G A F B H C ED G C F D H Même ***remarque*** que précédemment , lorsque les dimensions ne sont pas compatibles

**28317**: Si A Mn1 , p1 ( K ) , B Mn1 , p2 ( K ) , C Mn2 , p1 ( K ) et A EB G A F B H C ED G C F D H Même remarque que précédemment , lorsque les dimensions ne ***sont*** pas compatibles

**28322**: On ***peut*** bien sûr , généraliser à un nombre de blocs plus grand , il faut faire attention à bien conserver les compatibilités entre les dimensions

**28336**: On peut bien sûr , généraliser à un nombre de blocs plus grand , il ***faut*** faire attention à bien conserver les compatibilités entre les dimensions

**28359**: Attention à ne pas oublier que la multiplication entre matrices ne ***est*** pas commutative ! Démonstration Il s' agit de simples vérifications par calculs ( laissées en exercice )

**28366**: Attention à ne pas oublier que la multiplication entre matrices ne est pas commutative ! Démonstration Il s' ***agit*** de simples vérifications par calculs ( laissées en exercice )

**28453**: A MatE1 , E10 ( 1 1 f E1 ) où 1 ***est*** la projection sur E10 parallèlement à E20 C MatE1 , E20 ( 2 2 f E1 ) où 2 idE 0 1 est la projection sur E20 parallèlement à E10 Démonstration Immédiat en décomposant chaque élément de f ( E1 ) et chaque élément de f ( E2 ) sur la base ( E10 , E20 ) ( en exercice ) En particulier , pour le cas des endomorphismes ( c' est - à - dire E E 0 , E10 E1 et E20 E2 ) , on a : E1 stable par f C 0n2 , p1 et E2 stable par f B 0n1 , p2 Ainsi , les matrices - blocs permettent de visualiser simplement certains sous-espaces stables

**28476**: A MatE1 , E10 ( 1 1 f E1 ) où 1 est la projection sur E10 parallèlement à E20 C MatE1 , E20 ( 2 2 f E1 ) où 2 idE 0 1 ***est*** la projection sur E20 parallèlement à E10 Démonstration Immédiat en décomposant chaque élément de f ( E1 ) et chaque élément de f ( E2 ) sur la base ( E10 , E20 ) ( en exercice ) En particulier , pour le cas des endomorphismes ( c' est - à - dire E E 0 , E10 E1 et E20 E2 ) , on a : E1 stable par f C 0n2 , p1 et E2 stable par f B 0n1 , p2 Ainsi , les matrices - blocs permettent de visualiser simplement certains sous-espaces stables

**28542**: A MatE1 , E10 ( 1 1 f E1 ) où 1 est la projection sur E10 parallèlement à E20 C MatE1 , E20 ( 2 2 f E1 ) où 2 idE 0 1 est la projection sur E20 parallèlement à E10 Démonstration Immédiat en décomposant chaque élément de f ( E1 ) et chaque élément de f ( E2 ) sur la base ( E10 , E20 ) ( en exercice ) En particulier , pour le cas des endomorphismes ( c' est - à - dire E E 0 , E10 E1 et E20 E2 ) , on ***a*** : E1 stable par f C 0n2 , p1 et E2 stable par f B 0n1 , p2 Ainsi , les matrices - blocs permettent de visualiser simplement certains sous-espaces stables

**28567**: A MatE1 , E10 ( 1 1 f E1 ) où 1 est la projection sur E10 parallèlement à E20 C MatE1 , E20 ( 2 2 f E1 ) où 2 idE 0 1 est la projection sur E20 parallèlement à E10 Démonstration Immédiat en décomposant chaque élément de f ( E1 ) et chaque élément de f ( E2 ) sur la base ( E10 , E20 ) ( en exercice ) En particulier , pour le cas des endomorphismes ( c' est - à - dire E E 0 , E10 E1 et E20 E2 ) , on a : E1 stable par f C 0n2 , p1 et E2 stable par f B 0n1 , p2 Ainsi , les matrices - blocs ***permettent*** de visualiser simplement certains sous-espaces stables

**28580**: Utilisation La notation par blocs ***est*** extrêmement pratique pour faire des récurrences , ainsi que pour représenter simplement les matrices

**28604**: L' ensemble des matrices triangulaires supérieures ***est*** stable par la multiplication externe et par l' addition et la multiplication interne

**28630**: De plus , n ( K ) GLn ( K ) ***est*** stable par M 7 M où A GLn1 ( K ) et D GLn2 ( K ) , alors M GLn1 n2 ( K ) et son inverse est de la forme : On rencontre souvent des transvections - blocs , des dilatations - blocs et des permutations - blocs , ainsi : vérifie clairement ( en choisissant k et pour avoir des dimensions compatibles ) : soit une opération élémentaire sur les blocs : ( produit à gauche ) ( produit à droite ) De même avec les dilatations : ( produit à gauche ) ( produit à droite ) La permutation - bloc serait par exemple : Cela permet ( avec quelques précautions de calcul ) de faire des manipulations sur les matrices - blocs comme sur des matrices usuelles

**28637**: De plus , n ( K ) GLn ( K ) est stable par M 7 M où ***A*** GLn1 ( K ) et D GLn2 ( K ) , alors M GLn1 n2 ( K ) et son inverse est de la forme : On rencontre souvent des transvections - blocs , des dilatations - blocs et des permutations - blocs , ainsi : vérifie clairement ( en choisissant k et pour avoir des dimensions compatibles ) : soit une opération élémentaire sur les blocs : ( produit à gauche ) ( produit à droite ) De même avec les dilatations : ( produit à gauche ) ( produit à droite ) La permutation - bloc serait par exemple : Cela permet ( avec quelques précautions de calcul ) de faire des manipulations sur les matrices - blocs comme sur des matrices usuelles

**28659**: De plus , n ( K ) GLn ( K ) est stable par M 7 M où A GLn1 ( K ) et D GLn2 ( K ) , alors M GLn1 n2 ( K ) et son inverse ***est*** de la forme : On rencontre souvent des transvections - blocs , des dilatations - blocs et des permutations - blocs , ainsi : vérifie clairement ( en choisissant k et pour avoir des dimensions compatibles ) : soit une opération élémentaire sur les blocs : ( produit à gauche ) ( produit à droite ) De même avec les dilatations : ( produit à gauche ) ( produit à droite ) La permutation - bloc serait par exemple : Cela permet ( avec quelques précautions de calcul ) de faire des manipulations sur les matrices - blocs comme sur des matrices usuelles

**28665**: De plus , n ( K ) GLn ( K ) est stable par M 7 M où A GLn1 ( K ) et D GLn2 ( K ) , alors M GLn1 n2 ( K ) et son inverse est de la forme : On ***rencontre*** souvent des transvections - blocs , des dilatations - blocs et des permutations - blocs , ainsi : vérifie clairement ( en choisissant k et pour avoir des dimensions compatibles ) : soit une opération élémentaire sur les blocs : ( produit à gauche ) ( produit à droite ) De même avec les dilatations : ( produit à gauche ) ( produit à droite ) La permutation - bloc serait par exemple : Cela permet ( avec quelques précautions de calcul ) de faire des manipulations sur les matrices - blocs comme sur des matrices usuelles

**28684**: De plus , n ( K ) GLn ( K ) est stable par M 7 M où A GLn1 ( K ) et D GLn2 ( K ) , alors M GLn1 n2 ( K ) et son inverse est de la forme : On rencontre souvent des transvections - blocs , des dilatations - blocs et des permutations - blocs , ainsi : ***vérifie*** clairement ( en choisissant k et pour avoir des dimensions compatibles ) : soit une opération élémentaire sur les blocs : ( produit à gauche ) ( produit à droite ) De même avec les dilatations : ( produit à gauche ) ( produit à droite ) La permutation - bloc serait par exemple : Cela permet ( avec quelques précautions de calcul ) de faire des manipulations sur les matrices - blocs comme sur des matrices usuelles

**28741**: De plus , n ( K ) GLn ( K ) est stable par M 7 M où A GLn1 ( K ) et D GLn2 ( K ) , alors M GLn1 n2 ( K ) et son inverse est de la forme : On rencontre souvent des transvections - blocs , des dilatations - blocs et des permutations - blocs , ainsi : vérifie clairement ( en choisissant k et pour avoir des dimensions compatibles ) : soit une opération élémentaire sur les blocs : ( produit à gauche ) ( produit à droite ) De même avec les dilatations : ( produit à gauche ) ( produit à droite ) La permutation - bloc serait par exemple : Cela ***permet*** ( avec quelques précautions de calcul ) de faire des manipulations sur les matrices - blocs comme sur des matrices usuelles

**28764**: ***a.*** doit être inversible , car on veut conserver le rang ! ! 1

**28765**: a. ***doit*** être inversible , car on veut conserver le rang ! ! 1

**28771**: a. doit être inversible , car on ***veut*** conserver le rang ! ! 1

**28780**: Si ***A*** GLn ( K ) est de la forme : .C L est inversible 2

**28785**: Si A GLn ( K ) ***est*** de la forme : .C L est inversible 2

**28792**: Si A GLn ( K ) est de la forme : .C L ***est*** inversible 2

**28797**: On ***peut*** en déduire que si A GLn ( K ) , alors il existe une matrice de permutation P , une matrice T1 triangulaire inférieure avec des 1 sur la diagonale et une matrice T2 triangulaire supérieure telles Produit de Kronecker Soit A ai , j Mn1 , p1 ( K ) et B Mn2 , p2 ( K ) , on définit le produit de Kronecker de A et B comme la matrice - blocs : alors pour toute matrice B à coefficients dans K , on a 2

**28810**: On peut en déduire que si A GLn ( K ) , alors il ***existe*** une matrice de permutation P , une matrice T1 triangulaire inférieure avec des 1 sur la diagonale et une matrice T2 triangulaire supérieure telles Produit de Kronecker Soit A ai , j Mn1 , p1 ( K ) et B Mn2 , p2 ( K ) , on définit le produit de Kronecker de A et B comme la matrice - blocs : alors pour toute matrice B à coefficients dans K , on a 2

**28859**: On peut en déduire que si A GLn ( K ) , alors il existe une matrice de permutation P , une matrice T1 triangulaire inférieure avec des 1 sur la diagonale et une matrice T2 triangulaire supérieure telles Produit de Kronecker Soit A ai , j Mn1 , p1 ( K ) et B Mn2 , p2 ( K ) , on ***définit*** le produit de Kronecker de A et B comme la matrice - blocs : alors pour toute matrice B à coefficients dans K , on a 2

**28885**: On peut en déduire que si A GLn ( K ) , alors il existe une matrice de permutation P , une matrice T1 triangulaire inférieure avec des 1 sur la diagonale et une matrice T2 triangulaire supérieure telles Produit de Kronecker Soit A ai , j Mn1 , p1 ( K ) et B Mn2 , p2 ( K ) , on définit le produit de Kronecker de A et B comme la matrice - blocs : alors pour toute matrice B à coefficients dans K , on ***a*** 2

**28890**: Pour toute ***A*** Mn ( K ) , la matrice de l' application : dans la base canonique ( judicieusement ordonnée ) est : 3

**28910**: Pour toute A Mn ( K ) , la matrice de l' application : dans la base canonique ( judicieusement ordonnée ) ***est*** : 3

**28926**: En prenant le même ordre de la base canonique , pour toute ***A*** Mn ( K ) la matrice de de l' application : Proposition 2.8 Démonstration Il s' agit d' une simple vérification par calculs ( laissée en exercice )

**28943**: En prenant le même ordre de la base canonique , pour toute A Mn ( K ) la matrice de de l' application : Proposition 2.8 Démonstration Il s' ***agit*** d' une simple vérification par calculs ( laissée en exercice )

**28957**: ( ***a*** ) Démontrer que : rang(M ) rang ( b ) En déduire le rang de M

**28979**: ( c ) On ***suppose*** M inversible

**29006**: 2.9.2 Inverser la matrice : 2.9.3 Calculer rang(A B ) en fonction de rang(A ) et rang(B ) pour ***A*** Mn , p ( K ) et B Mq , s ( K )

**29083**: On ***considère*** la matrice diagonale par blocs : ( a ) Démontrer que ( b ) Démontrer que : rang(A ) rang(B ) rang(D ) ( c ) Donner un exemple où l' inégalité est stricte

**29091**: On considère la matrice diagonale par blocs : ( ***a*** ) Démontrer que ( b ) Démontrer que : rang(A ) rang(B ) rang(D ) ( c ) Donner un exemple où l' inégalité est stricte

**29116**: On considère la matrice diagonale par blocs : ( a ) Démontrer que ( b ) Démontrer que : rang(A ) rang(B ) rang(D ) ( c ) Donner un exemple où l' inégalité ***est*** stricte

**29128**: ( d ) Démontrer que si B ou D ***est*** une matrice carrée inversible , alors il y a égalité

**29137**: ( d ) Démontrer que si B ou D est une matrice carrée inversible , alors il y ***a*** égalité

**29158**: Une permutation de E ***est*** une bijection de E dans E. On note S(E ) l' ensemble des permutations de E Dans le cas particulier où E 1 , p avec p N , on note Sp S(1 , p ) l' ensemble des permutations de 1 , p On a les propriétés suivantes : idE S(E ) , en particulier S(E ) ne est jamais vide S(E ) est stable par composition : la composition est associative : il existe un élément neutre , en l' occurrence idE : S(E ) est stable par passage à l' inverse ( qui existe toujours et est unique ) : On dit que ( S(E ) , ) est un groupe

**29204**: Une permutation de E est une bijection de E dans E. On note S(E ) l' ensemble des permutations de E Dans le cas particulier où E 1 , p avec p N , on note Sp S(1 , p ) l' ensemble des permutations de 1 , p On ***a*** les propriétés suivantes : idE S(E ) , en particulier S(E ) ne est jamais vide S(E ) est stable par composition : la composition est associative : il existe un élément neutre , en l' occurrence idE : S(E ) est stable par passage à l' inverse ( qui existe toujours et est unique ) : On dit que ( S(E ) , ) est un groupe

**29218**: Une permutation de E est une bijection de E dans E. On note S(E ) l' ensemble des permutations de E Dans le cas particulier où E 1 , p avec p N , on note Sp S(1 , p ) l' ensemble des permutations de 1 , p On a les propriétés suivantes : idE S(E ) , en particulier S(E ) ne ***est*** jamais vide S(E ) est stable par composition : la composition est associative : il existe un élément neutre , en l' occurrence idE : S(E ) est stable par passage à l' inverse ( qui existe toujours et est unique ) : On dit que ( S(E ) , ) est un groupe

**29223**: Une permutation de E est une bijection de E dans E. On note S(E ) l' ensemble des permutations de E Dans le cas particulier où E 1 , p avec p N , on note Sp S(1 , p ) l' ensemble des permutations de 1 , p On a les propriétés suivantes : idE S(E ) , en particulier S(E ) ne est jamais vide S(E ) ***est*** stable par composition : la composition est associative : il existe un élément neutre , en l' occurrence idE : S(E ) est stable par passage à l' inverse ( qui existe toujours et est unique ) : On dit que ( S(E ) , ) est un groupe

**29230**: Une permutation de E est une bijection de E dans E. On note S(E ) l' ensemble des permutations de E Dans le cas particulier où E 1 , p avec p N , on note Sp S(1 , p ) l' ensemble des permutations de 1 , p On a les propriétés suivantes : idE S(E ) , en particulier S(E ) ne est jamais vide S(E ) est stable par composition : la composition ***est*** associative : il existe un élément neutre , en l' occurrence idE : S(E ) est stable par passage à l' inverse ( qui existe toujours et est unique ) : On dit que ( S(E ) , ) est un groupe

**29234**: Une permutation de E est une bijection de E dans E. On note S(E ) l' ensemble des permutations de E Dans le cas particulier où E 1 , p avec p N , on note Sp S(1 , p ) l' ensemble des permutations de 1 , p On a les propriétés suivantes : idE S(E ) , en particulier S(E ) ne est jamais vide S(E ) est stable par composition : la composition est associative : il ***existe*** un élément neutre , en l' occurrence idE : S(E ) est stable par passage à l' inverse ( qui existe toujours et est unique ) : On dit que ( S(E ) , ) est un groupe

**29246**: Une permutation de E est une bijection de E dans E. On note S(E ) l' ensemble des permutations de E Dans le cas particulier où E 1 , p avec p N , on note Sp S(1 , p ) l' ensemble des permutations de 1 , p On a les propriétés suivantes : idE S(E ) , en particulier S(E ) ne est jamais vide S(E ) est stable par composition : la composition est associative : il existe un élément neutre , en l' occurrence idE : S(E ) ***est*** stable par passage à l' inverse ( qui existe toujours et est unique ) : On dit que ( S(E ) , ) est un groupe

**29255**: Une permutation de E est une bijection de E dans E. On note S(E ) l' ensemble des permutations de E Dans le cas particulier où E 1 , p avec p N , on note Sp S(1 , p ) l' ensemble des permutations de 1 , p On a les propriétés suivantes : idE S(E ) , en particulier S(E ) ne est jamais vide S(E ) est stable par composition : la composition est associative : il existe un élément neutre , en l' occurrence idE : S(E ) est stable par passage à l' inverse ( qui ***existe*** toujours et est unique ) : On dit que ( S(E ) , ) est un groupe

**29258**: Une permutation de E est une bijection de E dans E. On note S(E ) l' ensemble des permutations de E Dans le cas particulier où E 1 , p avec p N , on note Sp S(1 , p ) l' ensemble des permutations de 1 , p On a les propriétés suivantes : idE S(E ) , en particulier S(E ) ne est jamais vide S(E ) est stable par composition : la composition est associative : il existe un élément neutre , en l' occurrence idE : S(E ) est stable par passage à l' inverse ( qui existe toujours et ***est*** unique ) : On dit que ( S(E ) , ) est un groupe

**29263**: Une permutation de E est une bijection de E dans E. On note S(E ) l' ensemble des permutations de E Dans le cas particulier où E 1 , p avec p N , on note Sp S(1 , p ) l' ensemble des permutations de 1 , p On a les propriétés suivantes : idE S(E ) , en particulier S(E ) ne est jamais vide S(E ) est stable par composition : la composition est associative : il existe un élément neutre , en l' occurrence idE : S(E ) est stable par passage à l' inverse ( qui existe toujours et est unique ) : On ***dit*** que ( S(E ) , ) est un groupe

**29270**: Une permutation de E est une bijection de E dans E. On note S(E ) l' ensemble des permutations de E Dans le cas particulier où E 1 , p avec p N , on note Sp S(1 , p ) l' ensemble des permutations de 1 , p On a les propriétés suivantes : idE S(E ) , en particulier S(E ) ne est jamais vide S(E ) est stable par composition : la composition est associative : il existe un élément neutre , en l' occurrence idE : S(E ) est stable par passage à l' inverse ( qui existe toujours et est unique ) : On dit que ( S(E ) , ) ***est*** un groupe

**29276**: On l' ***appelle*** le groupe symétrique de E et Sp s' appelle le groupe symétrique de degré p. Soit p N

**29285**: On l' appelle le groupe symétrique de E et Sp s' ***appelle*** le groupe symétrique de degré p. Soit p N

**29297**: On ***a*** card Sp p ! Démonstration Pour construire une permutation Sp : On a p choix possibles pour ( 1 ) ( les p éléments de 1 , p ) Une fois ( 1 ) choisi , on a p 1 choix possibles pour ( 2 ) ( les p 1 éléments de 1 , p ( 1 ) ) Plus généralement , une fois ( 1 ) ,

**29310**: On a card Sp p ! Démonstration Pour construire une permutation Sp : On ***a*** p choix possibles pour ( 1 ) ( les p éléments de 1 , p ) Une fois ( 1 ) choisi , on a p 1 choix possibles pour ( 2 ) ( les p 1 éléments de 1 , p ( 1 ) ) Plus généralement , une fois ( 1 ) ,

**29335**: On a card Sp p ! Démonstration Pour construire une permutation Sp : On a p choix possibles pour ( 1 ) ( les p éléments de 1 , p ) Une fois ( 1 ) choisi , on ***a*** p 1 choix possibles pour ( 2 ) ( les p 1 éléments de 1 , p ( 1 ) ) Plus généralement , une fois ( 1 ) ,

**29373**: , ( i ) ***choisis*** , on a p i choix possibles pour ( i 1 ) ( les p i Une fois ( 1 ) ,

**29376**: , ( i ) choisis , on ***a*** p i choix possibles pour ( i 1 ) ( les p i Une fois ( 1 ) ,

**29404**: , ( p 1 ) ***choisis*** , il ne reste plus qu' un seul choix pour ( p ) ( l' unique élément de Finalement , on a choix possibles pour construire , d' où le résultat

**29408**: , ( p 1 ) choisis , il ne ***reste*** plus qu' un seul choix pour ( p ) ( l' unique élément de Finalement , on a choix possibles pour construire , d' où le résultat

**29426**: , ( p 1 ) choisis , il ne reste plus qu' un seul choix pour ( p ) ( l' unique élément de Finalement , on ***a*** choix possibles pour construire , d' où le résultat

**29439**: La démonstration ***peut*** se faire plus rigoureusement par récurrence sur p. Soit p N , p 2 , et soit ( i , j ) 1 , p , i 6 j. La transposition i , j est la permutation de Sp qui échange i et j et qui laisse stable les autres entiers : Soit p N , p 2

**29474**: La démonstration peut se faire plus rigoureusement par récurrence sur p. Soit p N , p 2 , et soit ( i , j ) 1 , p , i 6 j. La transposition i , j ***est*** la permutation de Sp qui échange i et j et qui laisse stable les autres entiers : Soit p N , p 2

**29486**: La démonstration peut se faire plus rigoureusement par récurrence sur p. Soit p N , p 2 , et soit ( i , j ) 1 , p , i 6 j. La transposition i , j est la permutation de Sp qui échange i et j et qui ***laisse*** stable les autres entiers : Soit p N , p 2

**29505**: Toute permutation 1 , p s' ***écrit*** à l' aide de transpositions , c' est - à - dire qu' il existe des transpositions de Sp , notée 1 ,

**29520**: Toute permutation 1 , p s' écrit à l' aide de transpositions , c' est - à - dire qu' il ***existe*** des transpositions de Sp , notée 1 ,

**29563**: Soit p N , soit Sp , on ***appelle*** signature de et on note a : a. On fait le produit sur tous les couples ( i , j ) 1 , p2 tels que i j. Pour p 1 , on obtient un produit vide qui vaut 1 par convention

**29568**: Soit p N , soit Sp , on appelle signature de et on ***note*** a : a. On fait le produit sur tous les couples ( i , j ) 1 , p2 tels que i j. Pour p 1 , on obtient un produit vide qui vaut 1 par convention

**29569**: Soit p N , soit Sp , on appelle signature de et on note ***a*** : a. On fait le produit sur tous les couples ( i , j ) 1 , p2 tels que i j. Pour p 1 , on obtient un produit vide qui vaut 1 par convention

**29571**: Soit p N , soit Sp , on appelle signature de et on note a : ***a.*** On fait le produit sur tous les couples ( i , j ) 1 , p2 tels que i j. Pour p 1 , on obtient un produit vide qui vaut 1 par convention

**29573**: Soit p N , soit Sp , on appelle signature de et on note a : a. On ***fait*** le produit sur tous les couples ( i , j ) 1 , p2 tels que i j. Pour p 1 , on obtient un produit vide qui vaut 1 par convention

**29597**: Soit p N , soit Sp , on appelle signature de et on note a : a. On fait le produit sur tous les couples ( i , j ) 1 , p2 tels que i j. Pour p 1 , on ***obtient*** un produit vide qui vaut 1 par convention

**29602**: Soit p N , soit Sp , on appelle signature de et on note a : a. On fait le produit sur tous les couples ( i , j ) 1 , p2 tels que i j. Pour p 1 , on obtient un produit vide qui ***vaut*** 1 par convention

**29608**: On ***a*** ( ) ( 1)N , où N est le nombre d' inversions , c' est - à - dire les couples ( i , j ) 1 , n tels que i j et ( i ) ( j )

**29616**: On a ( ) ( 1)N , où N ***est*** le nombre d' inversions , c' est - à - dire les couples ( i , j ) 1 , n tels que i j et ( i ) ( j )

**29667**: La signature ***est*** à valeurs dans 1 , 1 et : Démonstration En effet , pour le premier produit , on peut faire le changement de variable ( i , j ) ( 0 ( i ) , 0 ( j ) ) ( grâce à la bijectivité de 0 )

**29686**: La signature est à valeurs dans 1 , 1 et : Démonstration En effet , pour le premier produit , on ***peut*** faire le changement de variable ( i , j ) ( 0 ( i ) , 0 ( j ) ) ( grâce à la bijectivité de 0 )

**29722**: La signature d' une transposition ***est*** 1

**29728**: Démonstration On se ***place*** dans Sp avec p N , p 2

**29739**: C' ***est*** vrai pour 2,1 1,2 On exprime alors les autres transpositions à l' aide de 1,2 et 2,1

**29745**: C' est vrai pour 2,1 1,2 On ***exprime*** alors les autres transpositions à l' aide de 1,2 et 2,1

**29773**: Par exemple , si ( k , ) 1 , p2 , Les autres cas ***sont*** analogues ( laissés en exercice )

**29783**: Cela nous ***donne*** un moyen effectif de calculer la signature d' une permutation ( bien qu' il existe des algorithmes nettement plus performants )

**29798**: Cela nous donne un moyen effectif de calculer la signature d' une permutation ( bien qu' il ***existe*** des algorithmes nettement plus performants )

**29822**: En effet , en décomposant une permutation à l' aide de transpositions : et cela ne ***dépend*** pas de la décomposition choisie ( qui ne est pas unique ) car si est une autre décomposition à l' aide de transpositions , on a ( ) ( 1)r ( 1)s ( r et s ont même parité )

**29831**: En effet , en décomposant une permutation à l' aide de transpositions : et cela ne dépend pas de la décomposition choisie ( qui ne ***est*** pas unique ) car si est une autre décomposition à l' aide de transpositions , on a ( ) ( 1)r ( 1)s ( r et s ont même parité )

**29837**: En effet , en décomposant une permutation à l' aide de transpositions : et cela ne dépend pas de la décomposition choisie ( qui ne est pas unique ) car si ***est*** une autre décomposition à l' aide de transpositions , on a ( ) ( 1)r ( 1)s ( r et s ont même parité )

**29848**: En effet , en décomposant une permutation à l' aide de transpositions : et cela ne dépend pas de la décomposition choisie ( qui ne est pas unique ) car si est une autre décomposition à l' aide de transpositions , on ***a*** ( ) ( 1)r ( 1)s ( r et s ont même parité )

**29859**: En effet , en décomposant une permutation à l' aide de transpositions : et cela ne dépend pas de la décomposition choisie ( qui ne est pas unique ) car si est une autre décomposition à l' aide de transpositions , on a ( ) ( 1)r ( 1)s ( r et s ***ont*** même parité )

**29873**: Soit la permutation de 1 , 9 donnée par ***a*** : alors sa signature vaut : 1

**29878**: Soit la permutation de 1 , 9 donnée par a : alors sa signature ***vaut*** : 1

**29886**: En effet : on ***a*** ( 1 ) 3 et ( 3 ) 1 , ce qui donne la transposition 1,3 Finalement , on a une décomposition en 6 transpositions : a. C' est - à - dire que l' entier en ( 2 , j ) ( deuxième ligne ) est l' image par de l' entier en ( 1 , j ) ( première ligne )

**29899**: En effet : on a ( 1 ) 3 et ( 3 ) 1 , ce qui ***donne*** la transposition 1,3 Finalement , on a une décomposition en 6 transpositions : a. C' est - à - dire que l' entier en ( 2 , j ) ( deuxième ligne ) est l' image par de l' entier en ( 1 , j ) ( première ligne )

**29906**: En effet : on a ( 1 ) 3 et ( 3 ) 1 , ce qui donne la transposition 1,3 Finalement , on ***a*** une décomposition en 6 transpositions : a. C' est - à - dire que l' entier en ( 2 , j ) ( deuxième ligne ) est l' image par de l' entier en ( 1 , j ) ( première ligne )

**29913**: En effet : on a ( 1 ) 3 et ( 3 ) 1 , ce qui donne la transposition 1,3 Finalement , on a une décomposition en 6 transpositions : ***a.*** C' est - à - dire que l' entier en ( 2 , j ) ( deuxième ligne ) est l' image par de l' entier en ( 1 , j ) ( première ligne )

**29933**: En effet : on a ( 1 ) 3 et ( 3 ) 1 , ce qui donne la transposition 1,3 Finalement , on a une décomposition en 6 transpositions : a. C' est - à - dire que l' entier en ( 2 , j ) ( deuxième ligne ) ***est*** l' image par de l' entier en ( 1 , j ) ( première ligne )

**29955**: Bien retenir qu' il ***suffit*** de regarder ce qui se passe avec les transpositions ! Formes p - linéaires sur un espace vectoriel de dimension n Soit E un K - espace vectoriel de dimension finie , soit p N

**29961**: Bien retenir qu' il suffit de regarder ce qui se ***passe*** avec les transpositions ! Formes p - linéaires sur un espace vectoriel de dimension n Soit E un K - espace vectoriel de dimension finie , soit p N

**29993**: On ***appelle*** forme p - linéaire sur E toute application : telle que : soit linéaire Lp ( E , K ) l' ensemble des formes p - linéaires sur E L' ensemble Lp ( E , K ) est un K - espace vectoriel pour les opérations usuelles

**30031**: On appelle forme p - linéaire sur E toute application : telle que : soit linéaire Lp ( E , K ) l' ensemble des formes p - linéaires sur E L' ensemble Lp ( E , K ) ***est*** un K - espace vectoriel pour les opérations usuelles

**30047**: Pour p 1 , on ***retrouve*** les formes linéaires : L1 ( E ) E ?

**30064**: Pour p 2 , on ***dit*** forme bilinéaire au lieu de forme 2-linéaire

**30097**: Considérons La première application ***est*** bilinéaire sur K mais ne est pas linéaire sur K2 alors que la seconde est linéaire sur K2 mais pas bilinéaire sur K. 1

**30103**: Considérons La première application est bilinéaire sur K mais ne ***est*** pas linéaire sur K2 alors que la seconde est linéaire sur K2 mais pas bilinéaire sur K. 1

**30112**: Considérons La première application est bilinéaire sur K mais ne est pas linéaire sur K2 alors que la seconde ***est*** linéaire sur K2 mais pas bilinéaire sur K. 1

**30126**: Le produit scalaire ***est*** une forme bilinéaire sur Rn

**30137**: L' application ***est*** une forme bilinéaire sur C 0 ( 0 , 1 , R )

**30162**: , fp ***sont*** des formes linéaires sur E , alors est une forme p - linéaire sur E. Soit E un K - espace vectoriel de dimension finie , soit p N , p 2 , et soit Lp ( E , K )

**30170**: , fp sont des formes linéaires sur E , alors ***est*** une forme p - linéaire sur E. Soit E un K - espace vectoriel de dimension finie , soit p N , p 2 , et soit Lp ( E , K )

**30206**: On ***dit*** que est symétrique si : Sp ( E , K ) l' ensemble des formes p - linéaires sur E symétriques On dit que est antisymétrique si : Ap ( E , K ) l' ensemble des formes p - linéaires sur E antisymétriques 1

**30208**: On dit que ***est*** symétrique si : Sp ( E , K ) l' ensemble des formes p - linéaires sur E symétriques On dit que est antisymétrique si : Ap ( E , K ) l' ensemble des formes p - linéaires sur E antisymétriques 1

**30229**: On dit que est symétrique si : Sp ( E , K ) l' ensemble des formes p - linéaires sur E symétriques On ***dit*** que est antisymétrique si : Ap ( E , K ) l' ensemble des formes p - linéaires sur E antisymétriques 1

**30231**: On dit que est symétrique si : Sp ( E , K ) l' ensemble des formes p - linéaires sur E symétriques On dit que ***est*** antisymétrique si : Ap ( E , K ) l' ensemble des formes p - linéaires sur E antisymétriques 1

**30260**: Dans l' espace euclidien R3 , si ***est*** une forme linéaire sur R3 , alors l' application : est bilinéaire antisymétrique

**30271**: Dans l' espace euclidien R3 , si est une forme linéaire sur R3 , alors l' application : ***est*** bilinéaire antisymétrique

**30295**: Dans l' espace euclidien R3 , l' application produit mixte définie par : ( où h , i ***est*** le produit scalaire euclidien ) est 3-linéaire antisymétrique

**30301**: Dans l' espace euclidien R3 , l' application produit mixte définie par : ( où h , i est le produit scalaire euclidien ) ***est*** 3-linéaire antisymétrique

**30318**: Sp ( E , K ) et Ap ( E , K ) ***sont*** des sous-espaces vectoriels de Lp ( E , K )

**30349**: D' après l' étude de Sp de la partie précédent , pour savoir si une forme p - linéaire ***est*** symétrique ou anti-symétrique , il suffit de se restreindre aux transpositions

**30355**: D' après l' étude de Sp de la partie précédent , pour savoir si une forme p - linéaire est symétrique ou anti-symétrique , il ***suffit*** de se restreindre aux transpositions

**30365**: En particulier : ***est*** symétrique si , et seulement si , échanger deux vecteurs ne change pas le signe ( ci-dessous seules les i - èmes et j - ièmes variables sont explicitées ) : est antisymétrique si , et seulement si , échanger deux vecteurs change le signe ( ci-dessous seules les i - èmes et j - ièmes variables sont explicitées ) : Soit Lp ( E , K )

**30377**: En particulier : est symétrique si , et seulement si , échanger deux vecteurs ne ***change*** pas le signe ( ci-dessous seules les i - èmes et j - ièmes variables sont explicitées ) : est antisymétrique si , et seulement si , échanger deux vecteurs change le signe ( ci-dessous seules les i - èmes et j - ièmes variables sont explicitées ) : Soit Lp ( E , K )

**30393**: En particulier : est symétrique si , et seulement si , échanger deux vecteurs ne change pas le signe ( ci-dessous seules les i - èmes et j - ièmes variables ***sont*** explicitées ) : est antisymétrique si , et seulement si , échanger deux vecteurs change le signe ( ci-dessous seules les i - èmes et j - ièmes variables sont explicitées ) : Soit Lp ( E , K )

**30397**: En particulier : est symétrique si , et seulement si , échanger deux vecteurs ne change pas le signe ( ci-dessous seules les i - èmes et j - ièmes variables sont explicitées ) : ***est*** antisymétrique si , et seulement si , échanger deux vecteurs change le signe ( ci-dessous seules les i - èmes et j - ièmes variables sont explicitées ) : Soit Lp ( E , K )

**30408**: En particulier : est symétrique si , et seulement si , échanger deux vecteurs ne change pas le signe ( ci-dessous seules les i - èmes et j - ièmes variables sont explicitées ) : est antisymétrique si , et seulement si , échanger deux vecteurs ***change*** le signe ( ci-dessous seules les i - èmes et j - ièmes variables sont explicitées ) : Soit Lp ( E , K )

**30423**: En particulier : est symétrique si , et seulement si , échanger deux vecteurs ne change pas le signe ( ci-dessous seules les i - èmes et j - ièmes variables sont explicitées ) : est antisymétrique si , et seulement si , échanger deux vecteurs change le signe ( ci-dessous seules les i - èmes et j - ièmes variables ***sont*** explicitées ) : Soit Lp ( E , K )

**30436**: Alors ***est*** antisymétrique si , et seulement si , elle est alternée , c' est - à - dire : Démonstration Soit ( x1 ,

**30445**: Alors est antisymétrique si , et seulement si , elle ***est*** alternée , c' est - à - dire : Démonstration Soit ( x1 ,

**30496**: , xp ) E p et soit ( i , j ) 1 , p2 tel que i 6 j ( ci-dessous , seules les i - èmes et j - ièmes variables ***sont*** explicitées )

**30500**: ***Supposons*** antisymétrique

**30505**: Alors : ***Supposons*** alternée

**30547**: ) donc ***est*** antisymétrique ( car on peut se restreindre aux transpositions , voir la remarque ci - dessus )

**30552**: ) donc est antisymétrique ( car on ***peut*** se restreindre aux transpositions , voir la remarque ci - dessus )

**30571**: Attention , ce résultat ne ***est*** plus vrai dans d' autres corps K que R ou C , pour lesquels 1 K 1 K 0 K

**30658**: , xp ) ne ***change*** pas en ajoutant à xi une combinaison linéaire de tous les xk avec k 1 ,

**30697**: , xp ) ***est*** liée , cela veut dire qu' il existe i 1 , p tel que xi soit une combinaison linéaire des Puisque est linéaire par rapport à sa i - ème variable : puisque xk apparaît deux fois dans (

**30701**: , xp ) est liée , cela ***veut*** dire qu' il existe i 1 , p tel que xi soit une combinaison linéaire des Puisque est linéaire par rapport à sa i - ème variable : puisque xk apparaît deux fois dans (

**30705**: , xp ) est liée , cela veut dire qu' il ***existe*** i 1 , p tel que xi soit une combinaison linéaire des Puisque est linéaire par rapport à sa i - ème variable : puisque xk apparaît deux fois dans (

**30719**: , xp ) est liée , cela veut dire qu' il existe i 1 , p tel que xi soit une combinaison linéaire des Puisque ***est*** linéaire par rapport à sa i - ème variable : puisque xk apparaît deux fois dans (

**30732**: , xp ) est liée , cela veut dire qu' il existe i 1 , p tel que xi soit une combinaison linéaire des Puisque est linéaire par rapport à sa i - ème variable : puisque xk ***apparaît*** deux fois dans (

**30756**: ) car k 6 i et ***est*** alternée

**30796**: Alors : Démonstration Comme ***est*** une forme p - linéaire : xin , n .ein Or ( ei1 ,

**30822**: , ein ) 0 dès que deux éléments ***sont*** égaux car est alternée ( voir la propriété 3.5 , page précédente )

**30825**: , ein ) 0 dès que deux éléments sont égaux car ***est*** alternée ( voir la propriété 3.5 , page précédente )

**30845**: Dans la somme ci - dessus , on ***peut*** garder uniquement les familles ( i1 ,

**30866**: , in ) d' indices tous distincts , ce qui ***correspond*** exactement aux permutations de 1 , n. On a donc , en utilisant le fait que est antisymétrique : Pour conclure , il reste à démontrer que est une forme n - linéaire alternée non nulle

**30875**: , in ) d' indices tous distincts , ce qui correspond exactement aux permutations de 1 , n. On ***a*** donc , en utilisant le fait que est antisymétrique : Pour conclure , il reste à démontrer que est une forme n - linéaire alternée non nulle

**30883**: , in ) d' indices tous distincts , ce qui correspond exactement aux permutations de 1 , n. On a donc , en utilisant le fait que ***est*** antisymétrique : Pour conclure , il reste à démontrer que est une forme n - linéaire alternée non nulle

**30890**: , in ) d' indices tous distincts , ce qui correspond exactement aux permutations de 1 , n. On a donc , en utilisant le fait que est antisymétrique : Pour conclure , il ***reste*** à démontrer que est une forme n - linéaire alternée non nulle

**30894**: , in ) d' indices tous distincts , ce qui correspond exactement aux permutations de 1 , n. On a donc , en utilisant le fait que est antisymétrique : Pour conclure , il reste à démontrer que ***est*** une forme n - linéaire alternée non nulle

**30914**: Le fait que D soit une forme n - linéaire ***est*** une simple vérification ( en exercice )

**30941**: Par le changement de variable k ( j ) : L' application s 7 ( 0 ) 1 ***est*** une bijection de Sn dans lui-même donc on peut faire le changement ce qui démontre que D est antisymétrique

**30950**: Par le changement de variable k ( j ) : L' application s 7 ( 0 ) 1 est une bijection de Sn dans lui-même donc on ***peut*** faire le changement ce qui démontre que D est antisymétrique

**30956**: Par le changement de variable k ( j ) : L' application s 7 ( 0 ) 1 est une bijection de Sn dans lui-même donc on peut faire le changement ce qui ***démontre*** que D est antisymétrique

**30959**: Par le changement de variable k ( j ) : L' application s 7 ( 0 ) 1 est une bijection de Sn dans lui-même donc on peut faire le changement ce qui démontre que D ***est*** antisymétrique

**30983**: Or ei , j 0 si i 6 j et ei , i 1 , donc les produits ci - dessus ***sont*** nuls pour tous les différents de l' identité

**31000**: Finalement , En particulier , D ne ***est*** pas la forme n - linéaire nulle

**31039**: , en ) et D une forme n - linéaire alternée non nulle , ce qui ***démontre*** que An ( E , K ) VectD d' où le résultat

**31057**: Plus généralement , on ***a*** les dimensions suivantes : et dim Ap ( E , K ) 1

**31073**: On ***remarque*** en particulier que la famille e?(i1 , ... , in ) est une base de Lp ( E , K ) , avec : 2

**31085**: On remarque en particulier que la famille e?(i1 , ... , in ) ***est*** une base de Lp ( E , K ) , avec : 2

**31105**: de même , la famille ***est*** une base de Sp ( E , K ) 3

**31124**: et la famille , lorsque p n ***est*** une base de Ap ( E , K )

**31173**: , en ) une base de E On ***appelle*** déterminant des vecteurs ( x1 ,

**31192**: , xn ) dans la base E et on ***note*** : a. Attention , le nombre de vecteurs doit être égal à la dimension de l' espace

**31201**: , xn ) dans la base E et on note : a. Attention , le nombre de vecteurs ***doit*** être égal à la dimension de l' espace

**31231**: Considérons E K2 muni de la base canonique E ( e1 , e2 ) ainsi que deux vecteurs u ( ***a*** , b ) a.e1 b.e2 et v ( c , d ) c.e1 d.e2 de E. Le groupe symétrique S2 a 2 ! 2 éléments , l' identité id1,2 et la transposition Soit E un K - espace vectoriel de dimension finie avec n dim E 1 , soit E ( e1 ,

**31252**: Considérons E K2 muni de la base canonique E ( e1 , e2 ) ainsi que deux vecteurs u ( a , b ) a.e1 b.e2 et v ( c , d ) c.e1 d.e2 de E. Le groupe symétrique S2 ***a*** 2 ! 2 éléments , l' identité id1,2 et la transposition Soit E un K - espace vectoriel de dimension finie avec n dim E 1 , soit E ( e1 ,

**31298**: detE ***est*** une forme n - linéaire antisymétrique sur E ( et donc alternée )

**31338**: , xp ) ne ***change*** pas en ajoutant à xi une combinaison linéaire de tous les xj avec j 1 ,

**31386**: , p , j 6 i. Démonstration Les trois propriétés ont déjà été démontrées lors de la démonstration du théorème 3.1 , page 157 et les deux dernières ***sont*** exactement la propriété 3.6 , page 156

**31426**: , en ) une base de c' ***est*** le cas : Démonstration 1

**31434**: C' ***est*** le point 2 de la propriété 3.7 , de la présente page en remarquant que detB est une forme n - linéaire alternée sur E. 2

**31451**: C' est le point 2 de la propriété 3.7 , de la présente page en remarquant que detB ***est*** une forme n - linéaire alternée sur E. 2

**31462**: ***Supposons*** que C soit une base de E. D' après le résultat précédent , nous avons Par contraposition , si la famille C ne est pas une base de E alors elle est liée donc detE ( c1 ,

**31477**: Supposons que C soit une base de E. D' après le résultat précédent , nous ***avons*** Par contraposition , si la famille C ne est pas une base de E alors elle est liée donc detE ( c1 ,

**31486**: Supposons que C soit une base de E. D' après le résultat précédent , nous avons Par contraposition , si la famille C ne ***est*** pas une base de E alors elle est liée donc detE ( c1 ,

**31494**: Supposons que C soit une base de E. D' après le résultat précédent , nous avons Par contraposition , si la famille C ne est pas une base de E alors elle ***est*** liée donc detE ( c1 ,

**31523**: Déterminant d' une matrice carrée Soit ***a*** A ai , j ( i , j)1,n2 Mn ( K ) , on appelle déterminant de A et on note Autrement dit , c' est le déterminant de la famille des n colonnes de A dans la base canonique de Mn,1 ( K )

**31538**: Déterminant d' une matrice carrée Soit a A ai , j ( i , j)1,n2 Mn ( K ) , on ***appelle*** déterminant de A et on note Autrement dit , c' est le déterminant de la famille des n colonnes de A dans la base canonique de Mn,1 ( K )

**31546**: Déterminant d' une matrice carrée Soit a A ai , j ( i , j)1,n2 Mn ( K ) , on appelle déterminant de A et on note Autrement ***dit*** , c' est le déterminant de la famille des n colonnes de A dans la base canonique de Mn,1 ( K )

**31549**: Déterminant d' une matrice carrée Soit a A ai , j ( i , j)1,n2 Mn ( K ) , on appelle déterminant de A et on note Autrement dit , c' ***est*** le déterminant de la famille des n colonnes de A dans la base canonique de Mn,1 ( K )

**31570**: ***a.*** Attention , il faut que la matrice soit carrée

**31574**: a. Attention , il ***faut*** que la matrice soit carrée

**31591**: On ***mémorise*** par : 1

**31598**: L' application ***est*** une forme n - linéaire antisymétrique sur Mn,1 ( K ) ( donc alternée )

**31622**: Pour toute ***A*** Mn ( K ) , A est inversible si , et seulement si , det(A ) 6 0

**31629**: Pour toute A Mn ( K ) , A ***est*** inversible si , et seulement si , det(A ) 6 0

**31644**: Si c' ***est*** le cas : Démonstration Notons C ( C1 ,

**31672**: C' ***est*** une conséquence immédiate du fait que detC est une forme n - linéaire alternée sur Mn,1 ( K )

**31680**: C' est une conséquence immédiate du fait que detC ***est*** une forme n - linéaire alternée sur Mn,1 ( K )

**31702**: ***Considérons*** l' application On vérifie alors que c' est une forme n - linéaire alternée

**31706**: Considérons l' application On ***vérifie*** alors que c' est une forme n - linéaire alternée

**31710**: Considérons l' application On vérifie alors que c' ***est*** une forme n - linéaire alternée

**31739**: , An ***sont*** les colonnes de A. Pour toute B Mn ( K ) , en note B1 ,

**31775**: Alors A ***est*** inversible si , et seulement si , ses colonnes A1 ,

**31792**: , An ***forment*** une base de d' où le résultat

**31803**: ***Considérons*** l' application On vérifie alors que c' est une forme n - linéaire alternée

**31807**: Considérons l' application On ***vérifie*** alors que c' est une forme n - linéaire alternée

**31811**: Considérons l' application On vérifie alors que c' ***est*** une forme n - linéaire alternée

**31837**: D' après le point 2 de la propriété 3.7 , page 159 , d' où , pour toute ***A*** Mn ( K ) de colonnes A1 ,

**31855**: , An : Le déterminant ne ***est*** pas linéaire ! Il est en général faux que det(A B ) det A det B. On a donc une information intéressante : si A et B sont deux matrices semblables de Mn ( K ) , alors : det A det B En effet , si il existe P GLn ( K ) telle que B P 1 A P : det(A ) det(P ) det(A ) La réciproque est fausse ( par exemple , une matrice non inversible est de déterminant nulle , mais ne est pas semblable à la matrice nulle sauf si elle est elle-même nulle )

**31860**: , An : Le déterminant ne est pas linéaire ! Il ***est*** en général faux que det(A B ) det A det B. On a donc une information intéressante : si A et B sont deux matrices semblables de Mn ( K ) , alors : det A det B En effet , si il existe P GLn ( K ) telle que B P 1 A P : det(A ) det(P ) det(A ) La réciproque est fausse ( par exemple , une matrice non inversible est de déterminant nulle , mais ne est pas semblable à la matrice nulle sauf si elle est elle-même nulle )

**31873**: , An : Le déterminant ne est pas linéaire ! Il est en général faux que det(A B ) det A det B. On ***a*** donc une information intéressante : si A et B sont deux matrices semblables de Mn ( K ) , alors : det A det B En effet , si il existe P GLn ( K ) telle que B P 1 A P : det(A ) det(P ) det(A ) La réciproque est fausse ( par exemple , une matrice non inversible est de déterminant nulle , mais ne est pas semblable à la matrice nulle sauf si elle est elle-même nulle )

**31883**: , An : Le déterminant ne est pas linéaire ! Il est en général faux que det(A B ) det A det B. On a donc une information intéressante : si A et B ***sont*** deux matrices semblables de Mn ( K ) , alors : det A det B En effet , si il existe P GLn ( K ) telle que B P 1 A P : det(A ) det(P ) det(A ) La réciproque est fausse ( par exemple , une matrice non inversible est de déterminant nulle , mais ne est pas semblable à la matrice nulle sauf si elle est elle-même nulle )

**31904**: , An : Le déterminant ne est pas linéaire ! Il est en général faux que det(A B ) det A det B. On a donc une information intéressante : si A et B sont deux matrices semblables de Mn ( K ) , alors : det A det B En effet , si il ***existe*** P GLn ( K ) telle que B P 1 A P : det(A ) det(P ) det(A ) La réciproque est fausse ( par exemple , une matrice non inversible est de déterminant nulle , mais ne est pas semblable à la matrice nulle sauf si elle est elle-même nulle )

**31926**: , An : Le déterminant ne est pas linéaire ! Il est en général faux que det(A B ) det A det B. On a donc une information intéressante : si A et B sont deux matrices semblables de Mn ( K ) , alors : det A det B En effet , si il existe P GLn ( K ) telle que B P 1 A P : det(A ) det(P ) det(A ) La réciproque ***est*** fausse ( par exemple , une matrice non inversible est de déterminant nulle , mais ne est pas semblable à la matrice nulle sauf si elle est elle-même nulle )

**31936**: , An : Le déterminant ne est pas linéaire ! Il est en général faux que det(A B ) det A det B. On a donc une information intéressante : si A et B sont deux matrices semblables de Mn ( K ) , alors : det A det B En effet , si il existe P GLn ( K ) telle que B P 1 A P : det(A ) det(P ) det(A ) La réciproque est fausse ( par exemple , une matrice non inversible ***est*** de déterminant nulle , mais ne est pas semblable à la matrice nulle sauf si elle est elle-même nulle )

**31943**: , An : Le déterminant ne est pas linéaire ! Il est en général faux que det(A B ) det A det B. On a donc une information intéressante : si A et B sont deux matrices semblables de Mn ( K ) , alors : det A det B En effet , si il existe P GLn ( K ) telle que B P 1 A P : det(A ) det(P ) det(A ) La réciproque est fausse ( par exemple , une matrice non inversible est de déterminant nulle , mais ne ***est*** pas semblable à la matrice nulle sauf si elle est elle-même nulle )

**31953**: , An : Le déterminant ne est pas linéaire ! Il est en général faux que det(A B ) det A det B. On a donc une information intéressante : si A et B sont deux matrices semblables de Mn ( K ) , alors : det A det B En effet , si il existe P GLn ( K ) telle que B P 1 A P : det(A ) det(P ) det(A ) La réciproque est fausse ( par exemple , une matrice non inversible est de déterminant nulle , mais ne est pas semblable à la matrice nulle sauf si elle ***est*** elle-même nulle )

**31989**: Le déterminant de u , noté det u , ***est*** défini par det u det MatE ( u ) où E est une base quelconque de E Cette définition a bien un sens : les matrices de u dans deux bases de E différentes sont semblables , donc elles ont même déterminant ( remarque 3.9 , page précédente )

**32001**: Le déterminant de u , noté det u , est défini par det u det MatE ( u ) où E ***est*** une base quelconque de E Cette définition a bien un sens : les matrices de u dans deux bases de E différentes sont semblables , donc elles ont même déterminant ( remarque 3.9 , page précédente )

**32009**: Le déterminant de u , noté det u , est défini par det u det MatE ( u ) où E est une base quelconque de E Cette définition ***a*** bien un sens : les matrices de u dans deux bases de E différentes sont semblables , donc elles ont même déterminant ( remarque 3.9 , page précédente )

**32024**: Le déterminant de u , noté det u , est défini par det u det MatE ( u ) où E est une base quelconque de E Cette définition a bien un sens : les matrices de u dans deux bases de E différentes ***sont*** semblables , donc elles ont même déterminant ( remarque 3.9 , page précédente )

**32029**: Le déterminant de u , noté det u , est défini par det u det MatE ( u ) où E est une base quelconque de E Cette définition a bien un sens : les matrices de u dans deux bases de E différentes sont semblables , donc elles ***ont*** même déterminant ( remarque 3.9 , page précédente )

**32108**: u ***est*** inversible si , et seulement si , det(u ) 6 0

**32123**: Si c' ***est*** le cas det(u1 ) ( det u)1 Démonstration det(.u ) det(.U ) n det U n det u 3

**32145**: u ***est*** inversible si , et seulement si , U est inversible si , et seulement si , det u det U 6 0

**32154**: u est inversible si , et seulement si , U ***est*** inversible si , et seulement si , det u det U 6 0

**32171**: Si c' ***est*** le cas , U est inversible donc Méthodes de calcul de déterminants La formule définissant le déterminant et faisant intervenir Sn est utile théoriquement a mais inutilisable en pratique dès que n est plus grand que 4 ou 5

**32176**: Si c' est le cas , U ***est*** inversible donc Méthodes de calcul de déterminants La formule définissant le déterminant et faisant intervenir Sn est utile théoriquement a mais inutilisable en pratique dès que n est plus grand que 4 ou 5

**32193**: Si c' est le cas , U est inversible donc Méthodes de calcul de déterminants La formule définissant le déterminant et faisant intervenir Sn ***est*** utile théoriquement a mais inutilisable en pratique dès que n est plus grand que 4 ou 5

**32204**: Si c' est le cas , U est inversible donc Méthodes de calcul de déterminants La formule définissant le déterminant et faisant intervenir Sn est utile théoriquement a mais inutilisable en pratique dès que n ***est*** plus grand que 4 ou 5

**32217**: En effet , il y ***a*** n ! permutations de 1 , n dans lui-même

**32243**: Ainsi , pour calculer un déterminant d' une matrice carrée d' ordre 5 , il ***faut*** effectuer Pire encore , 60 ! ' 1082 est supérieur au nombre d' atomes observables dans l' univers , alors que les problèmes de mathématiques appliquées et d' ingénierie moderne nécessitent de traiter des matrices qui ont des centaines de milliers voire des millions de lignes ... Il faut donc trouver des méthodes plus efficaces

**32252**: Ainsi , pour calculer un déterminant d' une matrice carrée d' ordre 5 , il faut effectuer Pire encore , 60 ! ' 1082 ***est*** supérieur au nombre d' atomes observables dans l' univers , alors que les problèmes de mathématiques appliquées et d' ingénierie moderne nécessitent de traiter des matrices qui ont des centaines de milliers voire des millions de lignes ... Il faut donc trouver des méthodes plus efficaces

**32274**: Ainsi , pour calculer un déterminant d' une matrice carrée d' ordre 5 , il faut effectuer Pire encore , 60 ! ' 1082 est supérieur au nombre d' atomes observables dans l' univers , alors que les problèmes de mathématiques appliquées et d' ingénierie moderne ***nécessitent*** de traiter des matrices qui ont des centaines de milliers voire des millions de lignes ... Il faut donc trouver des méthodes plus efficaces

**32280**: Ainsi , pour calculer un déterminant d' une matrice carrée d' ordre 5 , il faut effectuer Pire encore , 60 ! ' 1082 est supérieur au nombre d' atomes observables dans l' univers , alors que les problèmes de mathématiques appliquées et d' ingénierie moderne nécessitent de traiter des matrices qui ***ont*** des centaines de milliers voire des millions de lignes ... Il faut donc trouver des méthodes plus efficaces

**32292**: Ainsi , pour calculer un déterminant d' une matrice carrée d' ordre 5 , il faut effectuer Pire encore , 60 ! ' 1082 est supérieur au nombre d' atomes observables dans l' univers , alors que les problèmes de mathématiques appliquées et d' ingénierie moderne nécessitent de traiter des matrices qui ont des centaines de milliers voire des millions de lignes ... Il ***faut*** donc trouver des méthodes plus efficaces

**32300**: ***a.*** Par exemple pour démontrer que ( ai , j ) ( i , j)1,n2 7 det ai , j ( i , j)1,n2 est polynomiale donc de classe C

**32317**: a. Par exemple pour démontrer que ( ai , j ) ( i , j)1,n2 7 det ***ai*** , j ( i , j)1,n2 est polynomiale donc de classe C

**32324**: a. Par exemple pour démontrer que ( ai , j ) ( i , j)1,n2 7 det ai , j ( i , j)1,n2 ***est*** polynomiale donc de classe C

**32351**: Si ***A*** est triangulaire supérieure ( ou triangulaire inférieure , ou diagonale ) , alors Démonstration Supposons que A soit triangulaire supérieure

**32352**: Si A ***est*** triangulaire supérieure ( ou triangulaire inférieure , ou diagonale ) , alors Démonstration Supposons que A soit triangulaire supérieure

**32374**: On ***a*** : Pour Sn , si il existe j 1 , n tel que ( j ) j , alors a(j),j 0 car A est triangulaire supérieure

**32381**: On a : Pour Sn , si il ***existe*** j 1 , n tel que ( j ) j , alors a(j),j 0 car A est triangulaire supérieure

**32398**: On a : Pour Sn , si il existe j 1 , n tel que ( j ) j , alors a(j),j 0 car A ***est*** triangulaire supérieure

**32411**: Dans l' expression ci - dessus , il ne ***reste*** plus que les termes correspondant à une permutation telle que ( j ) j pour tout j 1 , n. Démontrons qu' une telle permutation est l' identité

**32432**: Dans l' expression ci - dessus , il ne reste plus que les termes correspondant à une permutation telle que ( j ) j pour tout j 1 , n. ***Démontrons*** qu' une telle permutation est l' identité

**32437**: Dans l' expression ci - dessus , il ne reste plus que les termes correspondant à une permutation telle que ( j ) j pour tout j 1 , n. Démontrons qu' une telle permutation ***est*** l' identité

**32441**: ***Supposons*** que s1,k soit l' identité pour un k 1 , n. On a d' une part ( k 1 ) k 1 mais comme est injective , ( k 1 ) 1 , k d' où ( k 1 ) k 1 , c' est - à - dire que 1,k1 est l' identité

**32454**: Supposons que s1,k soit l' identité pour un k 1 , n. On ***a*** d' une part ( k 1 ) k 1 mais comme est injective , ( k 1 ) 1 , k d' où ( k 1 ) k 1 , c' est - à - dire que 1,k1 est l' identité

**32466**: Supposons que s1,k soit l' identité pour un k 1 , n. On a d' une part ( k 1 ) k 1 mais comme ***est*** injective , ( k 1 ) 1 , k d' où ( k 1 ) k 1 , c' est - à - dire que 1,k1 est l' identité

**32493**: Supposons que s1,k soit l' identité pour un k 1 , n. On a d' une part ( k 1 ) k 1 mais comme est injective , ( k 1 ) 1 , k d' où ( k 1 ) k 1 , c' est - à - dire que 1,k1 ***est*** l' identité

**32502**: Par principe de récurrence , ***est*** l' identité

**32517**: Dans la formule du déterminant ci - dessus , il ne ***reste*** donc plus que le terme correspondant à id1,n , d' où Pour une matrice triangulaire inférieure , on utilise le fait que sa transposée est triangulaire supérieure

**32536**: Dans la formule du déterminant ci - dessus , il ne reste donc plus que le terme correspondant à id1,n , d' où Pour une matrice triangulaire inférieure , on ***utilise*** le fait que sa transposée est triangulaire supérieure

**32542**: Dans la formule du déterminant ci - dessus , il ne reste donc plus que le terme correspondant à id1,n , d' où Pour une matrice triangulaire inférieure , on utilise le fait que sa transposée ***est*** triangulaire supérieure

**32555**: det(A ) det(D ) Démonstration Considérons l' application On ***vérifie*** que c' est une forme n1 -linéaire alternée sur Mn1 , 1 ( K)p donc , en notant C ( C1 ,

**32558**: det(A ) det(D ) Démonstration Considérons l' application On vérifie que c' ***est*** une forme n1 -linéaire alternée sur Mn1 , 1 ( K)p donc , en notant C ( C1 ,

**32604**: , An1 ***sont*** les colonnes d' une matrice A Mn1 ( K ) : Considérons maintenant l' application 0 ) la base On vérifie que c' est une forme n2 -linéaire alternée sur Mn2 , 1 ( K)n2 donc , en notant C 0 ( C10 ,

**32625**: , An1 sont les colonnes d' une matrice A Mn1 ( K ) : Considérons maintenant l' application 0 ) la base On ***vérifie*** que c' est une forme n2 -linéaire alternée sur Mn2 , 1 ( K)n2 donc , en notant C 0 ( C10 ,

**32628**: , An1 sont les colonnes d' une matrice A Mn1 ( K ) : Considérons maintenant l' application 0 ) la base On vérifie que c' ***est*** une forme n2 -linéaire alternée sur Mn2 , 1 ( K)n2 donc , en notant C 0 ( C10 ,

**32672**: , Bn2 ***sont*** les colonnes d' une matrice B Mn2 , 1 ( K ) : det(B ) det(A ) car c' est une matrice triangulaire supérieure ( voir la propriété 3.12 , page précédente ) , d' où le résultat

**32692**: , Bn2 sont les colonnes d' une matrice B Mn2 , 1 ( K ) : det(B ) det(A ) car c' ***est*** une matrice triangulaire supérieure ( voir la propriété 3.12 , page précédente ) , d' où le résultat

**32715**: Il ne y ***a*** pas de formule générale pour les déterminants des matrices - blocs

**32739**: Par exemple , si A , B , C et D ***sont*** dans Mn ( K ) , alors , si D est inversible a : mais cela ne fonctionne pas lorsque D ne est pas inversible

**32750**: Par exemple , si A , B , C et D sont dans Mn ( K ) , alors , si D ***est*** inversible a : mais cela ne fonctionne pas lorsque D ne est pas inversible

**32752**: Par exemple , si A , B , C et D sont dans Mn ( K ) , alors , si D est inversible ***a*** : mais cela ne fonctionne pas lorsque D ne est pas inversible

**32757**: Par exemple , si A , B , C et D sont dans Mn ( K ) , alors , si D est inversible a : mais cela ne ***fonctionne*** pas lorsque D ne est pas inversible

**32762**: Par exemple , si A , B , C et D sont dans Mn ( K ) , alors , si D est inversible a : mais cela ne fonctionne pas lorsque D ne ***est*** pas inversible

**32766**: ***a.*** Exercice conseillé

**32804**: les opérations Li Li .Lj et Ci Ci .Cj ( transvections ) ne ***modifient*** pas la valeur du déterminant ( d' après le point 5 de la propriété 3.7 , page 159 ) 2

**32836**: les opérations Li .Li et Ci .Ci ( dilatations ) ***multiplient*** le déterminant par ( car le détermiPage 164292 nant est n - linéaire par rapport aux colonnes et aux lignes ) 3

**32846**: les opérations Li .Li et Ci .Ci ( dilatations ) multiplient le déterminant par ( car le détermiPage 164292 nant ***est*** n - linéaire par rapport aux colonnes et aux lignes ) 3

**32870**: les opérations Li Lj et Ci Cj ( échanges ) ***multiplient*** le déterminant par 1 ( car le déterminant est n - linéaire alternée par rapport aux colonnes et aux lignes )

**32879**: les opérations Li Lj et Ci Cj ( échanges ) multiplient le déterminant par 1 ( car le déterminant ***est*** n - linéaire alternée par rapport aux colonnes et aux lignes )

**32897**: Ainsi , cet algorithme ***permet*** de calculer des déterminants en se ramenant à des matrices triangulaires supérieures

**32912**: On ***peut*** démontrer que le calcul du déterminant d' une matrice carrée de taille n par la méthode du pivot de Gauss nécessite un nombre d' opération de l' ordre de n3 , ce qui est bien meilleur que n!. Soit à calculer ( n 2 ) : En faisant les transvections successives ( qui conservent le déterminant ) : On fait apparaître deux lignes identiques , donc : Si a , b , c , d , e , f , g , h et i sont dans K , alors e f aei dhc gbf gec ahf dbi que l' on peut mémoriser par la règle de Sarrus : On remarque , qu' avant développement , Wxmaxima nous proposait une formule non développée , qu' on peut ré-écrire : Que se passe -t -il pour n plus grand ? On reconnaît l' expression : Soit A Mn ( K ) , soit ( k , ) 1 , n , on note a Ak , la matrice de Mn1 ( K ) obtenue à partir de A en supprimant sa k - ième ligne et sa -ième colonne

**32933**: On peut démontrer que le calcul du déterminant d' une matrice carrée de taille n par la méthode du pivot de Gauss ***nécessite*** un nombre d' opération de l' ordre de n3 , ce qui est bien meilleur que n!. Soit à calculer ( n 2 ) : En faisant les transvections successives ( qui conservent le déterminant ) : On fait apparaître deux lignes identiques , donc : Si a , b , c , d , e , f , g , h et i sont dans K , alors e f aei dhc gbf gec ahf dbi que l' on peut mémoriser par la règle de Sarrus : On remarque , qu' avant développement , Wxmaxima nous proposait une formule non développée , qu' on peut ré-écrire : Que se passe -t -il pour n plus grand ? On reconnaît l' expression : Soit A Mn ( K ) , soit ( k , ) 1 , n , on note a Ak , la matrice de Mn1 ( K ) obtenue à partir de A en supprimant sa k - ième ligne et sa -ième colonne

**32946**: On peut démontrer que le calcul du déterminant d' une matrice carrée de taille n par la méthode du pivot de Gauss nécessite un nombre d' opération de l' ordre de n3 , ce qui ***est*** bien meilleur que n!. Soit à calculer ( n 2 ) : En faisant les transvections successives ( qui conservent le déterminant ) : On fait apparaître deux lignes identiques , donc : Si a , b , c , d , e , f , g , h et i sont dans K , alors e f aei dhc gbf gec ahf dbi que l' on peut mémoriser par la règle de Sarrus : On remarque , qu' avant développement , Wxmaxima nous proposait une formule non développée , qu' on peut ré-écrire : Que se passe -t -il pour n plus grand ? On reconnaît l' expression : Soit A Mn ( K ) , soit ( k , ) 1 , n , on note a Ak , la matrice de Mn1 ( K ) obtenue à partir de A en supprimant sa k - ième ligne et sa -ième colonne

**32966**: On peut démontrer que le calcul du déterminant d' une matrice carrée de taille n par la méthode du pivot de Gauss nécessite un nombre d' opération de l' ordre de n3 , ce qui est bien meilleur que n!. Soit à calculer ( n 2 ) : En faisant les transvections successives ( qui ***conservent*** le déterminant ) : On fait apparaître deux lignes identiques , donc : Si a , b , c , d , e , f , g , h et i sont dans K , alors e f aei dhc gbf gec ahf dbi que l' on peut mémoriser par la règle de Sarrus : On remarque , qu' avant développement , Wxmaxima nous proposait une formule non développée , qu' on peut ré-écrire : Que se passe -t -il pour n plus grand ? On reconnaît l' expression : Soit A Mn ( K ) , soit ( k , ) 1 , n , on note a Ak , la matrice de Mn1 ( K ) obtenue à partir de A en supprimant sa k - ième ligne et sa -ième colonne

**32972**: On peut démontrer que le calcul du déterminant d' une matrice carrée de taille n par la méthode du pivot de Gauss nécessite un nombre d' opération de l' ordre de n3 , ce qui est bien meilleur que n!. Soit à calculer ( n 2 ) : En faisant les transvections successives ( qui conservent le déterminant ) : On ***fait*** apparaître deux lignes identiques , donc : Si a , b , c , d , e , f , g , h et i sont dans K , alors e f aei dhc gbf gec ahf dbi que l' on peut mémoriser par la règle de Sarrus : On remarque , qu' avant développement , Wxmaxima nous proposait une formule non développée , qu' on peut ré-écrire : Que se passe -t -il pour n plus grand ? On reconnaît l' expression : Soit A Mn ( K ) , soit ( k , ) 1 , n , on note a Ak , la matrice de Mn1 ( K ) obtenue à partir de A en supprimant sa k - ième ligne et sa -ième colonne

**32981**: On peut démontrer que le calcul du déterminant d' une matrice carrée de taille n par la méthode du pivot de Gauss nécessite un nombre d' opération de l' ordre de n3 , ce qui est bien meilleur que n!. Soit à calculer ( n 2 ) : En faisant les transvections successives ( qui conservent le déterminant ) : On fait apparaître deux lignes identiques , donc : Si ***a*** , b , c , d , e , f , g , h et i sont dans K , alors e f aei dhc gbf gec ahf dbi que l' on peut mémoriser par la règle de Sarrus : On remarque , qu' avant développement , Wxmaxima nous proposait une formule non développée , qu' on peut ré-écrire : Que se passe -t -il pour n plus grand ? On reconnaît l' expression : Soit A Mn ( K ) , soit ( k , ) 1 , n , on note a Ak , la matrice de Mn1 ( K ) obtenue à partir de A en supprimant sa k - ième ligne et sa -ième colonne

**32998**: On peut démontrer que le calcul du déterminant d' une matrice carrée de taille n par la méthode du pivot de Gauss nécessite un nombre d' opération de l' ordre de n3 , ce qui est bien meilleur que n!. Soit à calculer ( n 2 ) : En faisant les transvections successives ( qui conservent le déterminant ) : On fait apparaître deux lignes identiques , donc : Si a , b , c , d , e , f , g , h et i ***sont*** dans K , alors e f aei dhc gbf gec ahf dbi que l' on peut mémoriser par la règle de Sarrus : On remarque , qu' avant développement , Wxmaxima nous proposait une formule non développée , qu' on peut ré-écrire : Que se passe -t -il pour n plus grand ? On reconnaît l' expression : Soit A Mn ( K ) , soit ( k , ) 1 , n , on note a Ak , la matrice de Mn1 ( K ) obtenue à partir de A en supprimant sa k - ième ligne et sa -ième colonne

**33014**: On peut démontrer que le calcul du déterminant d' une matrice carrée de taille n par la méthode du pivot de Gauss nécessite un nombre d' opération de l' ordre de n3 , ce qui est bien meilleur que n!. Soit à calculer ( n 2 ) : En faisant les transvections successives ( qui conservent le déterminant ) : On fait apparaître deux lignes identiques , donc : Si a , b , c , d , e , f , g , h et i sont dans K , alors e f aei dhc gbf gec ahf dbi que l' on ***peut*** mémoriser par la règle de Sarrus : On remarque , qu' avant développement , Wxmaxima nous proposait une formule non développée , qu' on peut ré-écrire : Que se passe -t -il pour n plus grand ? On reconnaît l' expression : Soit A Mn ( K ) , soit ( k , ) 1 , n , on note a Ak , la matrice de Mn1 ( K ) obtenue à partir de A en supprimant sa k - ième ligne et sa -ième colonne

**33023**: On peut démontrer que le calcul du déterminant d' une matrice carrée de taille n par la méthode du pivot de Gauss nécessite un nombre d' opération de l' ordre de n3 , ce qui est bien meilleur que n!. Soit à calculer ( n 2 ) : En faisant les transvections successives ( qui conservent le déterminant ) : On fait apparaître deux lignes identiques , donc : Si a , b , c , d , e , f , g , h et i sont dans K , alors e f aei dhc gbf gec ahf dbi que l' on peut mémoriser par la règle de Sarrus : On ***remarque*** , qu' avant développement , Wxmaxima nous proposait une formule non développée , qu' on peut ré-écrire : Que se passe -t -il pour n plus grand ? On reconnaît l' expression : Soit A Mn ( K ) , soit ( k , ) 1 , n , on note a Ak , la matrice de Mn1 ( K ) obtenue à partir de A en supprimant sa k - ième ligne et sa -ième colonne

**33039**: On peut démontrer que le calcul du déterminant d' une matrice carrée de taille n par la méthode du pivot de Gauss nécessite un nombre d' opération de l' ordre de n3 , ce qui est bien meilleur que n!. Soit à calculer ( n 2 ) : En faisant les transvections successives ( qui conservent le déterminant ) : On fait apparaître deux lignes identiques , donc : Si a , b , c , d , e , f , g , h et i sont dans K , alors e f aei dhc gbf gec ahf dbi que l' on peut mémoriser par la règle de Sarrus : On remarque , qu' avant développement , Wxmaxima nous proposait une formule non développée , qu' on ***peut*** ré-écrire : Que se passe -t -il pour n plus grand ? On reconnaît l' expression : Soit A Mn ( K ) , soit ( k , ) 1 , n , on note a Ak , la matrice de Mn1 ( K ) obtenue à partir de A en supprimant sa k - ième ligne et sa -ième colonne

**33044**: On peut démontrer que le calcul du déterminant d' une matrice carrée de taille n par la méthode du pivot de Gauss nécessite un nombre d' opération de l' ordre de n3 , ce qui est bien meilleur que n!. Soit à calculer ( n 2 ) : En faisant les transvections successives ( qui conservent le déterminant ) : On fait apparaître deux lignes identiques , donc : Si a , b , c , d , e , f , g , h et i sont dans K , alors e f aei dhc gbf gec ahf dbi que l' on peut mémoriser par la règle de Sarrus : On remarque , qu' avant développement , Wxmaxima nous proposait une formule non développée , qu' on peut ré-écrire : Que se ***passe*** -t -il pour n plus grand ? On reconnaît l' expression : Soit A Mn ( K ) , soit ( k , ) 1 , n , on note a Ak , la matrice de Mn1 ( K ) obtenue à partir de A en supprimant sa k - ième ligne et sa -ième colonne

**33053**: On peut démontrer que le calcul du déterminant d' une matrice carrée de taille n par la méthode du pivot de Gauss nécessite un nombre d' opération de l' ordre de n3 , ce qui est bien meilleur que n!. Soit à calculer ( n 2 ) : En faisant les transvections successives ( qui conservent le déterminant ) : On fait apparaître deux lignes identiques , donc : Si a , b , c , d , e , f , g , h et i sont dans K , alors e f aei dhc gbf gec ahf dbi que l' on peut mémoriser par la règle de Sarrus : On remarque , qu' avant développement , Wxmaxima nous proposait une formule non développée , qu' on peut ré-écrire : Que se passe -t -il pour n plus grand ? On ***reconnaît*** l' expression : Soit A Mn ( K ) , soit ( k , ) 1 , n , on note a Ak , la matrice de Mn1 ( K ) obtenue à partir de A en supprimant sa k - ième ligne et sa -ième colonne

**33074**: On peut démontrer que le calcul du déterminant d' une matrice carrée de taille n par la méthode du pivot de Gauss nécessite un nombre d' opération de l' ordre de n3 , ce qui est bien meilleur que n!. Soit à calculer ( n 2 ) : En faisant les transvections successives ( qui conservent le déterminant ) : On fait apparaître deux lignes identiques , donc : Si a , b , c , d , e , f , g , h et i sont dans K , alors e f aei dhc gbf gec ahf dbi que l' on peut mémoriser par la règle de Sarrus : On remarque , qu' avant développement , Wxmaxima nous proposait une formule non développée , qu' on peut ré-écrire : Que se passe -t -il pour n plus grand ? On reconnaît l' expression : Soit A Mn ( K ) , soit ( k , ) 1 , n , on ***note*** a Ak , la matrice de Mn1 ( K ) obtenue à partir de A en supprimant sa k - ième ligne et sa -ième colonne

**33075**: On peut démontrer que le calcul du déterminant d' une matrice carrée de taille n par la méthode du pivot de Gauss nécessite un nombre d' opération de l' ordre de n3 , ce qui est bien meilleur que n!. Soit à calculer ( n 2 ) : En faisant les transvections successives ( qui conservent le déterminant ) : On fait apparaître deux lignes identiques , donc : Si a , b , c , d , e , f , g , h et i sont dans K , alors e f aei dhc gbf gec ahf dbi que l' on peut mémoriser par la règle de Sarrus : On remarque , qu' avant développement , Wxmaxima nous proposait une formule non développée , qu' on peut ré-écrire : Que se passe -t -il pour n plus grand ? On reconnaît l' expression : Soit A Mn ( K ) , soit ( k , ) 1 , n , on note ***a*** Ak , la matrice de Mn1 ( K ) obtenue à partir de A en supprimant sa k - ième ligne et sa -ième colonne

**33110**: Le déterminant de Ak , s' ***appelle*** mineur d' indice ( k , ) de A 2

**33123**: on ***appelle*** cofacteur d' indice ( k , ) de A l' expression : Cofacteurk , ( A ) ( 1)k det Ak , ( A ) 3

**33152**: on ***appelle*** comatrice de A la matrice de Mn ( K ) définie par : Cofacteur1,1 ( A ) Com(A ) Cofacteuri , j ( A)(i , j)1,n2 Cofacteurn,1 ( A ) a. Attention , cette notation peut également désigner le coefficient en ( k , ) de A. Cofacteurn,1 ( A ) Cofacteurn , n ( A ) ACom(A ) ) A Pour retrouver les signes dans la comatrice , il suffit d' alterner les signes lorsque qu' on se déplace de case en case dans la comatrice : Proposition 3.1 Développement selon une ligne ou une colonne 1

**33183**: on appelle comatrice de A la matrice de Mn ( K ) définie par : Cofacteur1,1 ( A ) Com(A ) Cofacteuri , j ( A)(i , j)1,n2 Cofacteurn,1 ( A ) ***a.*** Attention , cette notation peut également désigner le coefficient en ( k , ) de A. Cofacteurn,1 ( A ) Cofacteurn , n ( A ) ACom(A ) ) A Pour retrouver les signes dans la comatrice , il suffit d' alterner les signes lorsque qu' on se déplace de case en case dans la comatrice : Proposition 3.1 Développement selon une ligne ou une colonne 1

**33188**: on appelle comatrice de A la matrice de Mn ( K ) définie par : Cofacteur1,1 ( A ) Com(A ) Cofacteuri , j ( A)(i , j)1,n2 Cofacteurn,1 ( A ) a. Attention , cette notation ***peut*** également désigner le coefficient en ( k , ) de A. Cofacteurn,1 ( A ) Cofacteurn , n ( A ) ACom(A ) ) A Pour retrouver les signes dans la comatrice , il suffit d' alterner les signes lorsque qu' on se déplace de case en case dans la comatrice : Proposition 3.1 Développement selon une ligne ou une colonne 1

**33223**: on appelle comatrice de A la matrice de Mn ( K ) définie par : Cofacteur1,1 ( A ) Com(A ) Cofacteuri , j ( A)(i , j)1,n2 Cofacteurn,1 ( A ) a. Attention , cette notation peut également désigner le coefficient en ( k , ) de A. Cofacteurn,1 ( A ) Cofacteurn , n ( A ) ACom(A ) ) A Pour retrouver les signes dans la comatrice , il ***suffit*** d' alterner les signes lorsque qu' on se déplace de case en case dans la comatrice : Proposition 3.1 Développement selon une ligne ou une colonne 1

**33264**: , n , on ***a*** le développement suivant la i - ème ligne : ai , j Cofacteuri , j ( A ) 2

**33297**: , n , on ***a*** le développement suivant la j - ième colonne : ai , j Cofacteuri , j ( A ) Démonstration Notons A1 ,

**33358**: Soit j 1 , n. Comme le déterminant ***est*** une forme n - linéaire par rapport aux colonnes : En échangeant la j - ième colonne avec la j 1-ième , puis la j 1-ième avec la j 1-ième jusqu' à ce que la première colonne soit Ci , puisqu' il y a au total j 1 échanges : En échangeant la i - ème ligne avec la i 1-ième , puis la i 1-ième avec la i 1-ième jusqu' à ce que la première ligne soit 1 0 0 , puisqu' il y a au total i1 échanges , on a par la formule du déterminant d' une matrice - blocs ( 1)ij ai , j det(1 ) det Ai , j ai , j Cofacteuri , j ( A ) La formule du développement selon une colonne se démontre de la même façon ( ou en utilisant la transposée )

**33402**: Soit j 1 , n. Comme le déterminant est une forme n - linéaire par rapport aux colonnes : En échangeant la j - ième colonne avec la j 1-ième , puis la j 1-ième avec la j 1-ième jusqu' à ce que la première colonne soit Ci , puisqu' il y ***a*** au total j 1 échanges : En échangeant la i - ème ligne avec la i 1-ième , puis la i 1-ième avec la i 1-ième jusqu' à ce que la première ligne soit 1 0 0 , puisqu' il y a au total i1 échanges , on a par la formule du déterminant d' une matrice - blocs ( 1)ij ai , j det(1 ) det Ai , j ai , j Cofacteuri , j ( A ) La formule du développement selon une colonne se démontre de la même façon ( ou en utilisant la transposée )

**33444**: Soit j 1 , n. Comme le déterminant est une forme n - linéaire par rapport aux colonnes : En échangeant la j - ième colonne avec la j 1-ième , puis la j 1-ième avec la j 1-ième jusqu' à ce que la première colonne soit Ci , puisqu' il y a au total j 1 échanges : En échangeant la i - ème ligne avec la i 1-ième , puis la i 1-ième avec la i 1-ième jusqu' à ce que la première ligne soit 1 0 0 , puisqu' il y ***a*** au total i1 échanges , on a par la formule du déterminant d' une matrice - blocs ( 1)ij ai , j det(1 ) det Ai , j ai , j Cofacteuri , j ( A ) La formule du développement selon une colonne se démontre de la même façon ( ou en utilisant la transposée )

**33451**: Soit j 1 , n. Comme le déterminant est une forme n - linéaire par rapport aux colonnes : En échangeant la j - ième colonne avec la j 1-ième , puis la j 1-ième avec la j 1-ième jusqu' à ce que la première colonne soit Ci , puisqu' il y a au total j 1 échanges : En échangeant la i - ème ligne avec la i 1-ième , puis la i 1-ième avec la i 1-ième jusqu' à ce que la première ligne soit 1 0 0 , puisqu' il y a au total i1 échanges , on ***a*** par la formule du déterminant d' une matrice - blocs ( 1)ij ai , j det(1 ) det Ai , j ai , j Cofacteuri , j ( A ) La formule du développement selon une colonne se démontre de la même façon ( ou en utilisant la transposée )

**33490**: Soit j 1 , n. Comme le déterminant est une forme n - linéaire par rapport aux colonnes : En échangeant la j - ième colonne avec la j 1-ième , puis la j 1-ième avec la j 1-ième jusqu' à ce que la première colonne soit Ci , puisqu' il y a au total j 1 échanges : En échangeant la i - ème ligne avec la i 1-ième , puis la i 1-ième avec la i 1-ième jusqu' à ce que la première ligne soit 1 0 0 , puisqu' il y a au total i1 échanges , on a par la formule du déterminant d' une matrice - blocs ( 1)ij ai , j det(1 ) det Ai , j ai , j Cofacteuri , j ( A ) La formule du développement selon une colonne se ***démontre*** de la même façon ( ou en utilisant la transposée )

**33513**: Les formules de développements selon une ligne ou une colonne ***sont*** très utiles théoriquement mais inefficaces en général dès que n 5

**33531**: En effet , ces formules ***ramènent*** le calcul d' un déterminant d' une matrice d' ordre n à n calculs de déterminants de matrice d' ordre n 1

**33559**: En itérant , nous ***avons*** donc au total n ! calculs à faire

**33574**: Par contre , ces formules ***sont*** très utiles lorsque la matrice comporte plein de zéros ( matrice creuse ) ou pour obtenir des formules de récurrence

**33580**: Par contre , ces formules sont très utiles lorsque la matrice ***comporte*** plein de zéros ( matrice creuse ) ou pour obtenir des formules de récurrence

**33598**: Soit ( ***a*** , b , c ) K3 , calculer : On développe suivant la première colonne , puis le cofacteur d' indice ( 2 , 1 ) suivant la première ligne

**33609**: Soit ( a , b , c ) K3 , calculer : On ***développe*** suivant la première colonne , puis le cofacteur d' indice ( 2 , 1 ) suivant la première ligne

**33631**: Il ***vient*** : On reconnaît une suite récurrente linéaire d' ordre 2 à coefficients constants

**33634**: Il vient : On ***reconnaît*** une suite récurrente linéaire d' ordre 2 à coefficients constants

**33647**: On ***peut*** obtenir son terme général en utilisant le fait que 1 a et 2 a2 b c. On considère la fonction : C' est une fonction polynomiale en x. En effectuant les opérations élémentaires k 2 , n , Ck Ck C1 , puis en développant suivant la première colonne , on en déduit que cette fonction polynomiale est de degré inférieur ou égale à 1

**33658**: On peut obtenir son terme général en utilisant le fait que 1 ***a*** et 2 a2 b c. On considère la fonction : C' est une fonction polynomiale en x. En effectuant les opérations élémentaires k 2 , n , Ck Ck C1 , puis en développant suivant la première colonne , on en déduit que cette fonction polynomiale est de degré inférieur ou égale à 1

**33665**: On peut obtenir son terme général en utilisant le fait que 1 a et 2 a2 b c. On ***considère*** la fonction : C' est une fonction polynomiale en x. En effectuant les opérations élémentaires k 2 , n , Ck Ck C1 , puis en développant suivant la première colonne , on en déduit que cette fonction polynomiale est de degré inférieur ou égale à 1

**33670**: On peut obtenir son terme général en utilisant le fait que 1 a et 2 a2 b c. On considère la fonction : C' ***est*** une fonction polynomiale en x. En effectuant les opérations élémentaires k 2 , n , Ck Ck C1 , puis en développant suivant la première colonne , on en déduit que cette fonction polynomiale est de degré inférieur ou égale à 1

**33700**: On peut obtenir son terme général en utilisant le fait que 1 a et 2 a2 b c. On considère la fonction : C' est une fonction polynomiale en x. En effectuant les opérations élémentaires k 2 , n , Ck Ck C1 , puis en développant suivant la première colonne , on en ***déduit*** que cette fonction polynomiale est de degré inférieur ou égale à 1

**33705**: On peut obtenir son terme général en utilisant le fait que 1 a et 2 a2 b c. On considère la fonction : C' est une fonction polynomiale en x. En effectuant les opérations élémentaires k 2 , n , Ck Ck C1 , puis en développant suivant la première colonne , on en déduit que cette fonction polynomiale ***est*** de degré inférieur ou égale à 1

**33716**: Elle s' ***écrit*** donc sous la forme : x

**33726**: Pour x ***a*** , le déterminant vaut Pour x b , le déterminant vaut On en déduit alors , puis la valeur du déterminant en évaluant en x 0

**33730**: Pour x a , le déterminant ***vaut*** Pour x b , le déterminant vaut On en déduit alors , puis la valeur du déterminant en évaluant en x 0

**33737**: Pour x a , le déterminant vaut Pour x b , le déterminant ***vaut*** On en déduit alors , puis la valeur du déterminant en évaluant en x 0

**33740**: Pour x a , le déterminant vaut Pour x b , le déterminant vaut On en ***déduit*** alors , puis la valeur du déterminant en évaluant en x 0

**33768**: Par une récurrence sur n on ***obtient*** : 4

**33784**: , an ) ***est*** inversible si et seulement si det(V ( a1 ,

**33808**: , an ) ) 6 0 si et seulement si les ***ai*** sont tous différents

**33809**: , an ) ) 6 0 si et seulement si les ai ***sont*** tous différents

**33833**: Théorème 3.2 Propriété de la comatrice Soit A Mn ( K ) , alors : En particulier , si A ***est*** inversible : Démonstration Pour i j , le coefficient en ( i , i ) de A t Com(A ) vaut , en utilisant la formule du développement suivant la i - ième ligne : ai , k Cofacteuri , k ( A ) det A Pour i 6 j , posons B la matrice obtenue à partir de A en remplaçant la j - ième ligne par la i - ème ligne de A. Puisque B a deux lignes égales , det B 0

**33854**: Théorème 3.2 Propriété de la comatrice Soit A Mn ( K ) , alors : En particulier , si A est inversible : Démonstration Pour i j , le coefficient en ( i , i ) de A t Com(A ) ***vaut*** , en utilisant la formule du développement suivant la i - ième ligne : ai , k Cofacteuri , k ( A ) det A Pour i 6 j , posons B la matrice obtenue à partir de A en remplaçant la j - ième ligne par la i - ème ligne de A. Puisque B a deux lignes égales , det B 0

**33885**: Théorème 3.2 Propriété de la comatrice Soit A Mn ( K ) , alors : En particulier , si A est inversible : Démonstration Pour i j , le coefficient en ( i , i ) de A t Com(A ) vaut , en utilisant la formule du développement suivant la i - ième ligne : ai , k Cofacteuri , k ( A ) det A Pour i 6 j , ***posons*** B la matrice obtenue à partir de A en remplaçant la j - ième ligne par la i - ème ligne de A. Puisque B a deux lignes égales , det B 0

**33911**: Théorème 3.2 Propriété de la comatrice Soit A Mn ( K ) , alors : En particulier , si A est inversible : Démonstration Pour i j , le coefficient en ( i , i ) de A t Com(A ) vaut , en utilisant la formule du développement suivant la i - ième ligne : ai , k Cofacteuri , k ( A ) det A Pour i 6 j , posons B la matrice obtenue à partir de A en remplaçant la j - ième ligne par la i - ème ligne de A. Puisque B ***a*** deux lignes égales , det B 0

**33950**: Le coefficient en ( i , j ) de A t Com(A ) ***vaut*** ai , k Cofacteurj , k ( A ) ai , k Cofacteurk , j ( B ) det B 0 Finalement , A t Com(A ) det(A).In

**33988**: La relation t Com(A ) A det(A).In se ***démontre*** exactement de la même manière

**33997**: Si A ***est*** inversible , en multipliant par A1 : Com(A ) det(A).A1 et comme det A 6 0 , on obtient le résultat

**34016**: Si A est inversible , en multipliant par A1 : Com(A ) det(A).A1 et comme det A 6 0 , on ***obtient*** le résultat

**34022**: Cette formule ***est*** inutile pour calculer un inverse ! En effet , elle mène à des calculs trop Pour calculer l' inverse d' une matrice A , on peut par exemple utiliser la méthode du pivot de Gauss sur la matrice augmentée A In ( voir la partie sur les systèmes linéaires du chapitre précédent ) , ou résoudre , toujours avec la méthode du pivot de Gauss , un système générique A X Y , où X et Y sont dans Mn,1 ( K ) , ce qui donne X A1 Y

**34033**: Cette formule est inutile pour calculer un inverse ! En effet , elle ***mène*** à des calculs trop Pour calculer l' inverse d' une matrice A , on peut par exemple utiliser la méthode du pivot de Gauss sur la matrice augmentée A In ( voir la partie sur les systèmes linéaires du chapitre précédent ) , ou résoudre , toujours avec la méthode du pivot de Gauss , un système générique A X Y , où X et Y sont dans Mn,1 ( K ) , ce qui donne X A1 Y

**34048**: Cette formule est inutile pour calculer un inverse ! En effet , elle mène à des calculs trop Pour calculer l' inverse d' une matrice A , on ***peut*** par exemple utiliser la méthode du pivot de Gauss sur la matrice augmentée A In ( voir la partie sur les systèmes linéaires du chapitre précédent ) , ou résoudre , toujours avec la méthode du pivot de Gauss , un système générique A X Y , où X et Y sont dans Mn,1 ( K ) , ce qui donne X A1 Y

**34100**: Cette formule est inutile pour calculer un inverse ! En effet , elle mène à des calculs trop Pour calculer l' inverse d' une matrice A , on peut par exemple utiliser la méthode du pivot de Gauss sur la matrice augmentée A In ( voir la partie sur les systèmes linéaires du chapitre précédent ) , ou résoudre , toujours avec la méthode du pivot de Gauss , un système générique A X Y , où X et Y ***sont*** dans Mn,1 ( K ) , ce qui donne X A1 Y

**34109**: Cette formule est inutile pour calculer un inverse ! En effet , elle mène à des calculs trop Pour calculer l' inverse d' une matrice A , on peut par exemple utiliser la méthode du pivot de Gauss sur la matrice augmentée A In ( voir la partie sur les systèmes linéaires du chapitre précédent ) , ou résoudre , toujours avec la méthode du pivot de Gauss , un système générique A X Y , où X et Y sont dans Mn,1 ( K ) , ce qui ***donne*** X A1 Y

**34117**: Mais cette formule ***a*** un intérêt théorique

**34138**: Si on note A1 i , j ( i , j)1,n2 , alors l' application : ***est*** de classe C ( en particulier continue ) , donc une petite variation sur les coefficients de A provoque une petite variation sur les coefficients de A1 : on peut donc faire un calcul approché de A1 ! Faisons quelques calculs en Wxmaxima : Dans Rn , nous avons une base de référence : la base canonique Cn

**34145**: Si on note A1 i , j ( i , j)1,n2 , alors l' application : est de classe C ( en particulier ***continue*** ) , donc une petite variation sur les coefficients de A provoque une petite variation sur les coefficients de A1 : on peut donc faire un calcul approché de A1 ! Faisons quelques calculs en Wxmaxima : Dans Rn , nous avons une base de référence : la base canonique Cn

**34156**: Si on note A1 i , j ( i , j)1,n2 , alors l' application : est de classe C ( en particulier continue ) , donc une petite variation sur les coefficients de ***A*** provoque une petite variation sur les coefficients de A1 : on peut donc faire un calcul approché de A1 ! Faisons quelques calculs en Wxmaxima : Dans Rn , nous avons une base de référence : la base canonique Cn

**34157**: Si on note A1 i , j ( i , j)1,n2 , alors l' application : est de classe C ( en particulier continue ) , donc une petite variation sur les coefficients de A ***provoque*** une petite variation sur les coefficients de A1 : on peut donc faire un calcul approché de A1 ! Faisons quelques calculs en Wxmaxima : Dans Rn , nous avons une base de référence : la base canonique Cn

**34168**: Si on note A1 i , j ( i , j)1,n2 , alors l' application : est de classe C ( en particulier continue ) , donc une petite variation sur les coefficients de A provoque une petite variation sur les coefficients de A1 : on ***peut*** donc faire un calcul approché de A1 ! Faisons quelques calculs en Wxmaxima : Dans Rn , nous avons une base de référence : la base canonique Cn

**34177**: Si on note A1 i , j ( i , j)1,n2 , alors l' application : est de classe C ( en particulier continue ) , donc une petite variation sur les coefficients de A provoque une petite variation sur les coefficients de A1 : on peut donc faire un calcul approché de A1 ! ***Faisons*** quelques calculs en Wxmaxima : Dans Rn , nous avons une base de référence : la base canonique Cn

**34187**: Si on note A1 i , j ( i , j)1,n2 , alors l' application : est de classe C ( en particulier continue ) , donc une petite variation sur les coefficients de A provoque une petite variation sur les coefficients de A1 : on peut donc faire un calcul approché de A1 ! Faisons quelques calculs en Wxmaxima : Dans Rn , nous ***avons*** une base de référence : la base canonique Cn

**34218**: , en ) une autre base , alors : Lorsque ce déterminant ***est*** positif , on dit que la base E est directe

**34222**: , en ) une autre base , alors : Lorsque ce déterminant est positif , on ***dit*** que la base E est directe

**34227**: , en ) une autre base , alors : Lorsque ce déterminant est positif , on dit que la base E ***est*** directe

**34232**: Lorsqu' il ***est*** négatif , on dit qu' elle est indirecte

**34236**: Lorsqu' il est négatif , on ***dit*** qu' elle est indirecte

**34239**: Lorsqu' il est négatif , on dit qu' elle ***est*** indirecte

**34243**: On ***peut*** alors interpréter le déterminant de la manière suivante : v deux vecteurs de R2 , alors : où ( ) désigne l' aire de , le parallélogramme construit sur v

**34264**: On peut alors interpréter le déterminant de la manière suivante : v deux vecteurs de R2 , alors : où ( ) ***désigne*** l' aire de , le parallélogramme construit sur v

**34271**: On peut alors interpréter le déterminant de la manière suivante : v deux vecteurs de R2 , alors : où ( ) désigne l' aire de , le parallélogramme ***construit*** sur v

**34276**: On ***peut*** alors donner un sens à une aire orientée en considérant le déterminant sans la valeur absolue

**34305**: w trois vecteurs de R3 , alors : où ( ) ***désigne*** le volume de , le parallélépipède construit sur On avait trouvé une autre formule ( qui est la même ) car : Plus généralement , si E est un R - espace vectoriel de dimension finie et si E et E 0 deux bases de E , on dit que E 0 a la même orientation que E si detE ( E 0 ) 0

**34312**: w trois vecteurs de R3 , alors : où ( ) désigne le volume de , le parallélépipède ***construit*** sur On avait trouvé une autre formule ( qui est la même ) car : Plus généralement , si E est un R - espace vectoriel de dimension finie et si E et E 0 deux bases de E , on dit que E 0 a la même orientation que E si detE ( E 0 ) 0

**34322**: w trois vecteurs de R3 , alors : où ( ) désigne le volume de , le parallélépipède construit sur On avait trouvé une autre formule ( qui ***est*** la même ) car : Plus généralement , si E est un R - espace vectoriel de dimension finie et si E et E 0 deux bases de E , on dit que E 0 a la même orientation que E si detE ( E 0 ) 0

**34333**: w trois vecteurs de R3 , alors : où ( ) désigne le volume de , le parallélépipède construit sur On avait trouvé une autre formule ( qui est la même ) car : Plus généralement , si E ***est*** un R - espace vectoriel de dimension finie et si E et E 0 deux bases de E , on dit que E 0 a la même orientation que E si detE ( E 0 ) 0

**34354**: w trois vecteurs de R3 , alors : où ( ) désigne le volume de , le parallélépipède construit sur On avait trouvé une autre formule ( qui est la même ) car : Plus généralement , si E est un R - espace vectoriel de dimension finie et si E et E 0 deux bases de E , on ***dit*** que E 0 a la même orientation que E si detE ( E 0 ) 0

**34358**: w trois vecteurs de R3 , alors : où ( ) désigne le volume de , le parallélépipède construit sur On avait trouvé une autre formule ( qui est la même ) car : Plus généralement , si E est un R - espace vectoriel de dimension finie et si E et E 0 deux bases de E , on dit que E 0 ***a*** la même orientation que E si detE ( E 0 ) 0

**34373**: On ***démontre*** alors que la relation avoir la même orientation est une relation d' équivalence sur l' ensemble des bases de E. En fixant une base E de E , nous pouvons ainsi séparer les bases de E en deux ensembles ( les deux classes d' équivalences ) : celles qui ont la même orientation que E et celles qui ne ont pas la même orientation que E

**34382**: On démontre alors que la relation avoir la même orientation ***est*** une relation d' équivalence sur l' ensemble des bases de E. En fixant une base E de E , nous pouvons ainsi séparer les bases de E en deux ensembles ( les deux classes d' équivalences ) : celles qui ont la même orientation que E et celles qui ne ont pas la même orientation que E

**34403**: On démontre alors que la relation avoir la même orientation est une relation d' équivalence sur l' ensemble des bases de E. En fixant une base E de E , nous ***pouvons*** ainsi séparer les bases de E en deux ensembles ( les deux classes d' équivalences ) : celles qui ont la même orientation que E et celles qui ne ont pas la même orientation que E

**34423**: On démontre alors que la relation avoir la même orientation est une relation d' équivalence sur l' ensemble des bases de E. En fixant une base E de E , nous pouvons ainsi séparer les bases de E en deux ensembles ( les deux classes d' équivalences ) : celles qui ***ont*** la même orientation que E et celles qui ne ont pas la même orientation que E

**34433**: On démontre alors que la relation avoir la même orientation est une relation d' équivalence sur l' ensemble des bases de E. En fixant une base E de E , nous pouvons ainsi séparer les bases de E en deux ensembles ( les deux classes d' équivalences ) : celles qui ont la même orientation que E et celles qui ne ***ont*** pas la même orientation que E

**34442**: On ***peut*** alors décréter arbitrairement qu' une base E est une base directe , on dit alors que E est orienté

**34450**: On peut alors décréter arbitrairement qu' une base E ***est*** une base directe , on dit alors que E est orienté

**34456**: On peut alors décréter arbitrairement qu' une base E est une base directe , on ***dit*** alors que E est orienté

**34460**: On peut alors décréter arbitrairement qu' une base E est une base directe , on dit alors que E ***est*** orienté

**34472**: Toutes les bases ayant la même orientation que E ***sont*** dites directes et les autres indirectes

**34473**: Toutes les bases ayant la même orientation que E sont ***dites*** directes et les autres indirectes

**34484**: Dans Rn , il ***est*** bien entendu naturel de décréter que la base canonique est directe ( ce qui est cohérent avec la remarque précédente )

**34494**: Dans Rn , il est bien entendu naturel de décréter que la base canonique ***est*** directe ( ce qui est cohérent avec la remarque précédente )

**34499**: Dans Rn , il est bien entendu naturel de décréter que la base canonique est directe ( ce qui ***est*** cohérent avec la remarque précédente )

**34513**: Retour sur les systèmes linéaires On ***rappelle*** qu' un système linéaire de n équations à n inconnues de la forme d' inconnue X Mn,1 ( K ) avec A Mn ( K ) et B Mn,1 ( K ) est dit de Cramer lorsqu' il y a existence et unicité de la solution

**34546**: Retour sur les systèmes linéaires On rappelle qu' un système linéaire de n équations à n inconnues de la forme d' inconnue X Mn,1 ( K ) avec A Mn ( K ) et B Mn,1 ( K ) ***est*** dit de Cramer lorsqu' il y a existence et unicité de la solution

**34547**: Retour sur les systèmes linéaires On rappelle qu' un système linéaire de n équations à n inconnues de la forme d' inconnue X Mn,1 ( K ) avec A Mn ( K ) et B Mn,1 ( K ) est ***dit*** de Cramer lorsqu' il y a existence et unicité de la solution

**34553**: Retour sur les systèmes linéaires On rappelle qu' un système linéaire de n équations à n inconnues de la forme d' inconnue X Mn,1 ( K ) avec A Mn ( K ) et B Mn,1 ( K ) est dit de Cramer lorsqu' il y ***a*** existence et unicité de la solution

**34562**: Autrement ***dit*** , c' est un système de Cramer si , et seulement si , A est inversible si , et seulement si , Proposition 3.3 Soit A GLn ( K ) de colonnes A1 ,

**34565**: Autrement dit , c' ***est*** un système de Cramer si , et seulement si , A est inversible si , et seulement si , Proposition 3.3 Soit A GLn ( K ) de colonnes A1 ,

**34577**: Autrement dit , c' est un système de Cramer si , et seulement si , A ***est*** inversible si , et seulement si , Proposition 3.3 Soit A GLn ( K ) de colonnes A1 ,

**34617**: Alors l' unique solution du système de Cramer ***est*** donnée par Démonstration donc pour tout k 1 , n , Lorsque k 6 j , le déterminant ci - dessus est nul puisque deux colonnes sont égales , donc : ce qui donne le résultat puisque det A 6 0

**34639**: Alors l' unique solution du système de Cramer est donnée par Démonstration donc pour tout k 1 , n , Lorsque k 6 j , le déterminant ci - dessus ***est*** nul puisque deux colonnes sont égales , donc : ce qui donne le résultat puisque det A 6 0

**34644**: Alors l' unique solution du système de Cramer est donnée par Démonstration donc pour tout k 1 , n , Lorsque k 6 j , le déterminant ci - dessus est nul puisque deux colonnes ***sont*** égales , donc : ce qui donne le résultat puisque det A 6 0

**34651**: Alors l' unique solution du système de Cramer est donnée par Démonstration donc pour tout k 1 , n , Lorsque k 6 j , le déterminant ci - dessus est nul puisque deux colonnes sont égales , donc : ce qui ***donne*** le résultat puisque det A 6 0

**34663**: Cette proposition ne ***est*** pas une méthode pratique de résolution ( il est bien plus efficace d' utiliser la méthode du pivot de Gauss )

**34672**: Cette proposition ne est pas une méthode pratique de résolution ( il ***est*** bien plus efficace d' utiliser la méthode du pivot de Gauss )

**34687**: Elle ***a*** cependant un intérêt théorique , elle permet d' obtenir que la solution X dépend de manière continue ( et même C ) des coefficients de A et B : une petite variation sur les coefficients de A ou de B provoque une petite variation sur les coefficients de X : on peut donc faire un calcul approché de X ! Démontrer que A est inversible et que : 3.1.2 Soit V un K - espace vectoriel de dimension finie et soit p1 N , p2 N , p2 n. On considère l' ensemble des formes ( p1 p2 ) -linéaires vérifiant pour tout ( x1 ,

**34694**: Elle a cependant un intérêt théorique , elle ***permet*** d' obtenir que la solution X dépend de manière continue ( et même C ) des coefficients de A et B : une petite variation sur les coefficients de A ou de B provoque une petite variation sur les coefficients de X : on peut donc faire un calcul approché de X ! Démontrer que A est inversible et que : 3.1.2 Soit V un K - espace vectoriel de dimension finie et soit p1 N , p2 N , p2 n. On considère l' ensemble des formes ( p1 p2 ) -linéaires vérifiant pour tout ( x1 ,

**34701**: Elle a cependant un intérêt théorique , elle permet d' obtenir que la solution X ***dépend*** de manière continue ( et même C ) des coefficients de A et B : une petite variation sur les coefficients de A ou de B provoque une petite variation sur les coefficients de X : on peut donc faire un calcul approché de X ! Démontrer que A est inversible et que : 3.1.2 Soit V un K - espace vectoriel de dimension finie et soit p1 N , p2 N , p2 n. On considère l' ensemble des formes ( p1 p2 ) -linéaires vérifiant pour tout ( x1 ,

**34704**: Elle a cependant un intérêt théorique , elle permet d' obtenir que la solution X dépend de manière ***continue*** ( et même C ) des coefficients de A et B : une petite variation sur les coefficients de A ou de B provoque une petite variation sur les coefficients de X : on peut donc faire un calcul approché de X ! Démontrer que A est inversible et que : 3.1.2 Soit V un K - espace vectoriel de dimension finie et soit p1 N , p2 N , p2 n. On considère l' ensemble des formes ( p1 p2 ) -linéaires vérifiant pour tout ( x1 ,

**34728**: Elle a cependant un intérêt théorique , elle permet d' obtenir que la solution X dépend de manière continue ( et même C ) des coefficients de A et B : une petite variation sur les coefficients de A ou de B ***provoque*** une petite variation sur les coefficients de X : on peut donc faire un calcul approché de X ! Démontrer que A est inversible et que : 3.1.2 Soit V un K - espace vectoriel de dimension finie et soit p1 N , p2 N , p2 n. On considère l' ensemble des formes ( p1 p2 ) -linéaires vérifiant pour tout ( x1 ,

**34739**: Elle a cependant un intérêt théorique , elle permet d' obtenir que la solution X dépend de manière continue ( et même C ) des coefficients de A et B : une petite variation sur les coefficients de A ou de B provoque une petite variation sur les coefficients de X : on ***peut*** donc faire un calcul approché de X ! Démontrer que A est inversible et que : 3.1.2 Soit V un K - espace vectoriel de dimension finie et soit p1 N , p2 N , p2 n. On considère l' ensemble des formes ( p1 p2 ) -linéaires vérifiant pour tout ( x1 ,

**34751**: Elle a cependant un intérêt théorique , elle permet d' obtenir que la solution X dépend de manière continue ( et même C ) des coefficients de A et B : une petite variation sur les coefficients de A ou de B provoque une petite variation sur les coefficients de X : on peut donc faire un calcul approché de X ! Démontrer que A ***est*** inversible et que : 3.1.2 Soit V un K - espace vectoriel de dimension finie et soit p1 N , p2 N , p2 n. On considère l' ensemble des formes ( p1 p2 ) -linéaires vérifiant pour tout ( x1 ,

**34778**: Elle a cependant un intérêt théorique , elle permet d' obtenir que la solution X dépend de manière continue ( et même C ) des coefficients de A et B : une petite variation sur les coefficients de A ou de B provoque une petite variation sur les coefficients de X : on peut donc faire un calcul approché de X ! Démontrer que A est inversible et que : 3.1.2 Soit V un K - espace vectoriel de dimension finie et soit p1 N , p2 N , p2 n. On ***considère*** l' ensemble des formes ( p1 p2 ) -linéaires vérifiant pour tout ( x1 ,

**34814**: , xp1 p2 ) V p1 p2 , toute 1 Sp1 et toute 2 Sp2 : Quelle ***est*** la dimension du sous-espace vectoriel des formes p - linéaires vérifiant cette propriété ? det(A B ) det(A)p det(B)n 3.1.4 Démontrer que le volume d' un tétraèdre de sommets A , B , C et D , vaut : det(AB , AC , AD ) 3.1.5 Calculer les déterminants suivants : 3.1.6 Soit A Mn ( K ) , démontrer que : ( a ) On suppose D inversible démontrer que Démontrer par un exemple que ce ne est pas toujours vrai si D est non inversible

**34852**: , xp1 p2 ) V p1 p2 , toute 1 Sp1 et toute 2 Sp2 : Quelle est la dimension du sous-espace vectoriel des formes p - linéaires vérifiant cette propriété ? det(A B ) det(A)p det(B)n 3.1.4 Démontrer que le volume d' un tétraèdre de sommets A , B , C et D , ***vaut*** : det(AB , AC , AD ) 3.1.5 Calculer les déterminants suivants : 3.1.6 Soit A Mn ( K ) , démontrer que : ( a ) On suppose D inversible démontrer que Démontrer par un exemple que ce ne est pas toujours vrai si D est non inversible

**34878**: , xp1 p2 ) V p1 p2 , toute 1 Sp1 et toute 2 Sp2 : Quelle est la dimension du sous-espace vectoriel des formes p - linéaires vérifiant cette propriété ? det(A B ) det(A)p det(B)n 3.1.4 Démontrer que le volume d' un tétraèdre de sommets A , B , C et D , vaut : det(AB , AC , AD ) 3.1.5 Calculer les déterminants suivants : 3.1.6 Soit A Mn ( K ) , démontrer que : ( ***a*** ) On suppose D inversible démontrer que Démontrer par un exemple que ce ne est pas toujours vrai si D est non inversible

**34881**: , xp1 p2 ) V p1 p2 , toute 1 Sp1 et toute 2 Sp2 : Quelle est la dimension du sous-espace vectoriel des formes p - linéaires vérifiant cette propriété ? det(A B ) det(A)p det(B)n 3.1.4 Démontrer que le volume d' un tétraèdre de sommets A , B , C et D , vaut : det(AB , AC , AD ) 3.1.5 Calculer les déterminants suivants : 3.1.6 Soit A Mn ( K ) , démontrer que : ( a ) On ***suppose*** D inversible démontrer que Démontrer par un exemple que ce ne est pas toujours vrai si D est non inversible

**34893**: , xp1 p2 ) V p1 p2 , toute 1 Sp1 et toute 2 Sp2 : Quelle est la dimension du sous-espace vectoriel des formes p - linéaires vérifiant cette propriété ? det(A B ) det(A)p det(B)n 3.1.4 Démontrer que le volume d' un tétraèdre de sommets A , B , C et D , vaut : det(AB , AC , AD ) 3.1.5 Calculer les déterminants suivants : 3.1.6 Soit A Mn ( K ) , démontrer que : ( a ) On suppose D inversible démontrer que Démontrer par un exemple que ce ne ***est*** pas toujours vrai si D est non inversible

**34899**: , xp1 p2 ) V p1 p2 , toute 1 Sp1 et toute 2 Sp2 : Quelle est la dimension du sous-espace vectoriel des formes p - linéaires vérifiant cette propriété ? det(A B ) det(A)p det(B)n 3.1.4 Démontrer que le volume d' un tétraèdre de sommets A , B , C et D , vaut : det(AB , AC , AD ) 3.1.5 Calculer les déterminants suivants : 3.1.6 Soit A Mn ( K ) , démontrer que : ( a ) On suppose D inversible démontrer que Démontrer par un exemple que ce ne est pas toujours vrai si D ***est*** non inversible

**34924**: ( b ) Démontrer que pour D quelconque , ( c ) Calculer , dans le cas général det 3.1.8 On ***appelle*** décomposition LU d' une matrice A GLn ( K ) , la donnée de deux matrices L et U de Mn ( K ) telles que : L est triangulaire inférieure ( lower ) , avec des 1 sur la diagonale U est triangulaire supérieure ( upper )

**34953**: ( b ) Démontrer que pour D quelconque , ( c ) Calculer , dans le cas général det 3.1.8 On appelle décomposition LU d' une matrice A GLn ( K ) , la donnée de deux matrices L et U de Mn ( K ) telles que : L ***est*** triangulaire inférieure ( lower ) , avec des 1 sur la diagonale U est triangulaire supérieure ( upper )

**34967**: ( b ) Démontrer que pour D quelconque , ( c ) Calculer , dans le cas général det 3.1.8 On appelle décomposition LU d' une matrice A GLn ( K ) , la donnée de deux matrices L et U de Mn ( K ) telles que : L est triangulaire inférieure ( lower ) , avec des 1 sur la diagonale U ***est*** triangulaire supérieure ( upper )

**34975**: ( ***a*** ) Démontrer si une décomposition LU existe , alors elle est unique

**34986**: ( a ) Démontrer si une décomposition LU existe , alors elle ***est*** unique

**34994**: ( b ) Démontrer que ***A*** possède une décomposition LU si , et seulement si , tous ses mineurs principaux m1 , ... mn sont non nuls , définis par : 3.1.9 ( a ) Démontrer que si A , B , C et D sont dans Mn ( R ) et que A C C A , alors : ( b ) Démontrer que c' est faux , en général , lorsqu' il ne y a plus commutation

**34995**: ( b ) Démontrer que A ***possède*** une décomposition LU si , et seulement si , tous ses mineurs principaux m1 , ... mn sont non nuls , définis par : 3.1.9 ( a ) Démontrer que si A , B , C et D sont dans Mn ( R ) et que A C C A , alors : ( b ) Démontrer que c' est faux , en général , lorsqu' il ne y a plus commutation

**35013**: ( b ) Démontrer que A possède une décomposition LU si , et seulement si , tous ses mineurs principaux m1 , ... mn ***sont*** non nuls , définis par : 3.1.9 ( a ) Démontrer que si A , B , C et D sont dans Mn ( R ) et que A C C A , alors : ( b ) Démontrer que c' est faux , en général , lorsqu' il ne y a plus commutation

**35017**: ( b ) Démontrer que A possède une décomposition LU si , et seulement si , tous ses mineurs principaux m1 , ... mn sont non nuls , ***définis*** par : 3.1.9 ( a ) Démontrer que si A , B , C et D sont dans Mn ( R ) et que A C C A , alors : ( b ) Démontrer que c' est faux , en général , lorsqu' il ne y a plus commutation

**35022**: ( b ) Démontrer que A possède une décomposition LU si , et seulement si , tous ses mineurs principaux m1 , ... mn sont non nuls , définis par : 3.1.9 ( ***a*** ) Démontrer que si A , B , C et D sont dans Mn ( R ) et que A C C A , alors : ( b ) Démontrer que c' est faux , en général , lorsqu' il ne y a plus commutation

**35034**: ( b ) Démontrer que A possède une décomposition LU si , et seulement si , tous ses mineurs principaux m1 , ... mn sont non nuls , définis par : 3.1.9 ( a ) Démontrer que si A , B , C et D ***sont*** dans Mn ( R ) et que A C C A , alors : ( b ) Démontrer que c' est faux , en général , lorsqu' il ne y a plus commutation

**35055**: ( b ) Démontrer que A possède une décomposition LU si , et seulement si , tous ses mineurs principaux m1 , ... mn sont non nuls , définis par : 3.1.9 ( a ) Démontrer que si A , B , C et D sont dans Mn ( R ) et que A C C A , alors : ( b ) Démontrer que c' ***est*** faux , en général , lorsqu' il ne y a plus commutation

**35065**: ( b ) Démontrer que A possède une décomposition LU si , et seulement si , tous ses mineurs principaux m1 , ... mn sont non nuls , définis par : 3.1.9 ( a ) Démontrer que si A , B , C et D sont dans Mn ( R ) et que A C C A , alors : ( b ) Démontrer que c' est faux , en général , lorsqu' il ne y ***a*** plus commutation

**35155**: Résoudre les équations suivantes d' inconnues X Mn ( R ) : 3.1.14 À quelle(s ) condition(s ) , connaissant les affixes des milieux des côtés d' un polygone fermé à n côtés , ***existe*** -t -il un tel polygone ? Préciser dans tous les cas le procédé de construction du ou des polygone(s ) solution(s )

**35180**: Que ***signifie*** géométriquement la condition de compatibilité obtenue ? 3.1.15 Déterminer les ensembles de quatre points du plan tels que la somme des distances d' un point au trois autres est constante

**35209**: Que signifie géométriquement la condition de compatibilité obtenue ? 3.1.15 Déterminer les ensembles de quatre points du plan tels que la somme des distances d' un point au trois autres ***est*** constante

**35238**: On ***dit*** que K est une valeur propre de u si il existe x E 0E tel que On dit alors que x est un vecteur propre de u associé à

**35241**: On dit que K ***est*** une valeur propre de u si il existe x E 0E tel que On dit alors que x est un vecteur propre de u associé à

**35249**: On dit que K est une valeur propre de u si il ***existe*** x E 0E tel que On dit alors que x est un vecteur propre de u associé à

**35256**: On dit que K est une valeur propre de u si il existe x E 0E tel que On ***dit*** alors que x est un vecteur propre de u associé à

**35260**: On dit que K est une valeur propre de u si il existe x E 0E tel que On dit alors que x ***est*** un vecteur propre de u associé à

**35277**: L' ensemble des valeurs propres de u s' ***appelle*** , dans ce cours , le spectre de u et se note Sp(u )

**35289**: L' ensemble des valeurs propres de u s' appelle , dans ce cours , le spectre de u et se ***note*** Sp(u )

**35298**: Si Sp(u ) , on ***appelle*** espace propre de u associé à le sous-espace vectoriel de E noté Eu ( ) et défini par Par définition , un vecteur propre ne est jamais nul

**35324**: Si Sp(u ) , on appelle espace propre de u associé à le sous-espace vectoriel de E noté Eu ( ) et défini par Par définition , un vecteur propre ne ***est*** jamais nul

**35339**: La restriction de u à un espace propre Eu ( ) ***est*** une homothétie de rapport

**35357**: De même pour une matrice A Mn ( K ) , on ***appelle*** valeur propre et vecteur propre de A tout couple Le spectre Sp(A ) de A est l' ensemble des valeurs propres de A et pour Sp(A ) , EA ( ) Ker(A .In ) est l' espace propre associé

**35373**: De même pour une matrice A Mn ( K ) , on appelle valeur propre et vecteur propre de A tout couple Le spectre Sp(A ) de A ***est*** l' ensemble des valeurs propres de A et pour Sp(A ) , EA ( ) Ker(A .In ) est l' espace propre associé

**35392**: De même pour une matrice A Mn ( K ) , on appelle valeur propre et vecteur propre de A tout couple Le spectre Sp(A ) de A est l' ensemble des valeurs propres de A et pour Sp(A ) , EA ( ) Ker(A .In ) ***est*** l' espace propre associé

**35399**: Si ***A*** MatE ( u ) est la matrice de u dans une base E de E ( en dimension finie ) , alors Sp(A ) Sp(u ) et x est un vecteur propre de u si , et seulement si , MatE ( x ) est un vecteur propre de A. La recherche des valeurs propres et vecteurs propres d' un endomorphisme u ( on parle d' éléments propres ) revient à résoudre un système linéaire homogène : On utilise donc les diverses méthodes de résolution des systèmes linéaires

**35404**: Si A MatE ( u ) ***est*** la matrice de u dans une base E de E ( en dimension finie ) , alors Sp(A ) Sp(u ) et x est un vecteur propre de u si , et seulement si , MatE ( x ) est un vecteur propre de A. La recherche des valeurs propres et vecteurs propres d' un endomorphisme u ( on parle d' éléments propres ) revient à résoudre un système linéaire homogène : On utilise donc les diverses méthodes de résolution des systèmes linéaires

**35428**: Si A MatE ( u ) est la matrice de u dans une base E de E ( en dimension finie ) , alors Sp(A ) Sp(u ) et x ***est*** un vecteur propre de u si , et seulement si , MatE ( x ) est un vecteur propre de A. La recherche des valeurs propres et vecteurs propres d' un endomorphisme u ( on parle d' éléments propres ) revient à résoudre un système linéaire homogène : On utilise donc les diverses méthodes de résolution des systèmes linéaires

**35444**: Si A MatE ( u ) est la matrice de u dans une base E de E ( en dimension finie ) , alors Sp(A ) Sp(u ) et x est un vecteur propre de u si , et seulement si , MatE ( x ) ***est*** un vecteur propre de A. La recherche des valeurs propres et vecteurs propres d' un endomorphisme u ( on parle d' éléments propres ) revient à résoudre un système linéaire homogène : On utilise donc les diverses méthodes de résolution des systèmes linéaires

**35464**: Si A MatE ( u ) est la matrice de u dans une base E de E ( en dimension finie ) , alors Sp(A ) Sp(u ) et x est un vecteur propre de u si , et seulement si , MatE ( x ) est un vecteur propre de A. La recherche des valeurs propres et vecteurs propres d' un endomorphisme u ( on ***parle*** d' éléments propres ) revient à résoudre un système linéaire homogène : On utilise donc les diverses méthodes de résolution des systèmes linéaires

**35469**: Si A MatE ( u ) est la matrice de u dans une base E de E ( en dimension finie ) , alors Sp(A ) Sp(u ) et x est un vecteur propre de u si , et seulement si , MatE ( x ) est un vecteur propre de A. La recherche des valeurs propres et vecteurs propres d' un endomorphisme u ( on parle d' éléments propres ) ***revient*** à résoudre un système linéaire homogène : On utilise donc les diverses méthodes de résolution des systèmes linéaires

**35478**: Si A MatE ( u ) est la matrice de u dans une base E de E ( en dimension finie ) , alors Sp(A ) Sp(u ) et x est un vecteur propre de u si , et seulement si , MatE ( x ) est un vecteur propre de A. La recherche des valeurs propres et vecteurs propres d' un endomorphisme u ( on parle d' éléments propres ) revient à résoudre un système linéaire homogène : On ***utilise*** donc les diverses méthodes de résolution des systèmes linéaires

**35496**: Par exemple , la matrice : ne ***possède*** qu' une valeur propre 2

**35507**: L' espace propre associé ***est*** de dimension 1

**35516**: Comme la valeur propre ***est*** différente de 2 , le rang est toujours 3

**35523**: Comme la valeur propre est différente de 2 , le rang ***est*** toujours 3

**35530**: Il ne y ***a*** pas d' autre valeur propre que celles trouvées

**35545**: Par ailleurs , comme nous ***avons*** travailler sur les lignes uniquement , les vecteurs propres sont solutions d' un système triangulaire très facile à résoudre

**35555**: Par ailleurs , comme nous avons travailler sur les lignes uniquement , les vecteurs propres ***sont*** solutions d' un système triangulaire très facile à résoudre

**35567**: On ***peut*** parfois trouver des valeurs propres et les vecteurs propres directement à partir de la matrice , sans résoudre un système linéaire

**35598**: Par exemple , si n 2 et si ***a*** et b sont dans K , a 6 b , la matrice : possède les valeurs propres a ( n 1 ) b et a b ( on verra qu' il ne y en a pas d' autre ) et les espaces propres associés sont : Si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à 2 n et si k R , alors l' endomorphisme u L ( E ) défini par : Il faut parfois faire preuve de vision géométrique : si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à n et si u L ( E ) est défini par : alors alors les valeurs propres de u sont 1 et 1 et les vecteurs propres se calculent facilement dans une base adaptée

**35601**: Par exemple , si n 2 et si a et b ***sont*** dans K , a 6 b , la matrice : possède les valeurs propres a ( n 1 ) b et a b ( on verra qu' il ne y en a pas d' autre ) et les espaces propres associés sont : Si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à 2 n et si k R , alors l' endomorphisme u L ( E ) défini par : Il faut parfois faire preuve de vision géométrique : si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à n et si u L ( E ) est défini par : alors alors les valeurs propres de u sont 1 et 1 et les vecteurs propres se calculent facilement dans une base adaptée

**35605**: Par exemple , si n 2 et si a et b sont dans K , ***a*** 6 b , la matrice : possède les valeurs propres a ( n 1 ) b et a b ( on verra qu' il ne y en a pas d' autre ) et les espaces propres associés sont : Si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à 2 n et si k R , alors l' endomorphisme u L ( E ) défini par : Il faut parfois faire preuve de vision géométrique : si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à n et si u L ( E ) est défini par : alors alors les valeurs propres de u sont 1 et 1 et les vecteurs propres se calculent facilement dans une base adaptée

**35612**: Par exemple , si n 2 et si a et b sont dans K , a 6 b , la matrice : ***possède*** les valeurs propres a ( n 1 ) b et a b ( on verra qu' il ne y en a pas d' autre ) et les espaces propres associés sont : Si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à 2 n et si k R , alors l' endomorphisme u L ( E ) défini par : Il faut parfois faire preuve de vision géométrique : si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à n et si u L ( E ) est défini par : alors alors les valeurs propres de u sont 1 et 1 et les vecteurs propres se calculent facilement dans une base adaptée

**35616**: Par exemple , si n 2 et si a et b sont dans K , a 6 b , la matrice : possède les valeurs propres ***a*** ( n 1 ) b et a b ( on verra qu' il ne y en a pas d' autre ) et les espaces propres associés sont : Si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à 2 n et si k R , alors l' endomorphisme u L ( E ) défini par : Il faut parfois faire preuve de vision géométrique : si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à n et si u L ( E ) est défini par : alors alors les valeurs propres de u sont 1 et 1 et les vecteurs propres se calculent facilement dans une base adaptée

**35623**: Par exemple , si n 2 et si a et b sont dans K , a 6 b , la matrice : possède les valeurs propres a ( n 1 ) b et ***a*** b ( on verra qu' il ne y en a pas d' autre ) et les espaces propres associés sont : Si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à 2 n et si k R , alors l' endomorphisme u L ( E ) défini par : Il faut parfois faire preuve de vision géométrique : si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à n et si u L ( E ) est défini par : alors alors les valeurs propres de u sont 1 et 1 et les vecteurs propres se calculent facilement dans une base adaptée

**35633**: Par exemple , si n 2 et si a et b sont dans K , a 6 b , la matrice : possède les valeurs propres a ( n 1 ) b et a b ( on verra qu' il ne y en ***a*** pas d' autre ) et les espaces propres associés sont : Si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à 2 n et si k R , alors l' endomorphisme u L ( E ) défini par : Il faut parfois faire preuve de vision géométrique : si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à n et si u L ( E ) est défini par : alors alors les valeurs propres de u sont 1 et 1 et les vecteurs propres se calculent facilement dans une base adaptée

**35643**: Par exemple , si n 2 et si a et b sont dans K , a 6 b , la matrice : possède les valeurs propres a ( n 1 ) b et a b ( on verra qu' il ne y en a pas d' autre ) et les espaces propres associés ***sont*** : Si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à 2 n et si k R , alors l' endomorphisme u L ( E ) défini par : Il faut parfois faire preuve de vision géométrique : si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à n et si u L ( E ) est défini par : alors alors les valeurs propres de u sont 1 et 1 et les vecteurs propres se calculent facilement dans une base adaptée

**35647**: Par exemple , si n 2 et si a et b sont dans K , a 6 b , la matrice : possède les valeurs propres a ( n 1 ) b et a b ( on verra qu' il ne y en a pas d' autre ) et les espaces propres associés sont : Si E ***est*** le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à 2 n et si k R , alors l' endomorphisme u L ( E ) défini par : Il faut parfois faire preuve de vision géométrique : si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à n et si u L ( E ) est défini par : alors alors les valeurs propres de u sont 1 et 1 et les vecteurs propres se calculent facilement dans une base adaptée

**35681**: Par exemple , si n 2 et si a et b sont dans K , a 6 b , la matrice : possède les valeurs propres a ( n 1 ) b et a b ( on verra qu' il ne y en a pas d' autre ) et les espaces propres associés sont : Si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à 2 n et si k R , alors l' endomorphisme u L ( E ) défini par : Il ***faut*** parfois faire preuve de vision géométrique : si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à n et si u L ( E ) est défini par : alors alors les valeurs propres de u sont 1 et 1 et les vecteurs propres se calculent facilement dans une base adaptée

**35691**: Par exemple , si n 2 et si a et b sont dans K , a 6 b , la matrice : possède les valeurs propres a ( n 1 ) b et a b ( on verra qu' il ne y en a pas d' autre ) et les espaces propres associés sont : Si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à 2 n et si k R , alors l' endomorphisme u L ( E ) défini par : Il faut parfois faire preuve de vision géométrique : si E ***est*** le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à n et si u L ( E ) est défini par : alors alors les valeurs propres de u sont 1 et 1 et les vecteurs propres se calculent facilement dans une base adaptée

**35714**: Par exemple , si n 2 et si a et b sont dans K , a 6 b , la matrice : possède les valeurs propres a ( n 1 ) b et a b ( on verra qu' il ne y en a pas d' autre ) et les espaces propres associés sont : Si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à 2 n et si k R , alors l' endomorphisme u L ( E ) défini par : Il faut parfois faire preuve de vision géométrique : si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à n et si u L ( E ) ***est*** défini par : alors alors les valeurs propres de u sont 1 et 1 et les vecteurs propres se calculent facilement dans une base adaptée

**35725**: Par exemple , si n 2 et si a et b sont dans K , a 6 b , la matrice : possède les valeurs propres a ( n 1 ) b et a b ( on verra qu' il ne y en a pas d' autre ) et les espaces propres associés sont : Si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à 2 n et si k R , alors l' endomorphisme u L ( E ) défini par : Il faut parfois faire preuve de vision géométrique : si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à n et si u L ( E ) est défini par : alors alors les valeurs propres de u ***sont*** 1 et 1 et les vecteurs propres se calculent facilement dans une base adaptée

**35734**: Par exemple , si n 2 et si a et b sont dans K , a 6 b , la matrice : possède les valeurs propres a ( n 1 ) b et a b ( on verra qu' il ne y en a pas d' autre ) et les espaces propres associés sont : Si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à 2 n et si k R , alors l' endomorphisme u L ( E ) défini par : Il faut parfois faire preuve de vision géométrique : si E est le R - espace vectoriel des fonctions polynomiales de degré inférieur ou égal à n et si u L ( E ) est défini par : alors alors les valeurs propres de u sont 1 et 1 et les vecteurs propres se ***calculent*** facilement dans une base adaptée

**35766**: Polynôme caractéristique Soit E un K - espace vectoriel de dimension finie , soit u L ( E ) et soit K. Les propositions suivantes ***sont*** équivalentes : 3

**35775**: idE ne ***est*** pas injectif

**35784**: En particulier , comme E ***est*** de dimension finie , u est ne est pas inversible si , et seulement si , 0E Sp(u )

**35790**: En particulier , comme E est de dimension finie , u ***est*** ne est pas inversible si , et seulement si , 0E Sp(u )

**35792**: En particulier , comme E est de dimension finie , u est ne ***est*** pas inversible si , et seulement si , 0E Sp(u )

**35807**: Si c' ***est*** le cas , Ker u Eu ( 0 )

**35821**: Démonstration et on ***remarque*** que u est injectif si , et seulement si , Ker(u

**35824**: Démonstration et on remarque que u ***est*** injectif si , et seulement si , Ker(u

**35840**: Si E ***est*** de dimension finie , u est inversible si , et seulement si , il est injectif , d' où le résultat en prenant 0

**35846**: Si E est de dimension finie , u ***est*** inversible si , et seulement si , il est injectif , d' où le résultat en prenant 0

**35855**: Si E est de dimension finie , u est inversible si , et seulement si , il ***est*** injectif , d' où le résultat en prenant 0

**35895**: Le polynôme caractéristique de u , noté u , ***correspond*** à la fonction polynomiale : Afin d' alléger les notations et les calculs , on note le polynôme caractéristique u sous la forme d' un polynôme formel ( voir le cours sur les polynômes ) : au lieu d' une fonction polynomiale , où X est l' indéterminée , une variable ayant les mêmes règles de calcul que celles d' une variable dans K. Ainsi , une fonction polynomiale sur K de la forme se représente par Autrement dit , X correspond à la fonction polynomiale x 7 x. On note KX le K - espace vectoriel des polynômes ( formels ) à coefficients dans K et Kn X le sousespace vectoriel de KX des polynômes de degré inférieur ou égal à n. On définit de la même manière le polynôme caractéristique A det(A X.In ) d' une matrice carrée A Mn ( K )

**35911**: Le polynôme caractéristique de u , noté u , correspond à la fonction polynomiale : Afin d' alléger les notations et les calculs , on ***note*** le polynôme caractéristique u sous la forme d' un polynôme formel ( voir le cours sur les polynômes ) : au lieu d' une fonction polynomiale , où X est l' indéterminée , une variable ayant les mêmes règles de calcul que celles d' une variable dans K. Ainsi , une fonction polynomiale sur K de la forme se représente par Autrement dit , X correspond à la fonction polynomiale x 7 x. On note KX le K - espace vectoriel des polynômes ( formels ) à coefficients dans K et Kn X le sousespace vectoriel de KX des polynômes de degré inférieur ou égal à n. On définit de la même manière le polynôme caractéristique A det(A X.In ) d' une matrice carrée A Mn ( K )

**35941**: Le polynôme caractéristique de u , noté u , correspond à la fonction polynomiale : Afin d' alléger les notations et les calculs , on note le polynôme caractéristique u sous la forme d' un polynôme formel ( voir le cours sur les polynômes ) : au lieu d' une fonction polynomiale , où X ***est*** l' indéterminée , une variable ayant les mêmes règles de calcul que celles d' une variable dans K. Ainsi , une fonction polynomiale sur K de la forme se représente par Autrement dit , X correspond à la fonction polynomiale x 7 x. On note KX le K - espace vectoriel des polynômes ( formels ) à coefficients dans K et Kn X le sousespace vectoriel de KX des polynômes de degré inférieur ou égal à n. On définit de la même manière le polynôme caractéristique A det(A X.In ) d' une matrice carrée A Mn ( K )

**35971**: Le polynôme caractéristique de u , noté u , correspond à la fonction polynomiale : Afin d' alléger les notations et les calculs , on note le polynôme caractéristique u sous la forme d' un polynôme formel ( voir le cours sur les polynômes ) : au lieu d' une fonction polynomiale , où X est l' indéterminée , une variable ayant les mêmes règles de calcul que celles d' une variable dans K. Ainsi , une fonction polynomiale sur K de la forme se ***représente*** par Autrement dit , X correspond à la fonction polynomiale x 7 x. On note KX le K - espace vectoriel des polynômes ( formels ) à coefficients dans K et Kn X le sousespace vectoriel de KX des polynômes de degré inférieur ou égal à n. On définit de la même manière le polynôme caractéristique A det(A X.In ) d' une matrice carrée A Mn ( K )

**35974**: Le polynôme caractéristique de u , noté u , correspond à la fonction polynomiale : Afin d' alléger les notations et les calculs , on note le polynôme caractéristique u sous la forme d' un polynôme formel ( voir le cours sur les polynômes ) : au lieu d' une fonction polynomiale , où X est l' indéterminée , une variable ayant les mêmes règles de calcul que celles d' une variable dans K. Ainsi , une fonction polynomiale sur K de la forme se représente par Autrement ***dit*** , X correspond à la fonction polynomiale x 7 x. On note KX le K - espace vectoriel des polynômes ( formels ) à coefficients dans K et Kn X le sousespace vectoriel de KX des polynômes de degré inférieur ou égal à n. On définit de la même manière le polynôme caractéristique A det(A X.In ) d' une matrice carrée A Mn ( K )

**35977**: Le polynôme caractéristique de u , noté u , correspond à la fonction polynomiale : Afin d' alléger les notations et les calculs , on note le polynôme caractéristique u sous la forme d' un polynôme formel ( voir le cours sur les polynômes ) : au lieu d' une fonction polynomiale , où X est l' indéterminée , une variable ayant les mêmes règles de calcul que celles d' une variable dans K. Ainsi , une fonction polynomiale sur K de la forme se représente par Autrement dit , X ***correspond*** à la fonction polynomiale x 7 x. On note KX le K - espace vectoriel des polynômes ( formels ) à coefficients dans K et Kn X le sousespace vectoriel de KX des polynômes de degré inférieur ou égal à n. On définit de la même manière le polynôme caractéristique A det(A X.In ) d' une matrice carrée A Mn ( K )

**36020**: Le polynôme caractéristique de u , noté u , correspond à la fonction polynomiale : Afin d' alléger les notations et les calculs , on note le polynôme caractéristique u sous la forme d' un polynôme formel ( voir le cours sur les polynômes ) : au lieu d' une fonction polynomiale , où X est l' indéterminée , une variable ayant les mêmes règles de calcul que celles d' une variable dans K. Ainsi , une fonction polynomiale sur K de la forme se représente par Autrement dit , X correspond à la fonction polynomiale x 7 x. On note KX le K - espace vectoriel des polynômes ( formels ) à coefficients dans K et Kn X le sousespace vectoriel de KX des polynômes de degré inférieur ou égal à n. On ***définit*** de la même manière le polynôme caractéristique A det(A X.In ) d' une matrice carrée A Mn ( K )

**36043**: Si ***A*** est la matrice de u dans une base de E , alors A u

**36044**: Si A ***est*** la matrice de u dans une base de E , alors A u

**36062**: Deux matrices semblables ***ont*** même polynôme caractéristique

**36069**: La réciproque ***est*** fausse

**36094**: Alors u ***est*** de degré n. De plus , si n 2 , alors : Démonstration Si n 1 , c' est évident

**36113**: Alors u est de degré n. De plus , si n 2 , alors : Démonstration Si n 1 , c' ***est*** évident

**36116**: ***Supposons*** n 2

**36121**: Soit ***A*** ai , j ( i , j)1,n2 Mn ( K ) la matrice de u dans une base quelconque de E ( donc u A )

**36149**: ***Posons*** : ai , j si i 6 j trace Atrace u puisque ( k),k a(k),k et ( ) , a ( ) , ne dépendent pas de X. On en déduit le résultat , en remarquant que le terme constant est u ( 0 ) det(u 0

**36169**: Posons : ai , j si i 6 j trace Atrace u puisque ( k),k a(k),k et ( ) , ***a*** ( ) , ne dépendent pas de X. On en déduit le résultat , en remarquant que le terme constant est u ( 0 ) det(u 0

**36174**: Posons : ai , j si i 6 j trace Atrace u puisque ( k),k a(k),k et ( ) , a ( ) , ne ***dépendent*** pas de X. On en déduit le résultat , en remarquant que le terme constant est u ( 0 ) det(u 0

**36180**: Posons : ai , j si i 6 j trace Atrace u puisque ( k),k a(k),k et ( ) , a ( ) , ne dépendent pas de X. On en ***déduit*** le résultat , en remarquant que le terme constant est u ( 0 ) det(u 0

**36190**: Posons : ai , j si i 6 j trace Atrace u puisque ( k),k a(k),k et ( ) , a ( ) , ne dépendent pas de X. On en déduit le résultat , en remarquant que le terme constant ***est*** u ( 0 ) det(u 0

**36225**: idE ) det u. Soit E un K - espace vectoriel de dimension finie non nulle , soit u L ( E ) et soit K. Alors ***est*** une valeur propre de u si , et seulement si , u ( ) 0

**36243**: Autrement ***dit*** : Démonstration Immédiat en utilisant la propriété 4.1 , page précédente et en remarquant que u

**36263**: idE ne ***est*** pas injectif si , et seulement si , u ( ) det(u

**36283**: Si E ***est*** de dimension n 1 , alors u a au plus n valeurs propres ( car u est un polynôme de degré n , il admet donc au plus n racines )

**36291**: Si E est de dimension n 1 , alors u ***a*** au plus n valeurs propres ( car u est un polynôme de degré n , il admet donc au plus n racines )

**36300**: Si E est de dimension n 1 , alors u a au plus n valeurs propres ( car u ***est*** un polynôme de degré n , il admet donc au plus n racines )

**36308**: Si E est de dimension n 1 , alors u a au plus n valeurs propres ( car u est un polynôme de degré n , il ***admet*** donc au plus n racines )

**36322**: Si K C , alors u ***a*** au moins une valeur propre car sur C les polynômes non constants admettent toujours une racine ( c' est faux pour K R )

**36335**: Si K C , alors u a au moins une valeur propre car sur C les polynômes non constants ***admettent*** toujours une racine ( c' est faux pour K R )

**36341**: Si K C , alors u a au moins une valeur propre car sur C les polynômes non constants admettent toujours une racine ( c' ***est*** faux pour K R )

**36378**: Il pourrait être tentant de passer systématiquement par le polynôme caractéristique pour trouver les éléments propres de u , ce serait pourtant une grosse erreur , car si il nous ***permet*** de trouver les valeurs propres de u , il ne nous donne aucune information sur les espaces propres

**36390**: Il pourrait être tentant de passer systématiquement par le polynôme caractéristique pour trouver les éléments propres de u , ce serait pourtant une grosse erreur , car si il nous permet de trouver les valeurs propres de u , il ne nous ***donne*** aucune information sur les espaces propres

**36408**: Ainsi : i1 Ei , i1 Mk ( K ) ***sont*** telles que toutes les Ak ( k 1 , n ) ont même polynôme caractéristique alors que les espaces propres associés ont des dimensions allant de 1 à n. Soit E un K - espace vectoriel de dimension finie non nulle , soit u L ( E ) et soit Sp(u )

**36420**: Ainsi : i1 Ei , i1 Mk ( K ) sont telles que toutes les Ak ( k 1 , n ) ***ont*** même polynôme caractéristique alors que les espaces propres associés ont des dimensions allant de 1 à n. Soit E un K - espace vectoriel de dimension finie non nulle , soit u L ( E ) et soit Sp(u )

**36430**: Ainsi : i1 Ei , i1 Mk ( K ) sont telles que toutes les Ak ( k 1 , n ) ont même polynôme caractéristique alors que les espaces propres associés ***ont*** des dimensions allant de 1 à n. Soit E un K - espace vectoriel de dimension finie non nulle , soit u L ( E ) et soit Sp(u )

**36465**: La multiplicité de ***est*** la multiplicité a de en tant que racine du polynôme u

**36468**: La multiplicité de est la multiplicité ***a*** de en tant que racine du polynôme u

**36492**: On note multu ( ) la multiplicité de Si multu ( ) 1 on ***parle*** de valeur propre simple

**36498**: ***a.*** Une racine K de P KX non nul a pour multiplicité r N si P ( X ) r Q avec Q KX tel que Q ( ) 6 0

**36507**: a. Une racine K de P KX non nul ***a*** pour multiplicité r N si P ( X ) r Q avec Q KX tel que Q ( ) 6 0

**36582**: Par définition , il ***existe*** un vecteur propre associé à ( donc non nul ) , d' où Eu ( ) 6 0E et Posons d dim Eu ( )

**36602**: Par définition , il existe un vecteur propre associé à ( donc non nul ) , d' où Eu ( ) 6 0E et ***Posons*** d dim Eu ( )

**36658**: La restriction de u à Eu ( ) ***est*** une homothétie de rapport donc : d' où d multu ( )

**36731**: , Eu ( k ) ***sont*** en somme directe

**36741**: Démonstration Par récurrence sur k. ***sont*** en somme directe

**36751**: Soit k 2 , n. ***Supposons*** le résultat vrai au rang k. Prenons pour tout j 1 , k 1 , xj Eu ( j ) tel que ( écriture de 0E ) : D' après ces deux égalités : donc , par hypothèse de récurrence , x1 xk 0E et donc xk1 0E

**36758**: Soit k 2 , n. Supposons le résultat vrai au rang k. ***Prenons*** pour tout j 1 , k 1 , xj Eu ( j ) tel que ( écriture de 0E ) : D' après ces deux égalités : donc , par hypothèse de récurrence , x1 xk 0E et donc xk1 0E

**36803**: Le résultat ***est*** donc vrai au rang k 1

**36818**: Par principe de récurrence , le résultat ***est*** vrai pour tout k 2 , n. Soit E un K - espace vectoriel de dimension finie et soit u L ( E )

**36845**: On ***dit*** que u est diagonalisable si : Comme E est de dimension finie , alors u est diagonalisable si , et seulement si , E admet une base de vecteurs propres de u ( on dit alors que u se diagonalise dans cette base )

**36848**: On dit que u ***est*** diagonalisable si : Comme E est de dimension finie , alors u est diagonalisable si , et seulement si , E admet une base de vecteurs propres de u ( on dit alors que u se diagonalise dans cette base )

**36854**: On dit que u est diagonalisable si : Comme E ***est*** de dimension finie , alors u est diagonalisable si , et seulement si , E admet une base de vecteurs propres de u ( on dit alors que u se diagonalise dans cette base )

**36861**: On dit que u est diagonalisable si : Comme E est de dimension finie , alors u ***est*** diagonalisable si , et seulement si , E admet une base de vecteurs propres de u ( on dit alors que u se diagonalise dans cette base )

**36870**: On dit que u est diagonalisable si : Comme E est de dimension finie , alors u est diagonalisable si , et seulement si , E ***admet*** une base de vecteurs propres de u ( on dit alors que u se diagonalise dans cette base )

**36880**: On dit que u est diagonalisable si : Comme E est de dimension finie , alors u est diagonalisable si , et seulement si , E admet une base de vecteurs propres de u ( on ***dit*** alors que u se diagonalise dans cette base )

**36893**: Si E ***est*** de dimension finie , u L ( E ) est diagonalisable , alors dans toute base E de E formé de vecteurs propres de u , la matrice de u est diagonale et ses éléments sont les valeurs propres 1 ,

**36903**: Si E est de dimension finie , u L ( E ) ***est*** diagonalisable , alors dans toute base E de E formé de vecteurs propres de u , la matrice de u est diagonale et ses éléments sont les valeurs propres 1 ,

**36924**: Si E est de dimension finie , u L ( E ) est diagonalisable , alors dans toute base E de E formé de vecteurs propres de u , la matrice de u ***est*** diagonale et ses éléments sont les valeurs propres 1 ,

**36929**: Si E est de dimension finie , u L ( E ) est diagonalisable , alors dans toute base E de E formé de vecteurs propres de u , la matrice de u est diagonale et ses éléments ***sont*** les valeurs propres 1 ,

**36941**: , k qui ***apparaissent*** avec leur multiplicité : multu ( 1 ) fois multu ( 2 ) fois On dit qu' une matrice A Mn ( K ) est diagonalisable si , et seulement si , son endomorphisme canoniquement associé l' est

**36957**: , k qui apparaissent avec leur multiplicité : multu ( 1 ) fois multu ( 2 ) fois On ***dit*** qu' une matrice A Mn ( K ) est diagonalisable si , et seulement si , son endomorphisme canoniquement associé l' est

**36966**: , k qui apparaissent avec leur multiplicité : multu ( 1 ) fois multu ( 2 ) fois On dit qu' une matrice A Mn ( K ) ***est*** diagonalisable si , et seulement si , son endomorphisme canoniquement associé l' est

**36979**: , k qui apparaissent avec leur multiplicité : multu ( 1 ) fois multu ( 2 ) fois On dit qu' une matrice A Mn ( K ) est diagonalisable si , et seulement si , son endomorphisme canoniquement associé l' ***est***

**36982**: Autrement ***dit*** , une matrice est diagonalisable si elle est semblable à une matrice diagonale

**36986**: Autrement dit , une matrice ***est*** diagonalisable si elle est semblable à une matrice diagonale

**36990**: Autrement dit , une matrice est diagonalisable si elle ***est*** semblable à une matrice diagonale

**37004**: Cependant , pour une matrice , il ***est*** fondamental de préciser le corps K dans lequel on travaille ( il est possible qu' une matrice Mn ( R ) ne soit pas diagonalisable mais qu' elle soit diagonalisable si on la voit comme une matrice de Mn ( C ) )

**37014**: Cependant , pour une matrice , il est fondamental de préciser le corps K dans lequel on ***travaille*** ( il est possible qu' une matrice Mn ( R ) ne soit pas diagonalisable mais qu' elle soit diagonalisable si on la voit comme une matrice de Mn ( C ) )

**37017**: Cependant , pour une matrice , il est fondamental de préciser le corps K dans lequel on travaille ( il ***est*** possible qu' une matrice Mn ( R ) ne soit pas diagonalisable mais qu' elle soit diagonalisable si on la voit comme une matrice de Mn ( C ) )

**37038**: Cependant , pour une matrice , il est fondamental de préciser le corps K dans lequel on travaille ( il est possible qu' une matrice Mn ( R ) ne soit pas diagonalisable mais qu' elle soit diagonalisable si on la ***voit*** comme une matrice de Mn ( C ) )

**37056**: Par exemple , la matrice cos sin ***est*** diagonalisable dans M2 ( C ) , mais pas dans M2 ( R )

**37094**: Alors u ***est*** diagonalisable si , et seulement si , dim Eu ( ) dim E Démonstration Le sens direct est immédiat en considérant une base adaptée à la décomposition de E en somme directe d' espaces propres de u. Notons Sp(u ) 1 ,

**37112**: Alors u est diagonalisable si , et seulement si , dim Eu ( ) dim E Démonstration Le sens direct ***est*** immédiat en considérant une base adaptée à la décomposition de E en somme directe d' espaces propres de u. Notons Sp(u ) 1 ,

**37143**: , k et ***supposons*** que dim Eu ( i ) dim E Pour tout i 1 , k , soit Ei une base de Eu ( i )

**37180**: , Ek ) ***est*** une famille libre ( car les espaces propres sont en somme directe , proposition 4.1 , page 195 ) qui a i1 dim Eu ( i ) dim E éléments par hypothèse

**37189**: , Ek ) est une famille libre ( car les espaces propres ***sont*** en somme directe , proposition 4.1 , page 195 ) qui a i1 dim Eu ( i ) dim E éléments par hypothèse

**37201**: , Ek ) est une famille libre ( car les espaces propres sont en somme directe , proposition 4.1 , page 195 ) qui ***a*** i1 dim Eu ( i ) dim E éléments par hypothèse

**37216**: On en ***déduit*** que E est une base de E formée de vecteurs propres de u , donc u est diagonalisable

**37219**: On en déduit que E ***est*** une base de E formée de vecteurs propres de u , donc u est diagonalisable

**37233**: On en déduit que E est une base de E formée de vecteurs propres de u , donc u ***est*** diagonalisable

**37264**: Alors u ***est*** diagonalisable si , et seulement si , u est scindé et pour tout Sp(u ) , dim Eu ( ) multu ( ) Par définition , un polynôme P KX non constant est scindé si il est de la forme avec a K et , pour tout i 1 , k , i K Les i ne sont pas nécessairement tous différents

**37273**: Alors u est diagonalisable si , et seulement si , u ***est*** scindé et pour tout Sp(u ) , dim Eu ( ) multu ( ) Par définition , un polynôme P KX non constant est scindé si il est de la forme avec a K et , pour tout i 1 , k , i K Les i ne sont pas nécessairement tous différents

**37297**: Alors u est diagonalisable si , et seulement si , u est scindé et pour tout Sp(u ) , dim Eu ( ) multu ( ) Par définition , un polynôme P KX non constant ***est*** scindé si il est de la forme avec a K et , pour tout i 1 , k , i K Les i ne sont pas nécessairement tous différents

**37301**: Alors u est diagonalisable si , et seulement si , u est scindé et pour tout Sp(u ) , dim Eu ( ) multu ( ) Par définition , un polynôme P KX non constant est scindé si il ***est*** de la forme avec a K et , pour tout i 1 , k , i K Les i ne sont pas nécessairement tous différents

**37306**: Alors u est diagonalisable si , et seulement si , u est scindé et pour tout Sp(u ) , dim Eu ( ) multu ( ) Par définition , un polynôme P KX non constant est scindé si il est de la forme avec ***a*** K et , pour tout i 1 , k , i K Les i ne sont pas nécessairement tous différents

**37322**: Alors u est diagonalisable si , et seulement si , u est scindé et pour tout Sp(u ) , dim Eu ( ) multu ( ) Par définition , un polynôme P KX non constant est scindé si il est de la forme avec a K et , pour tout i 1 , k , i K Les i ne ***sont*** pas nécessairement tous différents

**37329**: Démonstration ***Supposons*** que u soit diagonalisable

**37358**: Dans une base E de E adaptée à la décomposition la matrice A MatE ( u ) de u dans la base E ***est*** diagonale : On a donc ( n dim E ) ce qui démontre que u est scindé

**37362**: Dans une base E de E adaptée à la décomposition la matrice A MatE ( u ) de u dans la base E est diagonale : On ***a*** donc ( n dim E ) ce qui démontre que u est scindé

**37371**: Dans une base E de E adaptée à la décomposition la matrice A MatE ( u ) de u dans la base E est diagonale : On a donc ( n dim E ) ce qui ***démontre*** que u est scindé

**37374**: Dans une base E de E adaptée à la décomposition la matrice A MatE ( u ) de u dans la base E est diagonale : On a donc ( n dim E ) ce qui démontre que u ***est*** scindé

**37381**: De plus , il ***est*** de degré n ( propriété 4.2 , page 194 ) donc multu ( i ) Mais pour tout i 1 , k , 1 di multu ( i ) ( propriété 4.4 , page 195 ) , ce qui implique que , pour tout Supposons que u soit scindé et que , pour tout i 1 , k , di multu ( i )

**37421**: De plus , il est de degré n ( propriété 4.2 , page 194 ) donc multu ( i ) Mais pour tout i 1 , k , 1 di multu ( i ) ( propriété 4.4 , page 195 ) , ce qui ***implique*** que , pour tout Supposons que u soit scindé et que , pour tout i 1 , k , di multu ( i )

**37426**: De plus , il est de degré n ( propriété 4.2 , page 194 ) donc multu ( i ) Mais pour tout i 1 , k , 1 di multu ( i ) ( propriété 4.4 , page 195 ) , ce qui implique que , pour tout ***Supposons*** que u soit scindé et que , pour tout i 1 , k , di multu ( i )

**37452**: Puisque les zéros de u ***sont*** les valeurs propres de u , on a : dj n dim E D' après la propriété 4.5 , page précédente , u est diagonalisable

**37460**: Puisque les zéros de u sont les valeurs propres de u , on ***a*** : dj n dim E D' après la propriété 4.5 , page précédente , u est diagonalisable

**37476**: Puisque les zéros de u sont les valeurs propres de u , on a : dj n dim E D' après la propriété 4.5 , page précédente , u ***est*** diagonalisable

**37481**: Si u ***admet*** n valeurs propres toutes différentes ( n dim E ) , alors u est diagonalisable car dim Eu ( ) multu ( ) 1 pour tout Sp(u )

**37495**: Si u admet n valeurs propres toutes différentes ( n dim E ) , alors u ***est*** diagonalisable car dim Eu ( ) multu ( ) 1 pour tout Sp(u )

**37512**: C' ***est*** le seul cas où il ne est pas nécessaire de calculer les dimensions des espaces propres de u , et où l' étude du polynôme caractéristique u suffit pour démontrer que u est diagonalisable

**37519**: C' est le seul cas où il ne ***est*** pas nécessaire de calculer les dimensions des espaces propres de u , et où l' étude du polynôme caractéristique u suffit pour démontrer que u est diagonalisable

**37545**: C' est le seul cas où il ne est pas nécessaire de calculer les dimensions des espaces propres de u , et où l' étude du polynôme caractéristique u suffit pour démontrer que u ***est*** diagonalisable

**37549**: On ***retrouve*** le fait qu' il est indispensable de préciser le corps dans lequel on se place quand on s' intéresse à la diagonalisation d' une matrice : sur C tous les polynômes non constants sont scindés , ce qui est faux sur R. La matrice de l' exemple 4.1 , page 184 ne est pas diagonalisable ( sur R ou sur C )

**37554**: On retrouve le fait qu' il ***est*** indispensable de préciser le corps dans lequel on se place quand on s' intéresse à la diagonalisation d' une matrice : sur C tous les polynômes non constants sont scindés , ce qui est faux sur R. La matrice de l' exemple 4.1 , page 184 ne est pas diagonalisable ( sur R ou sur C )

**37564**: On retrouve le fait qu' il est indispensable de préciser le corps dans lequel on se ***place*** quand on s' intéresse à la diagonalisation d' une matrice : sur C tous les polynômes non constants sont scindés , ce qui est faux sur R. La matrice de l' exemple 4.1 , page 184 ne est pas diagonalisable ( sur R ou sur C )

**37568**: On retrouve le fait qu' il est indispensable de préciser le corps dans lequel on se place quand on s' ***intéresse*** à la diagonalisation d' une matrice : sur C tous les polynômes non constants sont scindés , ce qui est faux sur R. La matrice de l' exemple 4.1 , page 184 ne est pas diagonalisable ( sur R ou sur C )

**37583**: On retrouve le fait qu' il est indispensable de préciser le corps dans lequel on se place quand on s' intéresse à la diagonalisation d' une matrice : sur C tous les polynômes non constants ***sont*** scindés , ce qui est faux sur R. La matrice de l' exemple 4.1 , page 184 ne est pas diagonalisable ( sur R ou sur C )

**37588**: On retrouve le fait qu' il est indispensable de préciser le corps dans lequel on se place quand on s' intéresse à la diagonalisation d' une matrice : sur C tous les polynômes non constants sont scindés , ce qui ***est*** faux sur R. La matrice de l' exemple 4.1 , page 184 ne est pas diagonalisable ( sur R ou sur C )

**37602**: On retrouve le fait qu' il est indispensable de préciser le corps dans lequel on se place quand on s' intéresse à la diagonalisation d' une matrice : sur C tous les polynômes non constants sont scindés , ce qui est faux sur R. La matrice de l' exemple 4.1 , page 184 ne ***est*** pas diagonalisable ( sur R ou sur C )

**37622**: La matrice de l' exemple 4.2 , page 189 ***est*** diagonalisable sur K. L' endomorphisme de l' exemple 4.3 , page 191 est diagonalisable

**37635**: La matrice de l' exemple 4.2 , page 189 est diagonalisable sur K. L' endomorphisme de l' exemple 4.3 , page 191 ***est*** diagonalisable

**37648**: Une rotation du plan euclidien R2 d' angle R.Z ne ***est*** pas diagonalisable

**37656**: Cependant , sa matrice ***est*** diagonalisable sur C. 2 2 Mn ( C ) est diagonalisable

**37666**: Cependant , sa matrice est diagonalisable sur C. 2 2 Mn ( C ) ***est*** diagonalisable

**37681**: 4.1.2 Diagonaliser ( c' est - à - dire démontrer qu' elle ***est*** diagonalisable et l' écrire sous la forme P D P 1 avec P GL3 ( R ) et D M3 ( R ) diagonale ) la matrice suivante : 4.1.3 Démontrer que les matrices suivantes sont semblables : 4.1.4 Diagonaliser l' endomorphisme de Kn X défini par ( pour n N ) : 4.1.5 Soit A Mn ( K ) diagonalisable et B Mp ( K ) diagonalisable , démontrer que A B est diagonalisable et préciser les éléments propres de A B en fonction de ceux de A et de B. 4.1.6 Trouver une condition nécessaire et suffisante sur A Mn ( C ) pour que soit diagonalisable 4.1.7 Démontrer que toute matrice circulante , c' est - à - dire de la forme : est diagonalisable dans Mn ( C ) et donner ses éléments propres

**37717**: 4.1.2 Diagonaliser ( c' est - à - dire démontrer qu' elle est diagonalisable et l' écrire sous la forme P D P 1 avec P GL3 ( R ) et D M3 ( R ) diagonale ) la matrice suivante : 4.1.3 Démontrer que les matrices suivantes ***sont*** semblables : 4.1.4 Diagonaliser l' endomorphisme de Kn X défini par ( pour n N ) : 4.1.5 Soit A Mn ( K ) diagonalisable et B Mp ( K ) diagonalisable , démontrer que A B est diagonalisable et préciser les éléments propres de A B en fonction de ceux de A et de B. 4.1.6 Trouver une condition nécessaire et suffisante sur A Mn ( C ) pour que soit diagonalisable 4.1.7 Démontrer que toute matrice circulante , c' est - à - dire de la forme : est diagonalisable dans Mn ( C ) et donner ses éléments propres

**37755**: 4.1.2 Diagonaliser ( c' est - à - dire démontrer qu' elle est diagonalisable et l' écrire sous la forme P D P 1 avec P GL3 ( R ) et D M3 ( R ) diagonale ) la matrice suivante : 4.1.3 Démontrer que les matrices suivantes sont semblables : 4.1.4 Diagonaliser l' endomorphisme de Kn X défini par ( pour n N ) : 4.1.5 Soit A Mn ( K ) diagonalisable et B Mp ( K ) diagonalisable , démontrer que A B ***est*** diagonalisable et préciser les éléments propres de A B en fonction de ceux de A et de B. 4.1.6 Trouver une condition nécessaire et suffisante sur A Mn ( C ) pour que soit diagonalisable 4.1.7 Démontrer que toute matrice circulante , c' est - à - dire de la forme : est diagonalisable dans Mn ( C ) et donner ses éléments propres

**37808**: 4.1.2 Diagonaliser ( c' est - à - dire démontrer qu' elle est diagonalisable et l' écrire sous la forme P D P 1 avec P GL3 ( R ) et D M3 ( R ) diagonale ) la matrice suivante : 4.1.3 Démontrer que les matrices suivantes sont semblables : 4.1.4 Diagonaliser l' endomorphisme de Kn X défini par ( pour n N ) : 4.1.5 Soit A Mn ( K ) diagonalisable et B Mp ( K ) diagonalisable , démontrer que A B est diagonalisable et préciser les éléments propres de A B en fonction de ceux de A et de B. 4.1.6 Trouver une condition nécessaire et suffisante sur A Mn ( C ) pour que soit diagonalisable 4.1.7 Démontrer que toute matrice circulante , c' est - à - dire de la forme : ***est*** diagonalisable dans Mn ( C ) et donner ses éléments propres

**37910**: Démontrer l' équivalence de : ( ***a*** ) u diagonalisable ( b ) tout sous-espace de E stable par u admet un supplémentaire stable par u. 4.1.13 Trouver les valeurs propres et vecteurs propres de l' endomorphisme de Rn X défini par : Trigonalisation Soit E un K - espace vectoriel de dimension finie n 1 et soit u L ( E )

**37924**: Démontrer l' équivalence de : ( a ) u diagonalisable ( b ) tout sous-espace de E stable par u ***admet*** un supplémentaire stable par u. 4.1.13 Trouver les valeurs propres et vecteurs propres de l' endomorphisme de Rn X défini par : Trigonalisation Soit E un K - espace vectoriel de dimension finie n 1 et soit u L ( E )

**37969**: On ***dit*** que u est dit trigonalisable si il existe un drapeau stable pour u , c' est - à - dire des sous-espaces vectoriels V1 ,

**37972**: On dit que u ***est*** dit trigonalisable si il existe un drapeau stable pour u , c' est - à - dire des sous-espaces vectoriels V1 ,

**37973**: On dit que u est ***dit*** trigonalisable si il existe un drapeau stable pour u , c' est - à - dire des sous-espaces vectoriels V1 ,

**37977**: On dit que u est dit trigonalisable si il ***existe*** un drapeau stable pour u , c' est - à - dire des sous-espaces vectoriels V1 ,

**38003**: , Vn de E qui ***sont*** stables par u et tels que : Nécessairement pour tout k 1 , n , dim Vk k. u est trigonalisable si , et seulement si , il existe une base de E dans laquelle la matrice de u est triangulaire supérieure ( considérer une base ( e1 ,

**38023**: , Vn de E qui sont stables par u et tels que : Nécessairement pour tout k 1 , n , dim Vk k. u ***est*** trigonalisable si , et seulement si , il existe une base de E dans laquelle la matrice de u est triangulaire supérieure ( considérer une base ( e1 ,

**38032**: , Vn de E qui sont stables par u et tels que : Nécessairement pour tout k 1 , n , dim Vk k. u est trigonalisable si , et seulement si , il ***existe*** une base de E dans laquelle la matrice de u est triangulaire supérieure ( considérer une base ( e1 ,

**38043**: , Vn de E qui sont stables par u et tels que : Nécessairement pour tout k 1 , n , dim Vk k. u est trigonalisable si , et seulement si , il existe une base de E dans laquelle la matrice de u ***est*** triangulaire supérieure ( considérer une base ( e1 ,

**38077**: , en ) de E adaptée au drapeau , c' est - à - dire telle La diagonale de cette matrice ***est*** alors constituée des valeurs propres de u ( avec leurs multiplicités )

**38092**: On ***dit*** qu' une matrice A Mn ( K ) est trigonalisable si , et seulement si , son endomorphisme canoniquement associé l' est

**38101**: On dit qu' une matrice A Mn ( K ) ***est*** trigonalisable si , et seulement si , son endomorphisme canoniquement associé l' est

**38114**: On dit qu' une matrice A Mn ( K ) est trigonalisable si , et seulement si , son endomorphisme canoniquement associé l' ***est***

**38117**: Autrement ***dit*** , une matrice est trigonalisable si elle est semblable à une matrice triangulaire supérieure

**38121**: Autrement dit , une matrice ***est*** trigonalisable si elle est semblable à une matrice triangulaire supérieure

**38125**: Autrement dit , une matrice est trigonalisable si elle ***est*** semblable à une matrice triangulaire supérieure

**38141**: De même que pour la diagonalisation , il ***est*** indispensable de préciser le corps K dans lequel on travaille

**38151**: De même que pour la diagonalisation , il est indispensable de préciser le corps K dans lequel on ***travaille***

**38181**: Alors u ***est*** trigonalisable si , et seulement si , u est scindé Démonstration Supposons que u soit trigonalisable

**38190**: Alors u est trigonalisable si , et seulement si , u ***est*** scindé Démonstration Supposons que u soit trigonalisable

**38193**: Alors u est trigonalisable si , et seulement si , u est scindé Démonstration ***Supposons*** que u soit trigonalisable

**38200**: Il ***existe*** une base E de E telle que A MatE ( u ) ai , j ( i , j)1,n2 Mn ( K ) ( avec n dim E ) soit triangulaire supérieure

**38235**: On ***a*** alors : donc u est scindé

**38240**: On a alors : donc u ***est*** scindé

**38243**: ***Démontrons*** la réciproque par récurrence sur n dim E. Pour n 1 , il ne y a rien à démontrer

**38259**: Démontrons la réciproque par récurrence sur n dim E. Pour n 1 , il ne y ***a*** rien à démontrer

**38268**: ***Supposons*** le résultat vrai au rang n. Soit u L ( E ) avec dim E n 1 tel que u soit scindé

**38296**: En particulier , il ***admet*** au moins une racine donc il existe un vecteur propre e1 de u : u(e1 ) 1 .e1 avec 1 K la valeur propre associée à e1

**38303**: En particulier , il admet au moins une racine donc il ***existe*** un vecteur propre e1 de u : u(e1 ) 1 .e1 avec 1 K la valeur propre associée à e1

**38331**: Comme e1 6 0E , on ***peut*** compléter ( e1 ) Puisque u est scindé , nécessairement N est aussi scindé

**38338**: Comme e1 6 0E , on peut compléter ( e1 ) Puisque u ***est*** scindé , nécessairement N est aussi scindé

**38343**: Comme e1 6 0E , on peut compléter ( e1 ) Puisque u est scindé , nécessairement N ***est*** aussi scindé

**38360**: , en1 ) qui ***est*** de dimension n , H la projection de H parallèlement à Vect(e1 ) et H u H L ( H ) qui a pour polynôme caractéristique N qui est scindé

**38383**: , en1 ) qui est de dimension n , H la projection de H parallèlement à Vect(e1 ) et H u H L ( H ) qui ***a*** pour polynôme caractéristique N qui est scindé

**38389**: , en1 ) qui est de dimension n , H la projection de H parallèlement à Vect(e1 ) et H u H L ( H ) qui a pour polynôme caractéristique N qui ***est*** scindé

**38400**: Par hypothèse de récurrence , H u H ***est*** trigonalisable donc il existe une base ( h2 ,

**38404**: Par hypothèse de récurrence , H u H est trigonalisable donc il ***existe*** une base ( h2 ,

**38456**: , hn1 ) ***est*** triangulaire supérieure , donc u est trigonalisable

**38462**: , hn1 ) est triangulaire supérieure , donc u ***est*** trigonalisable

**38472**: Par principe de récurrence , le résultat ***est*** vraie pour tout n N

**38496**: En particulier , tous les endomorphismes d' un C - espace vectoriel de dimension finie non nulle ***sont*** trigonalisables ( car les polynômes non constants sont scindés sur C )

**38504**: En particulier , tous les endomorphismes d' un C - espace vectoriel de dimension finie non nulle sont trigonalisables ( car les polynômes non constants ***sont*** scindés sur C )

**38511**: Ils ***possèdent*** des espaces stables de dimension quelconque

**38527**: La matrice : se trigonalise dans C. Il ***suffit*** pour effectuer la trigonalisation de suivre la démonstration du théorème

**38561**: Soit u L ( R ) tel que A MatC ( u ) où C ( c1 , c2 , c3 ) ***est*** la base canonique de R3

**38580**: On a vu que le vecteur e1 c1 c2 c3 ***est*** vecteur propre de u associé à la valeur propre 2

**38593**: On ***peut*** compléter en une base B1 ( e1 , e2 , e3 ) en prenant e2 c1 et e3 c2

**38619**: Bien vérifier que l' on ***a*** une base !

**38638**: Pour trouver la matrice de u dans la base B1 , on ***procède*** comme ceci : de la matrice A )

**38654**: On ***peut*** vérifier en Wxmaxima , mais il faut savoir le faire à la main ! On recommence sur la sous-matrice 2 2 : A1 est la matrice d' un endomorphisme u1 de E1 dans la base ( e2 , e3 ) , où E1 Vecte2 , e3

**38661**: On peut vérifier en Wxmaxima , mais il ***faut*** savoir le faire à la main ! On recommence sur la sous-matrice 2 2 : A1 est la matrice d' un endomorphisme u1 de E1 dans la base ( e2 , e3 ) , où E1 Vecte2 , e3

**38670**: On peut vérifier en Wxmaxima , mais il faut savoir le faire à la main ! On ***recommence*** sur la sous-matrice 2 2 : A1 est la matrice d' un endomorphisme u1 de E1 dans la base ( e2 , e3 ) , où E1 Vecte2 , e3

**38678**: On peut vérifier en Wxmaxima , mais il faut savoir le faire à la main ! On recommence sur la sous-matrice 2 2 : A1 ***est*** la matrice d' un endomorphisme u1 de E1 dans la base ( e2 , e3 ) , où E1 Vecte2 , e3

**38705**: On ***cherche*** un vecteur propre de A1 , il vérifie : ce qui donne immédiatement : x a , y a où a 6 0 , par exemple ( x , y ) ( 1 , 1 )

**38713**: On cherche un vecteur propre de A1 , il ***vérifie*** : ce qui donne immédiatement : x a , y a où a 6 0 , par exemple ( x , y ) ( 1 , 1 )

**38717**: On cherche un vecteur propre de A1 , il vérifie : ce qui ***donne*** immédiatement : x a , y a où a 6 0 , par exemple ( x , y ) ( 1 , 1 )

**38721**: On cherche un vecteur propre de A1 , il vérifie : ce qui donne immédiatement : x ***a*** , y a où a 6 0 , par exemple ( x , y ) ( 1 , 1 )

**38724**: On cherche un vecteur propre de A1 , il vérifie : ce qui donne immédiatement : x a , y ***a*** où a 6 0 , par exemple ( x , y ) ( 1 , 1 )

**38726**: On cherche un vecteur propre de A1 , il vérifie : ce qui donne immédiatement : x a , y a où ***a*** 6 0 , par exemple ( x , y ) ( 1 , 1 )

**38747**: Ce vecteur ***est*** exprimé dans la base ( e2 , e3 ) , c' est donc : où b1 est choisi pour conserver le vecteur propre de A et b3 est choisi pour compléter b2 en une base 4

**38759**: Ce vecteur est exprimé dans la base ( e2 , e3 ) , c' ***est*** donc : où b1 est choisi pour conserver le vecteur propre de A et b3 est choisi pour compléter b2 en une base 4

**38764**: Ce vecteur est exprimé dans la base ( e2 , e3 ) , c' est donc : où b1 ***est*** choisi pour conserver le vecteur propre de A et b3 est choisi pour compléter b2 en une base 4

**38775**: Ce vecteur est exprimé dans la base ( e2 , e3 ) , c' est donc : où b1 est choisi pour conserver le vecteur propre de A et b3 ***est*** choisi pour compléter b2 en une base 4

**38786**: On ***a*** alors : Finalement : Vérifions en Wxmaxima : Soit E un K - espace vectoriel de dimension finie non nulle , soit u E et soit F 6 0E un sous-espace vectoriel de E stable par u ( c' est - à - dire u(F ) F )

**38842**: si u ***est*** diagonalisable , alors u F est diagonalisable 3

**38848**: si u est diagonalisable , alors u F ***est*** diagonalisable 3

**38854**: si u ***est*** trigonalisable , alors u F est trigonalisable

**38860**: si u est trigonalisable , alors u F ***est*** trigonalisable

**38879**: diagonalisable , il ***existe*** une base de vecteurs propres de E donc on peut écrire : avec , pour tout i 1 , k , xi Eu ( i ) On a donc : Autrement dit , on a un système linéaire d' inconnues x1 ,

**38889**: diagonalisable , il existe une base de vecteurs propres de E donc on ***peut*** écrire : avec , pour tout i 1 , k , xi Eu ( i ) On a donc : Autrement dit , on a un système linéaire d' inconnues x1 ,

**38907**: diagonalisable , il existe une base de vecteurs propres de E donc on peut écrire : avec , pour tout i 1 , k , xi Eu ( i ) On ***a*** donc : Autrement dit , on a un système linéaire d' inconnues x1 ,

**38911**: diagonalisable , il existe une base de vecteurs propres de E donc on peut écrire : avec , pour tout i 1 , k , xi Eu ( i ) On a donc : Autrement ***dit*** , on a un système linéaire d' inconnues x1 ,

**38914**: diagonalisable , il existe une base de vecteurs propres de E donc on peut écrire : avec , pour tout i 1 , k , xi Eu ( i ) On a donc : Autrement dit , on ***a*** un système linéaire d' inconnues x1 ,

**38953**: On ***a*** alors On a donc un système linéaire dont le déterminant de la matrice associée est un déterminant de Vandermonde ( proposition 3.2 , page 173 )

**38956**: On a alors On ***a*** donc un système linéaire dont le déterminant de la matrice associée est un déterminant de Vandermonde ( proposition 3.2 , page 173 )

**38968**: On a alors On a donc un système linéaire dont le déterminant de la matrice associée ***est*** un déterminant de Vandermonde ( proposition 3.2 , page 173 )

**38989**: En particulier , comme les valeurs propres i ***sont*** toutes différentes , ce déterminant est non nul donc le système admet une unique solution

**38995**: En particulier , comme les valeurs propres i sont toutes différentes , ce déterminant ***est*** non nul donc le système admet une unique solution

**39001**: En particulier , comme les valeurs propres i sont toutes différentes , ce déterminant est non nul donc le système ***admet*** une unique solution

**39007**: On ***peut*** donc écrire : Puisque que ceci est vrai pour toute forme linéaire E ? et en remarquant que les i , j ne dépendent pas de ( formules de Cramer ) , on a : en utilisant le fait que F est stable par tous les uj

**39014**: On peut donc écrire : Puisque que ceci ***est*** vrai pour toute forme linéaire E ? et en remarquant que les i , j ne dépendent pas de ( formules de Cramer ) , on a : en utilisant le fait que F est stable par tous les uj

**39031**: On peut donc écrire : Puisque que ceci est vrai pour toute forme linéaire E ? et en remarquant que les i , j ne ***dépendent*** pas de ( formules de Cramer ) , on a : en utilisant le fait que F est stable par tous les uj

**39041**: On peut donc écrire : Puisque que ceci est vrai pour toute forme linéaire E ? et en remarquant que les i , j ne dépendent pas de ( formules de Cramer ) , on ***a*** : en utilisant le fait que F est stable par tous les uj

**39049**: On peut donc écrire : Puisque que ceci est vrai pour toute forme linéaire E ? et en remarquant que les i , j ne dépendent pas de ( formules de Cramer ) , on a : en utilisant le fait que F ***est*** stable par tous les uj

**39066**: On a donc démontré que tout élément de F s' ***écrit*** de manière unique comme une somme de vecteurs propres de u qui sont dans F , d' où : d' après le point 1

**39079**: On a donc démontré que tout élément de F s' écrit de manière unique comme une somme de vecteurs propres de u qui ***sont*** dans F , d' où : d' après le point 1

**39094**: On en ***déduit*** que v u F est diagonalisable

**39099**: On en déduit que v u F ***est*** diagonalisable

**39111**: Si F E , il ne y ***a*** rien à démontrer

**39137**: Supposons F 6 E. Soit F une base de F que l' on complète en une base E de E. On ***a*** alors ( n dim E et p dim F ) : On en déduit que u v D

**39151**: Supposons F 6 E. Soit F une base de F que l' on complète en une base E de E. On a alors ( n dim E et p dim F ) : On en ***déduit*** que u v D

**39159**: Or u ***est*** scindé car u est trigonalisable donc nécessairement v est aussi scindé , ce qui démontre que v est trigonalisable

**39163**: Or u est scindé car u ***est*** trigonalisable donc nécessairement v est aussi scindé , ce qui démontre que v est trigonalisable

**39168**: Or u est scindé car u est trigonalisable donc nécessairement v ***est*** aussi scindé , ce qui démontre que v est trigonalisable

**39174**: Or u est scindé car u est trigonalisable donc nécessairement v est aussi scindé , ce qui ***démontre*** que v est trigonalisable

**39177**: Or u est scindé car u est trigonalisable donc nécessairement v est aussi scindé , ce qui démontre que v ***est*** trigonalisable

**39181**: On ***peut*** démontrer le point 2 beaucoup plus rapidement avec la notion de polynôme d' endomorphisme ( voir le chapitre suivant )

**39204**: On ***peut*** retenir cette idée : lorsqu' on a un système linéaire dont les inconnues x1 ,

**39211**: On peut retenir cette idée : lorsqu' on ***a*** un système linéaire dont les inconnues x1 ,

**39225**: , xp ***sont*** des vecteurs d' un espace vectoriel E : ai , j .xj bi on peut utiliser la dualité

**39240**: , xp sont des vecteurs d' un espace vectoriel E : ai , j .xj bi on ***peut*** utiliser la dualité

**39246**: On ***considère*** une forme linéaire quelconque E ? et on obtient le système dont les inconnues ( x1 ) ,

**39255**: On considère une forme linéaire quelconque E ? et on ***obtient*** le système dont les inconnues ( x1 ) ,

**39272**: , ( xp ) ***sont*** des scalaires , que l' on sait résoudre ( typiquement avec l' al- gorithme du pivot de Gauss )

**39279**: , ( xp ) sont des scalaires , que l' on ***sait*** résoudre ( typiquement avec l' al- gorithme du pivot de Gauss )

**39295**: Si elles ***existent*** , les solutions s' écrivent sous la forme : Comme ceci est vrai pour toute forme linéaire E ? et les i , j ne dépendent pas de , on a Soit E un K - espace vectoriel de dimension finie et soit u et v deux endomorphismes de E qui commutent Démonstration Soit Sp(u ) et soit x Eu ( )

**39300**: Si elles existent , les solutions s' ***écrivent*** sous la forme : Comme ceci est vrai pour toute forme linéaire E ? et les i , j ne dépendent pas de , on a Soit E un K - espace vectoriel de dimension finie et soit u et v deux endomorphismes de E qui commutent Démonstration Soit Sp(u ) et soit x Eu ( )

**39307**: Si elles existent , les solutions s' écrivent sous la forme : Comme ceci ***est*** vrai pour toute forme linéaire E ? et les i , j ne dépendent pas de , on a Soit E un K - espace vectoriel de dimension finie et soit u et v deux endomorphismes de E qui commutent Démonstration Soit Sp(u ) et soit x Eu ( )

**39321**: Si elles existent , les solutions s' écrivent sous la forme : Comme ceci est vrai pour toute forme linéaire E ? et les i , j ne ***dépendent*** pas de , on a Soit E un K - espace vectoriel de dimension finie et soit u et v deux endomorphismes de E qui commutent Démonstration Soit Sp(u ) et soit x Eu ( )

**39326**: Si elles existent , les solutions s' écrivent sous la forme : Comme ceci est vrai pour toute forme linéaire E ? et les i , j ne dépendent pas de , on ***a*** Soit E un K - espace vectoriel de dimension finie et soit u et v deux endomorphismes de E qui commutent Démonstration Soit Sp(u ) et soit x Eu ( )

**39347**: Si elles existent , les solutions s' écrivent sous la forme : Comme ceci est vrai pour toute forme linéaire E ? et les i , j ne dépendent pas de , on a Soit E un K - espace vectoriel de dimension finie et soit u et v deux endomorphismes de E qui ***commutent*** Démonstration Soit Sp(u ) et soit x Eu ( )

**39360**: On ***a*** donc u(x ) .x donc : mais comme u et v commutent , on a v u(x ) u v(x ) donc : c' est - à - dire v(x ) Eu ( )

**39372**: On a donc u(x ) .x donc : mais comme u et v ***commutent*** , on a v u(x ) u v(x ) donc : c' est - à - dire v(x ) Eu ( )

**39375**: On a donc u(x ) .x donc : mais comme u et v commutent , on ***a*** v u(x ) u v(x ) donc : c' est - à - dire v(x ) Eu ( )

**39423**: Théorème 4.3 Critère de co-diagonalisation Soit E un K - espace vectoriel de dimension finie non nulle et soit u et v deux endomorphismes de E qui ***sont*** diagonalisables

**39430**: Alors u et v ***commutent*** ( u v v u ) si , et seulement si , ils sont co-diagonalisables , c' est - à - dire qu' ils se diagonalisent dans une même base de vecteurs propres

**39444**: Alors u et v commutent ( u v v u ) si , et seulement si , ils ***sont*** co-diagonalisables , c' est - à - dire qu' ils se diagonalisent dans une même base de vecteurs propres

**39466**: Démonstration ***Supposons*** que u et v soient co-diagonalisables

**39475**: Il ***existe*** une base E de E dans laquelle les matrices MatE ( u ) et MatE ( v ) sont diagonales

**39494**: Il existe une base E de E dans laquelle les matrices MatE ( u ) et MatE ( v ) ***sont*** diagonales

**39501**: En particulier , elles ***commutent*** donc : MatE ( u v ) MatE ( u ) MatE ( v ) MatE ( v ) MatE ( u ) MatE ( v u ) Supposons que u v v u. Soit Sp(u )

**39530**: En particulier , elles commutent donc : MatE ( u v ) MatE ( u ) MatE ( v ) MatE ( v ) MatE ( u ) MatE ( v u ) ***Supposons*** que u v v u. Soit Sp(u )

**39554**: D' après la propriété 4.7 , de la présente page , Eu ( ) ***est*** stable par v. Comme v est diagonalisable , on en déduit que v Eu ( ) est diagonalisable ( propriété 4.6 , page 203 )

**39560**: D' après la propriété 4.7 , de la présente page , Eu ( ) est stable par v. Comme v ***est*** diagonalisable , on en déduit que v Eu ( ) est diagonalisable ( propriété 4.6 , page 203 )

**39565**: D' après la propriété 4.7 , de la présente page , Eu ( ) est stable par v. Comme v est diagonalisable , on en ***déduit*** que v Eu ( ) est diagonalisable ( propriété 4.6 , page 203 )

**39571**: D' après la propriété 4.7 , de la présente page , Eu ( ) est stable par v. Comme v est diagonalisable , on en déduit que v Eu ( ) ***est*** diagonalisable ( propriété 4.6 , page 203 )

**39582**: Il ***existe*** donc une base de vecteurs propres ( pour v ) de Eu ( )

**39602**: Mais ces vecteurs propres ***sont*** dans Eu ( ) , donc ce sont aussi des vecteurs propres pour u. Puisque u est diagonalisable , on a D' après ce qui précède , pour toute Sp(u ) , on peut considérer une base de Eu ( ) de vecteurs propres à la fois de u et de v , donc on construit par somme directe une base de E de vecteurs propres à la fois de u et de v , donc u et v sont co-diagonalisables

**39610**: Mais ces vecteurs propres sont dans Eu ( ) , donc ce ***sont*** aussi des vecteurs propres pour u. Puisque u est diagonalisable , on a D' après ce qui précède , pour toute Sp(u ) , on peut considérer une base de Eu ( ) de vecteurs propres à la fois de u et de v , donc on construit par somme directe une base de E de vecteurs propres à la fois de u et de v , donc u et v sont co-diagonalisables

**39619**: Mais ces vecteurs propres sont dans Eu ( ) , donc ce sont aussi des vecteurs propres pour u. Puisque u ***est*** diagonalisable , on a D' après ce qui précède , pour toute Sp(u ) , on peut considérer une base de Eu ( ) de vecteurs propres à la fois de u et de v , donc on construit par somme directe une base de E de vecteurs propres à la fois de u et de v , donc u et v sont co-diagonalisables

**39623**: Mais ces vecteurs propres sont dans Eu ( ) , donc ce sont aussi des vecteurs propres pour u. Puisque u est diagonalisable , on ***a*** D' après ce qui précède , pour toute Sp(u ) , on peut considérer une base de Eu ( ) de vecteurs propres à la fois de u et de v , donc on construit par somme directe une base de E de vecteurs propres à la fois de u et de v , donc u et v sont co-diagonalisables

**39628**: Mais ces vecteurs propres sont dans Eu ( ) , donc ce sont aussi des vecteurs propres pour u. Puisque u est diagonalisable , on a D' après ce qui ***précède*** , pour toute Sp(u ) , on peut considérer une base de Eu ( ) de vecteurs propres à la fois de u et de v , donc on construit par somme directe une base de E de vecteurs propres à la fois de u et de v , donc u et v sont co-diagonalisables

**39636**: Mais ces vecteurs propres sont dans Eu ( ) , donc ce sont aussi des vecteurs propres pour u. Puisque u est diagonalisable , on a D' après ce qui précède , pour toute Sp(u ) , on ***peut*** considérer une base de Eu ( ) de vecteurs propres à la fois de u et de v , donc on construit par somme directe une base de E de vecteurs propres à la fois de u et de v , donc u et v sont co-diagonalisables

**39658**: Mais ces vecteurs propres sont dans Eu ( ) , donc ce sont aussi des vecteurs propres pour u. Puisque u est diagonalisable , on a D' après ce qui précède , pour toute Sp(u ) , on peut considérer une base de Eu ( ) de vecteurs propres à la fois de u et de v , donc on ***construit*** par somme directe une base de E de vecteurs propres à la fois de u et de v , donc u et v sont co-diagonalisables

**39682**: Mais ces vecteurs propres sont dans Eu ( ) , donc ce sont aussi des vecteurs propres pour u. Puisque u est diagonalisable , on a D' après ce qui précède , pour toute Sp(u ) , on peut considérer une base de Eu ( ) de vecteurs propres à la fois de u et de v , donc on construit par somme directe une base de E de vecteurs propres à la fois de u et de v , donc u et v ***sont*** co-diagonalisables

**39712**: Théorème 4.4 Critère de co-trigonalisation Soit E un K - espace vectoriel de dimension finie non nulle et soit u et v deux endomorphismes de E qui ***sont*** trigonalisables

**39719**: Si u et v ***commutent*** ( u v v u ) alors ils sont co-trigonalisables , c' est - à - dire qu' ils se trigonalisent dans une même base de vecteurs propres

**39728**: Si u et v commutent ( u v v u ) alors ils ***sont*** co-trigonalisables , c' est - à - dire qu' ils se trigonalisent dans une même base de vecteurs propres

**39751**: Démonstration On ***procède*** par récurrence sur n dim E. Si n 1 , il ne y a rien à démontrer

**39765**: Démonstration On procède par récurrence sur n dim E. Si n 1 , il ne y ***a*** rien à démontrer

**39774**: ***Supposons*** le résultat vrai au rang n et considérons u et v deux endomorphismes de E trigonalisables qui commutent ( avec dim E n 1 )

**39792**: Supposons le résultat vrai au rang n et considérons u et v deux endomorphismes de E trigonalisables qui ***commutent*** ( avec dim E n 1 )

**39802**: On ***démontre*** comme dans la démonstration du théorème 4.3 , page précédente qu' il existe un vecteur propre x commun à u et v. En particulier il est non nul donc on peut compléter ( x ) en une base E ( x , e2 ,

**39815**: On démontre comme dans la démonstration du théorème 4.3 , page précédente qu' il ***existe*** un vecteur propre x commun à u et v. En particulier il est non nul donc on peut compléter ( x ) en une base E ( x , e2 ,

**39828**: On démontre comme dans la démonstration du théorème 4.3 , page précédente qu' il existe un vecteur propre x commun à u et v. En particulier il ***est*** non nul donc on peut compléter ( x ) en une base E ( x , e2 ,

**39833**: On démontre comme dans la démonstration du théorème 4.3 , page précédente qu' il existe un vecteur propre x commun à u et v. En particulier il est non nul donc on ***peut*** compléter ( x ) en une base E ( x , e2 ,

**39858**: , en1 ) de E. Puisque Vect(x ) ***est*** et MatE ( v ) Comme u et v commutent , MatE ( u ) et MatE ( v ) aussi et comme : commutent aussi , ce qui donne MatE ( u ) MatE ( v ) donc M N N M

**39868**: , en1 ) de E. Puisque Vect(x ) est et MatE ( v ) Comme u et v ***commutent*** , MatE ( u ) et MatE ( v ) aussi et comme : commutent aussi , ce qui donne MatE ( u ) MatE ( v ) donc M N N M

**39883**: , en1 ) de E. Puisque Vect(x ) est et MatE ( v ) Comme u et v commutent , MatE ( u ) et MatE ( v ) aussi et comme : ***commutent*** aussi , ce qui donne MatE ( u ) MatE ( v ) donc M N N M

**39888**: , en1 ) de E. Puisque Vect(x ) est et MatE ( v ) Comme u et v commutent , MatE ( u ) et MatE ( v ) aussi et comme : commutent aussi , ce qui ***donne*** MatE ( u ) MatE ( v ) donc M N N M

**39913**: De plus , u ( X ) M et u ***est*** scindé ( car u est trigonalisable ) donc nécessairement M est scindé donc M est trigonalisable

**39918**: De plus , u ( X ) M et u est scindé ( car u ***est*** trigonalisable ) donc nécessairement M est scindé donc M est trigonalisable

**39924**: De plus , u ( X ) M et u est scindé ( car u est trigonalisable ) donc nécessairement M ***est*** scindé donc M est trigonalisable

**39928**: De plus , u ( X ) M et u est scindé ( car u est trigonalisable ) donc nécessairement M est scindé donc M ***est*** trigonalisable

**39935**: De même , N ***est*** trigonalisable

**39960**: , en1 ) , u0 h uH et v 0 H vH où H ***est*** la projection sur N commutent et sont trigonalisables , donc u0 et v 0 aussi

**39965**: , en1 ) , u0 h uH et v 0 H vH où H est la projection sur N ***commutent*** et sont trigonalisables , donc u0 et v 0 aussi

**39967**: , en1 ) , u0 h uH et v 0 H vH où H est la projection sur N commutent et ***sont*** trigonalisables , donc u0 et v 0 aussi

**39990**: Comme dim H n , d' après l' hypothèse de récurrence , il ***existe*** une base H ( h2 ,

**40014**: , hn1 ) de H dans laquelle les matrices de u0 et v 0 ***sont*** triangulaires supérieures

**40042**: , hn1 ) , les matrices de u et v ***sont*** triangulaires supérieures donc le résultat est vrai au rang n 1

**40048**: , hn1 ) , les matrices de u et v sont triangulaires supérieures donc le résultat ***est*** vrai au rang n 1

**40062**: Par principe de récurrence , le résultat ***est*** vrai pour tout n dim E N

**40072**: On ***peut*** généraliser les résultats des théorèmes 4.3 , page précédente et 4.4 , page précédente à un nombre quelconque d' endomorphismes qui commutent deux à deux

**40094**: On peut généraliser les résultats des théorèmes 4.3 , page précédente et 4.4 , page précédente à un nombre quelconque d' endomorphismes qui ***commutent*** deux à deux

**40104**: Démontrer que u et v ***sont*** co-diagonalisables et trouver une base de R3 pour laquelle MatC ( u ) et MatC ( v ) sont diagonales

**40123**: Démontrer que u et v sont co-diagonalisables et trouver une base de R3 pour laquelle MatC ( u ) et MatC ( v ) ***sont*** diagonales

**40129**: 4.2.2 Soit E ***est*** C - espace vectoriel de dimension finie non nulle et u et v sont deux endomorphismes de E qui vérifient : Démontrer que u et v possèdent au moins un vecteur propre commun

**40143**: 4.2.2 Soit E est C - espace vectoriel de dimension finie non nulle et u et v ***sont*** deux endomorphismes de E qui vérifient : Démontrer que u et v possèdent au moins un vecteur propre commun

**40149**: 4.2.2 Soit E est C - espace vectoriel de dimension finie non nulle et u et v sont deux endomorphismes de E qui ***vérifient*** : Démontrer que u et v possèdent au moins un vecteur propre commun

**40156**: 4.2.2 Soit E est C - espace vectoriel de dimension finie non nulle et u et v sont deux endomorphismes de E qui vérifient : Démontrer que u et v ***possèdent*** au moins un vecteur propre commun

**40167**: 4.2.3 Soit E ***est*** C - espace vectoriel de dimension finie non nulle et u , v et w trois endomorphismes de E qui vérifient : Démontrer que u et v possèdent au moins rang(w ) valeurs propres communes ( en comptant les multiplicités )

**40188**: 4.2.3 Soit E est C - espace vectoriel de dimension finie non nulle et u , v et w trois endomorphismes de E qui ***vérifient*** : Démontrer que u et v possèdent au moins rang(w ) valeurs propres communes ( en comptant les multiplicités )

**40195**: 4.2.3 Soit E est C - espace vectoriel de dimension finie non nulle et u , v et w trois endomorphismes de E qui vérifient : Démontrer que u et v ***possèdent*** au moins rang(w ) valeurs propres communes ( en comptant les multiplicités )

**40246**: Démontrer que u et v ***sont*** co-trigonalisables

**40263**: Applications de la réduction Wxmaxima Python , Systèmes linéaires récurrents à coefficients constants On ***appelle*** système linéaire récurrent à coefficients constants tout ensemble d' équations récurrentes qui peut se mettre sous la forme : X0 s' appelle la condition initiale et Bn le second membre

**40276**: Applications de la réduction Wxmaxima Python , Systèmes linéaires récurrents à coefficients constants On appelle système linéaire récurrent à coefficients constants tout ensemble d' équations récurrentes qui ***peut*** se mettre sous la forme : X0 s' appelle la condition initiale et Bn le second membre

**40285**: Applications de la réduction Wxmaxima Python , Systèmes linéaires récurrents à coefficients constants On appelle système linéaire récurrent à coefficients constants tout ensemble d' équations récurrentes qui peut se mettre sous la forme : X0 s' ***appelle*** la condition initiale et Bn le second membre

**40299**: Le système : s' ***appelle*** le système homogène associé

**40307**: La résolution ***passe*** par : 1

**40337**: La recherche d' une solution particulière , soit évidente , soit par variation de la constante ***a*** : Nous sommes donc ramenés au calcul de An

**40340**: La recherche d' une solution particulière , soit évidente , soit par variation de la constante a : Nous ***sommes*** donc ramenés au calcul de An

**40348**: ***a.*** Cette méthode ne fonctionne que lorsque A est inversible

**40352**: a. Cette méthode ne ***fonctionne*** que lorsque A est inversible

**40356**: a. Cette méthode ne fonctionne que lorsque A ***est*** inversible

**40360**: On ***peut*** utiliser la formule du binôme de Newton , notamment lorsque A est somme d' une matrice d' homothétie .In et d' une matrice nilpotente N ( c' est - à - dire qu' il existe p N tel que N p 0n ) : On a ainsi : Lorsque la matrice A est diagonalisable , il suffit de la diagonaliser , car si A P Diag(1 ,

**40372**: On peut utiliser la formule du binôme de Newton , notamment lorsque A ***est*** somme d' une matrice d' homothétie .In et d' une matrice nilpotente N ( c' est - à - dire qu' il existe p N tel que N p 0n ) : On a ainsi : Lorsque la matrice A est diagonalisable , il suffit de la diagonaliser , car si A P Diag(1 ,

**40395**: On peut utiliser la formule du binôme de Newton , notamment lorsque A est somme d' une matrice d' homothétie .In et d' une matrice nilpotente N ( c' est - à - dire qu' il ***existe*** p N tel que N p 0n ) : On a ainsi : Lorsque la matrice A est diagonalisable , il suffit de la diagonaliser , car si A P Diag(1 ,

**40406**: On peut utiliser la formule du binôme de Newton , notamment lorsque A est somme d' une matrice d' homothétie .In et d' une matrice nilpotente N ( c' est - à - dire qu' il existe p N tel que N p 0n ) : On ***a*** ainsi : Lorsque la matrice A est diagonalisable , il suffit de la diagonaliser , car si A P Diag(1 ,

**40413**: On peut utiliser la formule du binôme de Newton , notamment lorsque A est somme d' une matrice d' homothétie .In et d' une matrice nilpotente N ( c' est - à - dire qu' il existe p N tel que N p 0n ) : On a ainsi : Lorsque la matrice A ***est*** diagonalisable , il suffit de la diagonaliser , car si A P Diag(1 ,

**40417**: On peut utiliser la formule du binôme de Newton , notamment lorsque A est somme d' une matrice d' homothétie .In et d' une matrice nilpotente N ( c' est - à - dire qu' il existe p N tel que N p 0n ) : On a ainsi : Lorsque la matrice A est diagonalisable , il ***suffit*** de la diagonaliser , car si A P Diag(1 ,

**40424**: On peut utiliser la formule du binôme de Newton , notamment lorsque A est somme d' une matrice d' homothétie .In et d' une matrice nilpotente N ( c' est - à - dire qu' il existe p N tel que N p 0n ) : On a ainsi : Lorsque la matrice A est diagonalisable , il suffit de la diagonaliser , car si ***A*** P Diag(1 ,

**40443**: , p ) P 1 , Par exemple si : alors A ***est*** diagonalisable et on trouve : Dans le cas où A est trigonalisable ( ce qu' on peut toujours supposer en se plaçant dans C ) , on se contente de trigonaliser la matrice et on résout le système triangulaire associé : ce qui est facile car il est triangulaire

**40447**: , p ) P 1 , Par exemple si : alors A est diagonalisable et on ***trouve*** : Dans le cas où A est trigonalisable ( ce qu' on peut toujours supposer en se plaçant dans C ) , on se contente de trigonaliser la matrice et on résout le système triangulaire associé : ce qui est facile car il est triangulaire

**40453**: , p ) P 1 , Par exemple si : alors A est diagonalisable et on trouve : Dans le cas où ***A*** est trigonalisable ( ce qu' on peut toujours supposer en se plaçant dans C ) , on se contente de trigonaliser la matrice et on résout le système triangulaire associé : ce qui est facile car il est triangulaire

**40454**: , p ) P 1 , Par exemple si : alors A est diagonalisable et on trouve : Dans le cas où A ***est*** trigonalisable ( ce qu' on peut toujours supposer en se plaçant dans C ) , on se contente de trigonaliser la matrice et on résout le système triangulaire associé : ce qui est facile car il est triangulaire

**40460**: , p ) P 1 , Par exemple si : alors A est diagonalisable et on trouve : Dans le cas où A est trigonalisable ( ce qu' on ***peut*** toujours supposer en se plaçant dans C ) , on se contente de trigonaliser la matrice et on résout le système triangulaire associé : ce qui est facile car il est triangulaire

**40472**: , p ) P 1 , Par exemple si : alors A est diagonalisable et on trouve : Dans le cas où A est trigonalisable ( ce qu' on peut toujours supposer en se plaçant dans C ) , on se ***contente*** de trigonaliser la matrice et on résout le système triangulaire associé : ce qui est facile car il est triangulaire

**40479**: , p ) P 1 , Par exemple si : alors A est diagonalisable et on trouve : Dans le cas où A est trigonalisable ( ce qu' on peut toujours supposer en se plaçant dans C ) , on se contente de trigonaliser la matrice et on ***résout*** le système triangulaire associé : ce qui est facile car il est triangulaire

**40487**: , p ) P 1 , Par exemple si : alors A est diagonalisable et on trouve : Dans le cas où A est trigonalisable ( ce qu' on peut toujours supposer en se plaçant dans C ) , on se contente de trigonaliser la matrice et on résout le système triangulaire associé : ce qui ***est*** facile car il est triangulaire

**40491**: , p ) P 1 , Par exemple si : alors A est diagonalisable et on trouve : Dans le cas où A est trigonalisable ( ce qu' on peut toujours supposer en se plaçant dans C ) , on se contente de trigonaliser la matrice et on résout le système triangulaire associé : ce qui est facile car il ***est*** triangulaire

**40496**: Il ne ***est*** alors pas utile de calculer la matrice inverse de la matrice de passage , car on a seulement Xn P Yn

**40513**: Il ne est alors pas utile de calculer la matrice inverse de la matrice de passage , car on ***a*** seulement Xn P Yn

**40547**: Lorsque l' on part d' une suite récurrente multiple ( les coefficients étant constants ) , on ***peut*** vectorialiser et la considérer comme un système récurrent

**40565**: La recherche des racines de l' équation caractéristique ***correspond*** au calcul du polynôme caractéristique de la matrice du système associé

**40595**: Par exemple , on ***peut*** transformer : 4.3.1 Résoudre le système récurrent : 4.3.2 Calculer les puissances de A , où ( a ) A est-elle diagonalisable ? ( b ) Calculer An , pour n N. 4.3.4 Calculer An , n N lorsque : ( a ) A est-elle diagonalisable ? ( b ) Démontrer que A est semblable à ( c ) Calculer An pour n Z. Systèmes linéaires différentiels à coefficients constants On appelle système linéaire différentiel à coefficients constants tout système d' équations différentielles qui peut se mettre sous la forme : avec I un intervalle de R et où X0 X(t0 ) est la condition initiale à l' instant t0 I. La résolution passe par : 1

**40615**: Par exemple , on peut transformer : 4.3.1 Résoudre le système récurrent : 4.3.2 Calculer les puissances de A , où ( a ) ***A*** est-elle diagonalisable ? ( b ) Calculer An , pour n N. 4.3.4 Calculer An , n N lorsque : ( a ) A est-elle diagonalisable ? ( b ) Démontrer que A est semblable à ( c ) Calculer An pour n Z. Systèmes linéaires différentiels à coefficients constants On appelle système linéaire différentiel à coefficients constants tout système d' équations différentielles qui peut se mettre sous la forme : avec I un intervalle de R et où X0 X(t0 ) est la condition initiale à l' instant t0 I. La résolution passe par : 1

**40639**: Par exemple , on peut transformer : 4.3.1 Résoudre le système récurrent : 4.3.2 Calculer les puissances de A , où ( a ) A est-elle diagonalisable ? ( b ) Calculer An , pour n N. 4.3.4 Calculer An , n N lorsque : ( a ) ***A*** est-elle diagonalisable ? ( b ) Démontrer que A est semblable à ( c ) Calculer An pour n Z. Systèmes linéaires différentiels à coefficients constants On appelle système linéaire différentiel à coefficients constants tout système d' équations différentielles qui peut se mettre sous la forme : avec I un intervalle de R et où X0 X(t0 ) est la condition initiale à l' instant t0 I. La résolution passe par : 1

**40649**: Par exemple , on peut transformer : 4.3.1 Résoudre le système récurrent : 4.3.2 Calculer les puissances de A , où ( a ) A est-elle diagonalisable ? ( b ) Calculer An , pour n N. 4.3.4 Calculer An , n N lorsque : ( a ) A est-elle diagonalisable ? ( b ) Démontrer que A ***est*** semblable à ( c ) Calculer An pour n Z. Systèmes linéaires différentiels à coefficients constants On appelle système linéaire différentiel à coefficients constants tout système d' équations différentielles qui peut se mettre sous la forme : avec I un intervalle de R et où X0 X(t0 ) est la condition initiale à l' instant t0 I. La résolution passe par : 1

**40667**: Par exemple , on peut transformer : 4.3.1 Résoudre le système récurrent : 4.3.2 Calculer les puissances de A , où ( a ) A est-elle diagonalisable ? ( b ) Calculer An , pour n N. 4.3.4 Calculer An , n N lorsque : ( a ) A est-elle diagonalisable ? ( b ) Démontrer que A est semblable à ( c ) Calculer An pour n Z. Systèmes linéaires différentiels à coefficients constants On ***appelle*** système linéaire différentiel à coefficients constants tout système d' équations différentielles qui peut se mettre sous la forme : avec I un intervalle de R et où X0 X(t0 ) est la condition initiale à l' instant t0 I. La résolution passe par : 1

**40680**: Par exemple , on peut transformer : 4.3.1 Résoudre le système récurrent : 4.3.2 Calculer les puissances de A , où ( a ) A est-elle diagonalisable ? ( b ) Calculer An , pour n N. 4.3.4 Calculer An , n N lorsque : ( a ) A est-elle diagonalisable ? ( b ) Démontrer que A est semblable à ( c ) Calculer An pour n Z. Systèmes linéaires différentiels à coefficients constants On appelle système linéaire différentiel à coefficients constants tout système d' équations différentielles qui ***peut*** se mettre sous la forme : avec I un intervalle de R et où X0 X(t0 ) est la condition initiale à l' instant t0 I. La résolution passe par : 1

**40698**: Par exemple , on peut transformer : 4.3.1 Résoudre le système récurrent : 4.3.2 Calculer les puissances de A , où ( a ) A est-elle diagonalisable ? ( b ) Calculer An , pour n N. 4.3.4 Calculer An , n N lorsque : ( a ) A est-elle diagonalisable ? ( b ) Démontrer que A est semblable à ( c ) Calculer An pour n Z. Systèmes linéaires différentiels à coefficients constants On appelle système linéaire différentiel à coefficients constants tout système d' équations différentielles qui peut se mettre sous la forme : avec I un intervalle de R et où X0 X(t0 ) ***est*** la condition initiale à l' instant t0 I. La résolution passe par : 1

**40709**: Par exemple , on peut transformer : 4.3.1 Résoudre le système récurrent : 4.3.2 Calculer les puissances de A , où ( a ) A est-elle diagonalisable ? ( b ) Calculer An , pour n N. 4.3.4 Calculer An , n N lorsque : ( a ) A est-elle diagonalisable ? ( b ) Démontrer que A est semblable à ( c ) Calculer An pour n Z. Systèmes linéaires différentiels à coefficients constants On appelle système linéaire différentiel à coefficients constants tout système d' équations différentielles qui peut se mettre sous la forme : avec I un intervalle de R et où X0 X(t0 ) est la condition initiale à l' instant t0 I. La résolution ***passe*** par : 1

**40763**: Lorsque l' on part d' une équation différentielle linéaire d' ordre p , à coefficients constants de la forme : on ***peut*** la vectorialiser , pour se ramener à un système en posant Par exemple , on peut transformer : Le cas où la matrice A est diagonalisable est facile car si A P D P 1 avec D Diag(1 ,

**40779**: Lorsque l' on part d' une équation différentielle linéaire d' ordre p , à coefficients constants de la forme : on peut la vectorialiser , pour se ramener à un système en posant Par exemple , on ***peut*** transformer : Le cas où la matrice A est diagonalisable est facile car si A P D P 1 avec D Diag(1 ,

**40788**: Lorsque l' on part d' une équation différentielle linéaire d' ordre p , à coefficients constants de la forme : on peut la vectorialiser , pour se ramener à un système en posant Par exemple , on peut transformer : Le cas où la matrice A ***est*** diagonalisable est facile car si A P D P 1 avec D Diag(1 ,

**40790**: Lorsque l' on part d' une équation différentielle linéaire d' ordre p , à coefficients constants de la forme : on peut la vectorialiser , pour se ramener à un système en posant Par exemple , on peut transformer : Le cas où la matrice A est diagonalisable ***est*** facile car si A P D P 1 avec D Diag(1 ,

**40794**: Lorsque l' on part d' une équation différentielle linéaire d' ordre p , à coefficients constants de la forme : on peut la vectorialiser , pour se ramener à un système en posant Par exemple , on peut transformer : Le cas où la matrice A est diagonalisable est facile car si ***A*** P D P 1 avec D Diag(1 ,

**40823**: , p ) , alors En posant Y ( t ) P 1 X(t ) , on ***est*** ramené au système différentiel : qui est un système diagonal

**40830**: , p ) , alors En posant Y ( t ) P 1 X(t ) , on est ramené au système différentiel : qui ***est*** un système diagonal

**40844**: En notant yi les composantes de Y , on ***a*** donc : On calcule alors X(t ) P Y ( t )

**40848**: En notant yi les composantes de Y , on a donc : On ***calcule*** alors X(t ) P Y ( t )

**40862**: En particulier , on ***a*** jamais besoin de calculer l' inverse de P

**40899**: Par exemple , en reprenant le système de la remarque précédente ( obtenu à partir de 00 ( t ) ( t ) 0 ) , on ***a*** Il ne est pas difficile de démontrer que A est diagonalisable sur C , ses valeurs propres sont i et En posant Y P 1 X , on a le système donc il existe ( c1 , c2 ) C2 tel que : Si on ajoute la condition initiale ( qui correspond à ( 0 ) 1 et 0 ( 0 ) 0 ) , on trouve c1 c2 21 donc en particulier : exp(i t ) exp(i t ) On retrouve donc la solution bien connue de 00 ( t ) ( t ) 0 , ( 0 ) 1 et 0 ( 0 ) 0

**40902**: Par exemple , en reprenant le système de la remarque précédente ( obtenu à partir de 00 ( t ) ( t ) 0 ) , on a Il ne ***est*** pas difficile de démontrer que A est diagonalisable sur C , ses valeurs propres sont i et En posant Y P 1 X , on a le système donc il existe ( c1 , c2 ) C2 tel que : Si on ajoute la condition initiale ( qui correspond à ( 0 ) 1 et 0 ( 0 ) 0 ) , on trouve c1 c2 21 donc en particulier : exp(i t ) exp(i t ) On retrouve donc la solution bien connue de 00 ( t ) ( t ) 0 , ( 0 ) 1 et 0 ( 0 ) 0

**40909**: Par exemple , en reprenant le système de la remarque précédente ( obtenu à partir de 00 ( t ) ( t ) 0 ) , on a Il ne est pas difficile de démontrer que A ***est*** diagonalisable sur C , ses valeurs propres sont i et En posant Y P 1 X , on a le système donc il existe ( c1 , c2 ) C2 tel que : Si on ajoute la condition initiale ( qui correspond à ( 0 ) 1 et 0 ( 0 ) 0 ) , on trouve c1 c2 21 donc en particulier : exp(i t ) exp(i t ) On retrouve donc la solution bien connue de 00 ( t ) ( t ) 0 , ( 0 ) 1 et 0 ( 0 ) 0

**40917**: Par exemple , en reprenant le système de la remarque précédente ( obtenu à partir de 00 ( t ) ( t ) 0 ) , on a Il ne est pas difficile de démontrer que A est diagonalisable sur C , ses valeurs propres ***sont*** i et En posant Y P 1 X , on a le système donc il existe ( c1 , c2 ) C2 tel que : Si on ajoute la condition initiale ( qui correspond à ( 0 ) 1 et 0 ( 0 ) 0 ) , on trouve c1 c2 21 donc en particulier : exp(i t ) exp(i t ) On retrouve donc la solution bien connue de 00 ( t ) ( t ) 0 , ( 0 ) 1 et 0 ( 0 ) 0

**40928**: Par exemple , en reprenant le système de la remarque précédente ( obtenu à partir de 00 ( t ) ( t ) 0 ) , on a Il ne est pas difficile de démontrer que A est diagonalisable sur C , ses valeurs propres sont i et En posant Y P 1 X , on ***a*** le système donc il existe ( c1 , c2 ) C2 tel que : Si on ajoute la condition initiale ( qui correspond à ( 0 ) 1 et 0 ( 0 ) 0 ) , on trouve c1 c2 21 donc en particulier : exp(i t ) exp(i t ) On retrouve donc la solution bien connue de 00 ( t ) ( t ) 0 , ( 0 ) 1 et 0 ( 0 ) 0

**40933**: Par exemple , en reprenant le système de la remarque précédente ( obtenu à partir de 00 ( t ) ( t ) 0 ) , on a Il ne est pas difficile de démontrer que A est diagonalisable sur C , ses valeurs propres sont i et En posant Y P 1 X , on a le système donc il ***existe*** ( c1 , c2 ) C2 tel que : Si on ajoute la condition initiale ( qui correspond à ( 0 ) 1 et 0 ( 0 ) 0 ) , on trouve c1 c2 21 donc en particulier : exp(i t ) exp(i t ) On retrouve donc la solution bien connue de 00 ( t ) ( t ) 0 , ( 0 ) 1 et 0 ( 0 ) 0

**40945**: Par exemple , en reprenant le système de la remarque précédente ( obtenu à partir de 00 ( t ) ( t ) 0 ) , on a Il ne est pas difficile de démontrer que A est diagonalisable sur C , ses valeurs propres sont i et En posant Y P 1 X , on a le système donc il existe ( c1 , c2 ) C2 tel que : Si on ***ajoute*** la condition initiale ( qui correspond à ( 0 ) 1 et 0 ( 0 ) 0 ) , on trouve c1 c2 21 donc en particulier : exp(i t ) exp(i t ) On retrouve donc la solution bien connue de 00 ( t ) ( t ) 0 , ( 0 ) 1 et 0 ( 0 ) 0

**40951**: Par exemple , en reprenant le système de la remarque précédente ( obtenu à partir de 00 ( t ) ( t ) 0 ) , on a Il ne est pas difficile de démontrer que A est diagonalisable sur C , ses valeurs propres sont i et En posant Y P 1 X , on a le système donc il existe ( c1 , c2 ) C2 tel que : Si on ajoute la condition initiale ( qui ***correspond*** à ( 0 ) 1 et 0 ( 0 ) 0 ) , on trouve c1 c2 21 donc en particulier : exp(i t ) exp(i t ) On retrouve donc la solution bien connue de 00 ( t ) ( t ) 0 , ( 0 ) 1 et 0 ( 0 ) 0

**40966**: Par exemple , en reprenant le système de la remarque précédente ( obtenu à partir de 00 ( t ) ( t ) 0 ) , on a Il ne est pas difficile de démontrer que A est diagonalisable sur C , ses valeurs propres sont i et En posant Y P 1 X , on a le système donc il existe ( c1 , c2 ) C2 tel que : Si on ajoute la condition initiale ( qui correspond à ( 0 ) 1 et 0 ( 0 ) 0 ) , on ***trouve*** c1 c2 21 donc en particulier : exp(i t ) exp(i t ) On retrouve donc la solution bien connue de 00 ( t ) ( t ) 0 , ( 0 ) 1 et 0 ( 0 ) 0

**40981**: Par exemple , en reprenant le système de la remarque précédente ( obtenu à partir de 00 ( t ) ( t ) 0 ) , on a Il ne est pas difficile de démontrer que A est diagonalisable sur C , ses valeurs propres sont i et En posant Y P 1 X , on a le système donc il existe ( c1 , c2 ) C2 tel que : Si on ajoute la condition initiale ( qui correspond à ( 0 ) 1 et 0 ( 0 ) 0 ) , on trouve c1 c2 21 donc en particulier : exp(i t ) exp(i t ) On ***retrouve*** donc la solution bien connue de 00 ( t ) ( t ) 0 , ( 0 ) 1 et 0 ( 0 ) 0

**41019**: Dans le cas général , on trigonalise A ( ce qui ***est*** toujours possible en se plaçant dans C ) par A P T P 1 où T est une matrice triangulaire supérieure

**41036**: Dans le cas général , on trigonalise A ( ce qui est toujours possible en se plaçant dans C ) par A P T P 1 où T ***est*** une matrice triangulaire supérieure

**41043**: On ***est*** donc ramené ( par la même démarche qu' à l' exemple précédent en posant Y ( t ) P 1 X(t ) ) à un système de la forme où T est une matrice triangulaire supérieure que l' on peut résoudre facilement et on revient à X grâce à la relation X(t ) P Y ( t )

**41075**: On est donc ramené ( par la même démarche qu' à l' exemple précédent en posant Y ( t ) P 1 X(t ) ) à un système de la forme où T ***est*** une matrice triangulaire supérieure que l' on peut résoudre facilement et on revient à X grâce à la relation X(t ) P Y ( t )

**41083**: On est donc ramené ( par la même démarche qu' à l' exemple précédent en posant Y ( t ) P 1 X(t ) ) à un système de la forme où T est une matrice triangulaire supérieure que l' on ***peut*** résoudre facilement et on revient à X grâce à la relation X(t ) P Y ( t )

**41088**: On est donc ramené ( par la même démarche qu' à l' exemple précédent en posant Y ( t ) P 1 X(t ) ) à un système de la forme où T est une matrice triangulaire supérieure que l' on peut résoudre facilement et on ***revient*** à X grâce à la relation X(t ) P Y ( t )

**41132**: Démontrer que les solutions de X 0 A X ***sont*** des courbes planes

**41145**: 4.4.5 Résoudre le système différentiel 4.4.6 Déterminer les ***a*** R tels que le système admette au moins une solution non nulle bornée au voisinage de

**41165**: 4.4.7 Quelle ***est*** la nature des courbes intégrales de X 0 A X avec : Espaces stables La recherche des espaces stables par un endomorphisme u L ( E ) d' un K - espace vectoriel de dimension finie en dimension finie repose sur quelques résultats : 1

**41205**: 4.4.7 Quelle est la nature des courbes intégrales de X 0 A X avec : Espaces stables La recherche des espaces stables par un endomorphisme u L ( E ) d' un K - espace vectoriel de dimension finie en dimension finie ***repose*** sur quelques résultats : 1

**41214**: Si u ***est*** diagonalisable et E1 est un sous-espace vectoriel stable par u , alors : ( voir la propriété 4.6 , page 203 )

**41218**: Si u est diagonalisable et E1 ***est*** un sous-espace vectoriel stable par u , alors : ( voir la propriété 4.6 , page 203 )

**41242**: En particulier , E1 ***a*** une base constituée de vecteurs propres de u. 2

**41255**: Si u ***est*** trigonalisable , alors il admet des espaces stables de toute dimension ( donné par le drapeau ) et de plus , si E1 est un sous-espace vectoriel stable par u , alors : 3

**41260**: Si u est trigonalisable , alors il ***admet*** des espaces stables de toute dimension ( donné par le drapeau ) et de plus , si E1 est un sous-espace vectoriel stable par u , alors : 3

**41279**: Si u est trigonalisable , alors il admet des espaces stables de toute dimension ( donné par le drapeau ) et de plus , si E1 ***est*** un sous-espace vectoriel stable par u , alors : 3

**41293**: Si il ***existe*** un hyperplan H de E stable par u , alors u admet un vecteur propre

**41305**: Si il existe un hyperplan H de E stable par u , alors u ***admet*** un vecteur propre

**41354**: , en ) de E. Par projection , il ***existe*** un unique K tel que u(en ) .en H. Alors pour tout x E , u(x ) .x H ( vrai sur Vect(en ) et aussi sur H car il est stable par u ) donc Im(u

**41385**: , en ) de E. Par projection , il existe un unique K tel que u(en ) .en H. Alors pour tout x E , u(x ) .x H ( vrai sur Vect(en ) et aussi sur H car il ***est*** stable par u ) donc Im(u

**41399**: ide ) H. I Ces résultats ***suffisent*** en général lorsque l' on est en dimension 3 , car il ne y a que les droites dirigées par un vecteur propre et les hyperplans qui peuvent être stables ( avec en plus , bien évidemment , 0E Soit la matrice a : Le polynôme caractéristique est bien X 3

**41405**: ide ) H. I Ces résultats suffisent en général lorsque l' on ***est*** en dimension 3 , car il ne y a que les droites dirigées par un vecteur propre et les hyperplans qui peuvent être stables ( avec en plus , bien évidemment , 0E Soit la matrice a : Le polynôme caractéristique est bien X 3

**41414**: ide ) H. I Ces résultats suffisent en général lorsque l' on est en dimension 3 , car il ne y ***a*** que les droites dirigées par un vecteur propre et les hyperplans qui peuvent être stables ( avec en plus , bien évidemment , 0E Soit la matrice a : Le polynôme caractéristique est bien X 3

**41427**: ide ) H. I Ces résultats suffisent en général lorsque l' on est en dimension 3 , car il ne y a que les droites dirigées par un vecteur propre et les hyperplans qui ***peuvent*** être stables ( avec en plus , bien évidemment , 0E Soit la matrice a : Le polynôme caractéristique est bien X 3

**41442**: ide ) H. I Ces résultats suffisent en général lorsque l' on est en dimension 3 , car il ne y a que les droites dirigées par un vecteur propre et les hyperplans qui peuvent être stables ( avec en plus , bien évidemment , 0E Soit la matrice ***a*** : Le polynôme caractéristique est bien X 3

**41447**: ide ) H. I Ces résultats suffisent en général lorsque l' on est en dimension 3 , car il ne y a que les droites dirigées par un vecteur propre et les hyperplans qui peuvent être stables ( avec en plus , bien évidemment , 0E Soit la matrice a : Le polynôme caractéristique ***est*** bien X 3

**41455**: Les espaces stables ***sont*** : ( dimension 0 ) ( dimension 1 ) ( dimension 2 ) ( dimension 3 ) a. On remarquera que la matrice est symétrique et non diagonalisable , seules les matrices symétriques réelles sont automatiquement diagonalisables

**41473**: Les espaces stables sont : ( dimension 0 ) ( dimension 1 ) ( dimension 2 ) ( dimension 3 ) ***a.*** On remarquera que la matrice est symétrique et non diagonalisable , seules les matrices symétriques réelles sont automatiquement diagonalisables

**41479**: Les espaces stables sont : ( dimension 0 ) ( dimension 1 ) ( dimension 2 ) ( dimension 3 ) a. On remarquera que la matrice ***est*** symétrique et non diagonalisable , seules les matrices symétriques réelles sont automatiquement diagonalisables

**41490**: Les espaces stables sont : ( dimension 0 ) ( dimension 1 ) ( dimension 2 ) ( dimension 3 ) a. On remarquera que la matrice est symétrique et non diagonalisable , seules les matrices symétriques réelles ***sont*** automatiquement diagonalisables

**41509**: Démontrer que u ***laisse*** un plan stable

**41535**: 4.5.2 Soit E Rn , Sn , on note T l' application de E dans E définie par Démontrer que T ***est*** dans GL ( E )

**41608**: Démontrer que les sous espaces stables par f ***sont*** les Ker(f k ) pour k 0 , n. 4.5.5 Soit E un C - espace vectoriel de dimension finie n 1 et u L(E )

**41637**: ( ***a*** ) Démontrer que si u ne admet qu' un nombre fini de sous-espaces vectoriels stables alors chaque sous-espace propre est de dimension 1

**41644**: ( a ) Démontrer que si u ne ***admet*** qu' un nombre fini de sous-espaces vectoriels stables alors chaque sous-espace propre est de dimension 1

**41657**: ( a ) Démontrer que si u ne admet qu' un nombre fini de sous-espaces vectoriels stables alors chaque sous-espace propre ***est*** de dimension 1

**41666**: ( b ) Quels ***sont*** les sous-espaces vectoriels stables par u si un 0L ( E ) ? ( c ) Démontrer la réciproque de la question ( a )

**41690**: ( b ) Quels sont les sous-espaces vectoriels stables par u si un 0L ( E ) ? ( c ) Démontrer la réciproque de la question ( ***a*** )

**41716**: Démontrer que u ***est*** diagonalisable si , et seulement si , le polynôme caractéristique u de u est scindé et que tout sous-espace vectoriel de E stable par u admet un supplémentaire stable

**41730**: Démontrer que u est diagonalisable si , et seulement si , le polynôme caractéristique u de u ***est*** scindé et que tout sous-espace vectoriel de E stable par u admet un supplémentaire stable

**41742**: Démontrer que u est diagonalisable si , et seulement si , le polynôme caractéristique u de u est scindé et que tout sous-espace vectoriel de E stable par u ***admet*** un supplémentaire stable

**41749**: 4.5.7 On ***considère*** dans R3 l' endomorphisme f dont la matrice dans la base canonique est Déterminer les sous-espaces vectoriels de R3 qui sont stables par f

**41762**: 4.5.7 On considère dans R3 l' endomorphisme f dont la matrice dans la base canonique ***est*** Déterminer les sous-espaces vectoriels de R3 qui sont stables par f

**41770**: 4.5.7 On considère dans R3 l' endomorphisme f dont la matrice dans la base canonique est Déterminer les sous-espaces vectoriels de R3 qui ***sont*** stables par f

**41813**: Chapitre 5 Compléments sur la réduction des endomorphismes Polynômes d' endomorphisme Polynômes d' endomorphisme et polynômes annulateurs Soit E un K - espace vectoriel et soit ak X k KX Si u L ( E ) , on ***définit*** alors l' endomorphisme P ( u ) L ( E ) par a : On dit que P ( u ) est un polynôme d' endomorphisme en u. On note : Ku P ( u ) , P KX l' ensemble des polynômes en u On définit de même la notion de polynôme de matrice : Attention , il faut que la matrice soit carrée

**41826**: Chapitre 5 Compléments sur la réduction des endomorphismes Polynômes d' endomorphisme Polynômes d' endomorphisme et polynômes annulateurs Soit E un K - espace vectoriel et soit ak X k KX Si u L ( E ) , on définit alors l' endomorphisme P ( u ) L ( E ) par ***a*** : On dit que P ( u ) est un polynôme d' endomorphisme en u. On note : Ku P ( u ) , P KX l' ensemble des polynômes en u On définit de même la notion de polynôme de matrice : Attention , il faut que la matrice soit carrée

**41829**: Chapitre 5 Compléments sur la réduction des endomorphismes Polynômes d' endomorphisme Polynômes d' endomorphisme et polynômes annulateurs Soit E un K - espace vectoriel et soit ak X k KX Si u L ( E ) , on définit alors l' endomorphisme P ( u ) L ( E ) par a : On ***dit*** que P ( u ) est un polynôme d' endomorphisme en u. On note : Ku P ( u ) , P KX l' ensemble des polynômes en u On définit de même la notion de polynôme de matrice : Attention , il faut que la matrice soit carrée

**41835**: Chapitre 5 Compléments sur la réduction des endomorphismes Polynômes d' endomorphisme Polynômes d' endomorphisme et polynômes annulateurs Soit E un K - espace vectoriel et soit ak X k KX Si u L ( E ) , on définit alors l' endomorphisme P ( u ) L ( E ) par a : On dit que P ( u ) ***est*** un polynôme d' endomorphisme en u. On note : Ku P ( u ) , P KX l' ensemble des polynômes en u On définit de même la notion de polynôme de matrice : Attention , il faut que la matrice soit carrée

**41843**: Chapitre 5 Compléments sur la réduction des endomorphismes Polynômes d' endomorphisme Polynômes d' endomorphisme et polynômes annulateurs Soit E un K - espace vectoriel et soit ak X k KX Si u L ( E ) , on définit alors l' endomorphisme P ( u ) L ( E ) par a : On dit que P ( u ) est un polynôme d' endomorphisme en u. On ***note*** : Ku P ( u ) , P KX l' ensemble des polynômes en u On définit de même la notion de polynôme de matrice : Attention , il faut que la matrice soit carrée

**41860**: Chapitre 5 Compléments sur la réduction des endomorphismes Polynômes d' endomorphisme Polynômes d' endomorphisme et polynômes annulateurs Soit E un K - espace vectoriel et soit ak X k KX Si u L ( E ) , on définit alors l' endomorphisme P ( u ) L ( E ) par a : On dit que P ( u ) est un polynôme d' endomorphisme en u. On note : Ku P ( u ) , P KX l' ensemble des polynômes en u On ***définit*** de même la notion de polynôme de matrice : Attention , il faut que la matrice soit carrée

**41873**: Chapitre 5 Compléments sur la réduction des endomorphismes Polynômes d' endomorphisme Polynômes d' endomorphisme et polynômes annulateurs Soit E un K - espace vectoriel et soit ak X k KX Si u L ( E ) , on définit alors l' endomorphisme P ( u ) L ( E ) par a : On dit que P ( u ) est un polynôme d' endomorphisme en u. On note : Ku P ( u ) , P KX l' ensemble des polynômes en u On définit de même la notion de polynôme de matrice : Attention , il ***faut*** que la matrice soit carrée

**41882**: Si E ***est*** une base de E ( en dimension finie ) , alors : MatE P ( u ) P MatE ( u ) En particulier , Ku est un sous-espace vectoriel de L ( E ) et ( 0KX ) ( u ) 0L ( E )

**41909**: Si E est une base de E ( en dimension finie ) , alors : MatE P ( u ) P MatE ( u ) En particulier , Ku ***est*** un sous-espace vectoriel de L ( E ) et ( 0KX ) ( u ) 0L ( E )

**41940**: De plus , P ( u ) et Q(u ) ***commutent*** car : Soit E un K - espace vectoriel , soit P KX et soit u L ( E )

**41963**: On ***dit*** que P est un polynôme annulateur Soit E un K - espace vectoriel de dimension finie , soit u L ( E ) et soit P KX un polynôme annulateur de u. Alors : Autrement dit , les valeurs propres de sont racines de tout polynôme annulateur de P

**41966**: On dit que P ***est*** un polynôme annulateur Soit E un K - espace vectoriel de dimension finie , soit u L ( E ) et soit P KX un polynôme annulateur de u. Alors : Autrement dit , les valeurs propres de sont racines de tout polynôme annulateur de P

**41999**: On dit que P est un polynôme annulateur Soit E un K - espace vectoriel de dimension finie , soit u L ( E ) et soit P KX un polynôme annulateur de u. Alors : Autrement ***dit*** , les valeurs propres de sont racines de tout polynôme annulateur de P

**42005**: On dit que P est un polynôme annulateur Soit E un K - espace vectoriel de dimension finie , soit u L ( E ) et soit P KX un polynôme annulateur de u. Alors : Autrement dit , les valeurs propres de ***sont*** racines de tout polynôme annulateur de P

**42006**: On dit que P est un polynôme annulateur Soit E un K - espace vectoriel de dimension finie , soit u L ( E ) et soit P KX un polynôme annulateur de u. Alors : Autrement dit , les valeurs propres de sont ***racines*** de tout polynôme annulateur de P

**42035**: Alors : ( ce calcul ***est*** valable même quand P ne est pas annulateur de u )

**42041**: Alors : ( ce calcul est valable même quand P ne ***est*** pas annulateur de u )

**42050**: Puisque P ***est*** annulateur de u , on a P ( u)(x ) 0E P ( ) .x

**42056**: Puisque P est annulateur de u , on ***a*** P ( u)(x ) 0E P ( ) .x

**42073**: Mais x 6 0E ( c' ***est*** un vecteur propre ) donc P ( ) 0

**42088**: Attention , la réciproque ***est*** fausse : X ( X 1 ) est un polynôme annulateur de idE mais 0 1

**42096**: Attention , la réciproque est fausse : X ( X 1 ) ***est*** un polynôme annulateur de idE mais 0 1

**42118**: Ainsi , connaître les racines d' un polynôme annulateur de u ne ***donne*** que des candidats potentiels pour les valeurs propres de u. Le lemme des noyaux Théorème 5.1 dit lemme des noyaux Soit E un K - espace vectoriel et u L ( E )

**42135**: Ainsi , connaître les racines d' un polynôme annulateur de u ne donne que des candidats potentiels pour les valeurs propres de u. Le lemme des noyaux Théorème 5.1 ***dit*** lemme des noyaux Soit E un K - espace vectoriel et u L ( E )

**42160**: Si ( P , Q ) KX2 ***sont*** premiers deux à deux , alors : premiers deux à deux Démonstration Puisque P et Q sont premiers deux à deux , il existe ( U , V ) KX2 tel que ( identité de Bézout ) : Soit x Ker P ( u ) Ker Q(u )

**42177**: Si ( P , Q ) KX2 sont premiers deux à deux , alors : premiers deux à deux Démonstration Puisque P et Q ***sont*** premiers deux à deux , il existe ( U , V ) KX2 tel que ( identité de Bézout ) : Soit x Ker P ( u ) Ker Q(u )

**42184**: Si ( P , Q ) KX2 sont premiers deux à deux , alors : premiers deux à deux Démonstration Puisque P et Q sont premiers deux à deux , il ***existe*** ( U , V ) KX2 tel que ( identité de Bézout ) : Soit x Ker P ( u ) Ker Q(u )

**42223**: On en ***déduit*** Soit x Ker ( P Q)(u )

**42243**: D' après l' identité de Bézout : Les polynômes en u ***commutent*** donc : donc x1 Ker Q(u )

**42264**: De même , x2 Ker P ( u ) donc On en ***déduit*** le résultat

**42330**: Les propositions suivantes ***sont*** équivalentes : 1

**42336**: u ***est*** diagonalisable 2

**42341**: il ***existe*** un polynôme annulateur de u scindé à racines simples a

**42351**: il existe un polynôme annulateur de u scindé à racines simples ***a***

**42353**: ***a.*** C' est - à - dire P KX non nul de la forme P a ( X 1 ) ( X k ) où a K et les j K sont tous différents

**42368**: a. C' est - à - dire P KX non nul de la forme P ***a*** ( X 1 ) ( X k ) où a K et les j K sont tous différents

**42378**: a. C' est - à - dire P KX non nul de la forme P a ( X 1 ) ( X k ) où ***a*** K et les j K sont tous différents

**42384**: a. C' est - à - dire P KX non nul de la forme P a ( X 1 ) ( X k ) où a K et les j K ***sont*** tous différents

**42389**: Démonstration ***Supposons*** u soit diagonalisable et notons Sp(u ) 1 ,

**42394**: Démonstration Supposons u soit diagonalisable et ***notons*** Sp(u ) 1 ,

**42409**: Soit Le polynôme P ***est*** scindé à racines simples

**42424**: Les polynômes X i , i 1 , k ***sont*** premiers deux à deux donc d' après le lemme des noyaux : car u est diagonalisable

**42439**: Les polynômes X i , i 1 , k sont premiers deux à deux donc d' après le lemme des noyaux : car u ***est*** diagonalisable

**42443**: On ***a*** donc P ( u ) 0L ( E ) , c' est - à - dire que P est un polynôme annulateur de u scindé à racines simples

**42462**: On a donc P ( u ) 0L ( E ) , c' est - à - dire que P ***est*** un polynôme annulateur de u scindé à racines simples

**42473**: ***Supposons*** qu' il existe un polynôme P annulateur de u scindé à racines simples

**42476**: Supposons qu' il ***existe*** un polynôme P annulateur de u scindé à racines simples

**42494**: Écrivons - le sous la avec ***a*** K et les i KX tous différents

**42512**: Les polynômes X i , i 1 , k ***sont*** premiers deux à deux donc d' après le lemme des noyaux : Alors I 6 ( car E 6 0E )

**42541**: Pour tout i I , i ***est*** une valeur propre de u donc : donc u est diagonalisable

**42551**: Pour tout i I , i est une valeur propre de u donc : donc u ***est*** diagonalisable

**42558**: Si F 6 0E ***est*** un sous-espace vectoriel de E stable par u et si u est diagonalisable , alors u F est diagonalisable , car tout polynôme scindé à racines simples qui est annulateur de u est aussi annulateur de u F

**42570**: Si F 6 0E est un sous-espace vectoriel de E stable par u et si u ***est*** diagonalisable , alors u F est diagonalisable , car tout polynôme scindé à racines simples qui est annulateur de u est aussi annulateur de u F

**42576**: Si F 6 0E est un sous-espace vectoriel de E stable par u et si u est diagonalisable , alors u F ***est*** diagonalisable , car tout polynôme scindé à racines simples qui est annulateur de u est aussi annulateur de u F

**42587**: Si F 6 0E est un sous-espace vectoriel de E stable par u et si u est diagonalisable , alors u F est diagonalisable , car tout polynôme scindé à racines simples qui ***est*** annulateur de u est aussi annulateur de u F

**42591**: Si F 6 0E est un sous-espace vectoriel de E stable par u et si u est diagonalisable , alors u F est diagonalisable , car tout polynôme scindé à racines simples qui est annulateur de u ***est*** aussi annulateur de u F

**42599**: On ***retrouve*** ainsi le résultat de la propriété 4.6 , page 203 du chapitre 4

**42627**: Toute matrice A de Mn ( C ) telle qu' il ***existe*** p N tel que Ap In est diagonalisable

**42634**: Toute matrice A de Mn ( C ) telle qu' il existe p N tel que Ap In ***est*** diagonalisable

**42644**: En particulier , les matrices de symétries ***sont*** les seules qui vérifient A2 In

**42648**: En particulier , les matrices de symétries sont les seules qui ***vérifient*** A2 In

**42667**: Si une matrice M M2 n ( K ) de la forme : ***est*** diagonalisable , alors A et C sont diagonalisables ( la réciproque est fausse )

**42674**: Si une matrice M M2 n ( K ) de la forme : est diagonalisable , alors A et C ***sont*** diagonalisables ( la réciproque est fausse )

**42679**: Si une matrice M M2 n ( K ) de la forme : est diagonalisable , alors A et C sont diagonalisables ( la réciproque ***est*** fausse )

**42691**: La polynôme X p 1 ***est*** annulateur de A et est scindé , à racines simples dans Mn ( C )

**42696**: La polynôme X p 1 est annulateur de A et ***est*** scindé , à racines simples dans Mn ( C )

**42718**: De plus , toute symétrie ( cas p 2 ) ***vérifient*** par définition A2 In

**42727**: Réciproquement , si ***A*** vérifie A2 In , alors le polynôme X 2 1 est annulateur de A , scindé , à racines simples ( dans R ou C ) , donc A est semblable à une matrice diagonale avec des 1 sur la diagonale

**42728**: Réciproquement , si A ***vérifie*** A2 In , alors le polynôme X 2 1 est annulateur de A , scindé , à racines simples ( dans R ou C ) , donc A est semblable à une matrice diagonale avec des 1 sur la diagonale

**42738**: Réciproquement , si A vérifie A2 In , alors le polynôme X 2 1 ***est*** annulateur de A , scindé , à racines simples ( dans R ou C ) , donc A est semblable à une matrice diagonale avec des 1 sur la diagonale

**42757**: Réciproquement , si A vérifie A2 In , alors le polynôme X 2 1 est annulateur de A , scindé , à racines simples ( dans R ou C ) , donc A ***est*** semblable à une matrice diagonale avec des 1 sur la diagonale

**42771**: C' ***est*** une symétrie par rapport à EA ( 1 ) parallèlement 2

**42786**: Si M ***est*** diagonalisable , il existe un polynôme P non nul , scindé , à racines simples qui annule M

**42790**: Si M est diagonalisable , il ***existe*** un polynôme P non nul , scindé , à racines simples qui annule M

**42803**: Si M est diagonalisable , il existe un polynôme P non nul , scindé , à racines simples qui ***annule*** M

**42818**: Or donc , P ( A ) et P ( C ) ***sont*** des matrices nulles , donc A et C sont diagonalisables

**42827**: Or donc , P ( A ) et P ( C ) sont des matrices nulles , donc A et C ***sont*** diagonalisables

**42833**: Un contre-exemple simple ***est*** A C 0 et B 1

**42867**: L' idéal annulateur de u ***est*** défini par : Soit E un K - espace vectoriel et soit u L ( E )

**42891**: Iu ***est*** un idéal de KX 2

**42900**: si E ***est*** de dimension finie non nulle , Iu 6 0KX

**42915**: On ***vérifie*** immédiatement que Iu est un sous-groupe de KX et qu' il est stable par multiplication par un élément quelconque de KX

**42919**: On vérifie immédiatement que Iu ***est*** un sous-groupe de KX et qu' il est stable par multiplication par un élément quelconque de KX

**42927**: On vérifie immédiatement que Iu est un sous-groupe de KX et qu' il ***est*** stable par multiplication par un élément quelconque de KX

**42939**: C' ***est*** donc un idéal de KX

**42978**: Le polynôme minimal de u ***est*** l' unique polynôme unitaire de degré 1 u KX tel que Autrement dit , si P KX est annulateur de u , alors il existe Q KX tel que P u Q. L' existence et l' unicité du polynôme minimal sont assurées par le fait que KX est principal et que Iu est un idéal de KX non réduit à 0KX ( voir le cours sur les anneaux )

**42991**: Le polynôme minimal de u est l' unique polynôme unitaire de degré 1 u KX tel que Autrement ***dit*** , si P KX est annulateur de u , alors il existe Q KX tel que P u Q. L' existence et l' unicité du polynôme minimal sont assurées par le fait que KX est principal et que Iu est un idéal de KX non réduit à 0KX ( voir le cours sur les anneaux )

**42996**: Le polynôme minimal de u est l' unique polynôme unitaire de degré 1 u KX tel que Autrement dit , si P KX ***est*** annulateur de u , alors il existe Q KX tel que P u Q. L' existence et l' unicité du polynôme minimal sont assurées par le fait que KX est principal et que Iu est un idéal de KX non réduit à 0KX ( voir le cours sur les anneaux )

**43003**: Le polynôme minimal de u est l' unique polynôme unitaire de degré 1 u KX tel que Autrement dit , si P KX est annulateur de u , alors il ***existe*** Q KX tel que P u Q. L' existence et l' unicité du polynôme minimal sont assurées par le fait que KX est principal et que Iu est un idéal de KX non réduit à 0KX ( voir le cours sur les anneaux )

**43019**: Le polynôme minimal de u est l' unique polynôme unitaire de degré 1 u KX tel que Autrement dit , si P KX est annulateur de u , alors il existe Q KX tel que P u Q. L' existence et l' unicité du polynôme minimal ***sont*** assurées par le fait que KX est principal et que Iu est un idéal de KX non réduit à 0KX ( voir le cours sur les anneaux )

**43026**: Le polynôme minimal de u est l' unique polynôme unitaire de degré 1 u KX tel que Autrement dit , si P KX est annulateur de u , alors il existe Q KX tel que P u Q. L' existence et l' unicité du polynôme minimal sont assurées par le fait que KX ***est*** principal et que Iu est un idéal de KX non réduit à 0KX ( voir le cours sur les anneaux )

**43031**: Le polynôme minimal de u est l' unique polynôme unitaire de degré 1 u KX tel que Autrement dit , si P KX est annulateur de u , alors il existe Q KX tel que P u Q. L' existence et l' unicité du polynôme minimal sont assurées par le fait que KX est principal et que Iu ***est*** un idéal de KX non réduit à 0KX ( voir le cours sur les anneaux )

**43051**: Alors E ***est*** non vide donc admet un plus petit élément d N. On pose u Iu 6 0KX unitaire tel que deg u d , et d 6 0

**43055**: Alors E est non vide donc ***admet*** un plus petit élément d N. On pose u Iu 6 0KX unitaire tel que deg u d , et d 6 0

**43063**: Alors E est non vide donc admet un plus petit élément d N. On ***pose*** u Iu 6 0KX unitaire tel que deg u d , et d 6 0

**43085**: On ***effectue*** la division euclidienne de P par u : Or Q u Iu car Iu est un idéal de KX donc R P Q u Iu

**43100**: On effectue la division euclidienne de P par u : Or Q u Iu car Iu ***est*** un idéal de KX donc R P Q u Iu

**43113**: On ***a*** nécessairement R 0KX ( sinon cela contredit la définition de d ) donc P Q u , autrement dit Iu u KX ( l' autre inclusion est immédiate puisque u Iu et que Iu est un idéal de KX )

**43120**: On a nécessairement R 0KX ( sinon cela ***contredit*** la définition de d ) donc P Q u , autrement dit Iu u KX ( l' autre inclusion est immédiate puisque u Iu et que Iu est un idéal de KX )

**43132**: On a nécessairement R 0KX ( sinon cela contredit la définition de d ) donc P Q u , autrement ***dit*** Iu u KX ( l' autre inclusion est immédiate puisque u Iu et que Iu est un idéal de KX )

**43140**: On a nécessairement R 0KX ( sinon cela contredit la définition de d ) donc P Q u , autrement dit Iu u KX ( l' autre inclusion ***est*** immédiate puisque u Iu et que Iu est un idéal de KX )

**43148**: On a nécessairement R 0KX ( sinon cela contredit la définition de d ) donc P Q u , autrement dit Iu u KX ( l' autre inclusion est immédiate puisque u Iu et que Iu ***est*** un idéal de KX )

**43156**: On ***définit*** de même pour une matrice carrée A Mn ( K ) la notion de polynôme annulateur , d' idéal annulateur IA et de polynôme minimal A

**43186**: Si E ***est*** une base de E ( en dimension finie ) et si A est la matrice de u dans la base E , alors : Soit E un K - espace vectoriel de dimension finie non nulle et soit u L ( E )

**43199**: Si E est une base de E ( en dimension finie ) et si A ***est*** la matrice de u dans la base E , alors : Soit E un K - espace vectoriel de dimension finie non nulle et soit u L ( E )

**43240**: Alors : En particulier , le polynôme minimal u ***a*** les mêmes racines que le polynôme caractéristique u

**43262**: Démonstration Si Sp(u ) , alors u ( ) 0 car u ***est*** annulateur de u ( propriété 5.1 , page 228 )

**43274**: ***Supposons*** que u ( ) 0

**43282**: Il ***existe*** P KX tel que Comme u ( u ) 0L ( E ) , on obtient : On a P ( u ) 6 0L ( E ) car deg P deg u ( sinon cela contredirait le fait que u est le polynôme minimal de u )

**43298**: Il existe P KX tel que Comme u ( u ) 0L ( E ) , on ***obtient*** : On a P ( u ) 6 0L ( E ) car deg P deg u ( sinon cela contredirait le fait que u est le polynôme minimal de u )

**43301**: Il existe P KX tel que Comme u ( u ) 0L ( E ) , on obtient : On ***a*** P ( u ) 6 0L ( E ) car deg P deg u ( sinon cela contredirait le fait que u est le polynôme minimal de u )

**43324**: Il existe P KX tel que Comme u ( u ) 0L ( E ) , on obtient : On a P ( u ) 6 0L ( E ) car deg P deg u ( sinon cela contredirait le fait que u ***est*** le polynôme minimal de u )

**43334**: On en ***déduit*** que u

**43340**: idE ne ***est*** pas injectif , c' est - à - dire que Sp(u )

**43378**: Alors : u ***est*** diagonalisable si , et seulement si , u est scindé à racines simples Démonstration Si u est scindé à racines simples , comme u est annulateur de u , le théorème 5.2 , page 229 démontre que u est diagonalisable

**43387**: Alors : u est diagonalisable si , et seulement si , u ***est*** scindé à racines simples Démonstration Si u est scindé à racines simples , comme u est annulateur de u , le théorème 5.2 , page 229 démontre que u est diagonalisable

**43395**: Alors : u est diagonalisable si , et seulement si , u est scindé à racines simples Démonstration Si u ***est*** scindé à racines simples , comme u est annulateur de u , le théorème 5.2 , page 229 démontre que u est diagonalisable

**43403**: Alors : u est diagonalisable si , et seulement si , u est scindé à racines simples Démonstration Si u est scindé à racines simples , comme u ***est*** annulateur de u , le théorème 5.2 , page 229 démontre que u est diagonalisable

**43414**: Alors : u est diagonalisable si , et seulement si , u est scindé à racines simples Démonstration Si u est scindé à racines simples , comme u est annulateur de u , le théorème 5.2 , page 229 ***démontre*** que u est diagonalisable

**43417**: Alors : u est diagonalisable si , et seulement si , u est scindé à racines simples Démonstration Si u est scindé à racines simples , comme u est annulateur de u , le théorème 5.2 , page 229 démontre que u ***est*** diagonalisable

**43420**: ***Supposons*** que u soit diagonalisable

**43439**: Alors : ***est*** scindé à racines simples et est annulateur de u car il existe une base une base E de E constituée de vecteurs propres de u donc pour tout e E , P ( u)(e ) 0E ( car e est associé à une valeur propre i ) d' où P ( u ) 0L ( E )

**43445**: Alors : est scindé à racines simples et ***est*** annulateur de u car il existe une base une base E de E constituée de vecteurs propres de u donc pour tout e E , P ( u)(e ) 0E ( car e est associé à une valeur propre i ) d' où P ( u ) 0L ( E )

**43451**: Alors : est scindé à racines simples et est annulateur de u car il ***existe*** une base une base E de E constituée de vecteurs propres de u donc pour tout e E , P ( u)(e ) 0E ( car e est associé à une valeur propre i ) d' où P ( u ) 0L ( E )

**43479**: Alors : est scindé à racines simples et est annulateur de u car il existe une base une base E de E constituée de vecteurs propres de u donc pour tout e E , P ( u)(e ) 0E ( car e ***est*** associé à une valeur propre i ) d' où P ( u ) 0L ( E )

**43500**: On en ***déduit*** que u est scindé à racines simples car il divise P qui est lui-même scindé à racines simples

**43503**: On en déduit que u ***est*** scindé à racines simples car il divise P qui est lui-même scindé à racines simples

**43510**: On en déduit que u est scindé à racines simples car il ***divise*** P qui est lui-même scindé à racines simples

**43513**: On en déduit que u est scindé à racines simples car il divise P qui ***est*** lui-même scindé à racines simples

**43543**: Théorème de Cayley - Hamilton Soit P un polynôme unitaire : ak X k X p KX La matrice compagnon du polynôme P ***est*** définie par : Soit P KX un polynôme unitaire

**43557**: Alors : Démonstration ***Reprenons*** les notations de la définition 5.5 , page précédente et effectuons l' opération élémentaire : de sorte que ( on développe ensuite selon la première ligne et on reconnaît le déterminant d' une matrice triangulaire ) : Théorème 5.3 Cayley - Hamilton Soit E un K - espace vectoriel de dimension finie non nulle et u L ( E )

**43568**: Alors : Démonstration Reprenons les notations de la définition 5.5 , page précédente et ***effectuons*** l' opération élémentaire : de sorte que ( on développe ensuite selon la première ligne et on reconnaît le déterminant d' une matrice triangulaire ) : Théorème 5.3 Cayley - Hamilton Soit E un K - espace vectoriel de dimension finie non nulle et u L ( E )

**43578**: Alors : Démonstration Reprenons les notations de la définition 5.5 , page précédente et effectuons l' opération élémentaire : de sorte que ( on ***développe*** ensuite selon la première ligne et on reconnaît le déterminant d' une matrice triangulaire ) : Théorème 5.3 Cayley - Hamilton Soit E un K - espace vectoriel de dimension finie non nulle et u L ( E )

**43586**: Alors : Démonstration Reprenons les notations de la définition 5.5 , page précédente et effectuons l' opération élémentaire : de sorte que ( on développe ensuite selon la première ligne et on ***reconnaît*** le déterminant d' une matrice triangulaire ) : Théorème 5.3 Cayley - Hamilton Soit E un K - espace vectoriel de dimension finie non nulle et u L ( E )

**43621**: Alors Autrement ***dit*** , le polynôme caractéristique u de u est annulateur de u. Démonstration Soit x E , x 6 0E ( possible car n dim E 1 )

**43629**: Alors Autrement dit , le polynôme caractéristique u de u ***est*** annulateur de u. Démonstration Soit x E , x 6 0E ( possible car n dim E 1 )

**43651**: Il ***existe*** un plus petit entier p 1 tel que la famille ( x , u(x ) ,

**43698**: , up1 ( x ) ) ***est*** libre

**43705**: En particulier , il ***existe*** ak X k KX en reconnaissant la matrice compagnon C(P ) de P ( qui est unitaire )

**43721**: En particulier , il existe ak X k KX en reconnaissant la matrice compagnon C(P ) de P ( qui ***est*** unitaire )

**43726**: On ***a*** alors d' après la propriété 5.4 , page ci - contre : d' où , puisque P ( u)(x ) 0E : Puisque ceci est vrai pour tout x E ( c' est aussi vrai pour x 0E ) , on a u ( u ) 0L ( E )

**43751**: On a alors d' après la propriété 5.4 , page ci - contre : d' où , puisque P ( u)(x ) 0E : Puisque ceci ***est*** vrai pour tout x E ( c' est aussi vrai pour x 0E ) , on a u ( u ) 0L ( E )

**43759**: On a alors d' après la propriété 5.4 , page ci - contre : d' où , puisque P ( u)(x ) 0E : Puisque ceci est vrai pour tout x E ( c' ***est*** aussi vrai pour x 0E ) , on a u ( u ) 0L ( E )

**43768**: On a alors d' après la propriété 5.4 , page ci - contre : d' où , puisque P ( u)(x ) 0E : Puisque ceci est vrai pour tout x E ( c' est aussi vrai pour x 0E ) , on ***a*** u ( u ) 0L ( E )

**43784**: Le théorème de Cayley - Hamilton ***peut*** se reformuler ainsi : le polynôme minimal u divise le polynôme caractéristique u

**43793**: Le théorème de Cayley - Hamilton peut se reformuler ainsi : le polynôme minimal u ***divise*** le polynôme caractéristique u

**43813**: En particulier , puisque deg u dim E et u 6 0KX , on ***a*** : 1 deg u dim E On peut également démontrer ce théorème sans passer par la notion de matrice compagnon , en le démontrant d' abord dans le cas d' une matrice trigonalisable ( en utilisant notamment le fait que dans ce cas son polynôme caractéristique est scindé )

**43821**: En particulier , puisque deg u dim E et u 6 0KX , on a : 1 deg u dim E On ***peut*** également démontrer ce théorème sans passer par la notion de matrice compagnon , en le démontrant d' abord dans le cas d' une matrice trigonalisable ( en utilisant notamment le fait que dans ce cas son polynôme caractéristique est scindé )

**43860**: En particulier , puisque deg u dim E et u 6 0KX , on a : 1 deg u dim E On peut également démontrer ce théorème sans passer par la notion de matrice compagnon , en le démontrant d' abord dans le cas d' une matrice trigonalisable ( en utilisant notamment le fait que dans ce cas son polynôme caractéristique ***est*** scindé )

**43866**: Mais on ***peut*** toujours s' y ramener en se plaçant dans le corps des racines de A u avec A la matrice de u dans une base de E ( dans le cas où K R , cela revient à considérer A comme une matrice à coefficients dans C )

**43902**: Mais on peut toujours s' y ramener en se plaçant dans le corps des racines de A u avec A la matrice de u dans une base de E ( dans le cas où K R , cela ***revient*** à considérer A comme une matrice à coefficients dans C )

**43916**: On ***peut*** également , dans le cas K R ou C utiliser un argument topologique ( voir la partie suivante )

**43961**: Les propositions suivantes ***sont*** équivalentes : 1

**43967**: u ***est*** trigonalisable 2

**43972**: il ***existe*** un polynôme annulateur de u scindé 3

**43982**: u ***est*** scindé 4

**43987**: u ***est*** scindé

**44000**: Retour sur le calcul de puissances de matrices Lorsqu' on ***connaît*** un polynôme annulateur P KX d' une matrice A Mp ( K ) , pour calculer An on peut utiliser la division euclidienne de X n par P : La matrice : a pour polynôme annulateur ( X 2)3 donc : 5.1.1 Pour les matrices suivantes , préciser les polynômes caractéristiques , minimaux , espaces propres et préciser si elles sont diagonalisables ( dans Mn ( C ) ) : ( a ) Dk ( ) ( matrice de dilatation ) ( b ) Tk , ( ) ( matrice de transvection ) ( c ) P ( matrice de permutation )

**44019**: Retour sur le calcul de puissances de matrices Lorsqu' on connaît un polynôme annulateur P KX d' une matrice A Mp ( K ) , pour calculer An on ***peut*** utiliser la division euclidienne de X n par P : La matrice : a pour polynôme annulateur ( X 2)3 donc : 5.1.1 Pour les matrices suivantes , préciser les polynômes caractéristiques , minimaux , espaces propres et préciser si elles sont diagonalisables ( dans Mn ( C ) ) : ( a ) Dk ( ) ( matrice de dilatation ) ( b ) Tk , ( ) ( matrice de transvection ) ( c ) P ( matrice de permutation )

**44033**: Retour sur le calcul de puissances de matrices Lorsqu' on connaît un polynôme annulateur P KX d' une matrice A Mp ( K ) , pour calculer An on peut utiliser la division euclidienne de X n par P : La matrice : ***a*** pour polynôme annulateur ( X 2)3 donc : 5.1.1 Pour les matrices suivantes , préciser les polynômes caractéristiques , minimaux , espaces propres et préciser si elles sont diagonalisables ( dans Mn ( C ) ) : ( a ) Dk ( ) ( matrice de dilatation ) ( b ) Tk , ( ) ( matrice de transvection ) ( c ) P ( matrice de permutation )

**44061**: Retour sur le calcul de puissances de matrices Lorsqu' on connaît un polynôme annulateur P KX d' une matrice A Mp ( K ) , pour calculer An on peut utiliser la division euclidienne de X n par P : La matrice : a pour polynôme annulateur ( X 2)3 donc : 5.1.1 Pour les matrices suivantes , préciser les polynômes caractéristiques , minimaux , espaces propres et préciser si elles ***sont*** diagonalisables ( dans Mn ( C ) ) : ( a ) Dk ( ) ( matrice de dilatation ) ( b ) Tk , ( ) ( matrice de transvection ) ( c ) P ( matrice de permutation )

**44072**: Retour sur le calcul de puissances de matrices Lorsqu' on connaît un polynôme annulateur P KX d' une matrice A Mp ( K ) , pour calculer An on peut utiliser la division euclidienne de X n par P : La matrice : a pour polynôme annulateur ( X 2)3 donc : 5.1.1 Pour les matrices suivantes , préciser les polynômes caractéristiques , minimaux , espaces propres et préciser si elles sont diagonalisables ( dans Mn ( C ) ) : ( ***a*** ) Dk ( ) ( matrice de dilatation ) ( b ) Tk , ( ) ( matrice de transvection ) ( c ) P ( matrice de permutation )

**44124**: Quelle ***est*** la structure algébrique de E ? Démontrer que si A E vérifie Ap I2 pour un p N , alors A12 I2

**44134**: Quelle est la structure algébrique de E ? Démontrer que si ***A*** E vérifie Ap I2 pour un p N , alors A12 I2

**44136**: Quelle est la structure algébrique de E ? Démontrer que si A E ***vérifie*** Ap I2 pour un p N , alors A12 I2

**44172**: ( ***a*** ) Démontrer qu' il existe un unique polynôme unitaire P KX vérifiant : On note mx ce polynôme

**44177**: ( a ) Démontrer qu' il ***existe*** un unique polynôme unitaire P KX vérifiant : On note mx ce polynôme

**44221**: ( c ) Démontrer que le polynôme minimal f de f ***vérifie*** : iii

**44226**: Il ***existe*** un x E tel que f mx

**44269**: 5.1.5 Soit E un C - espace vectoriel de dimension finie non nulle et u L ( E ) , donner une condition nécessaire et suffisante pour que : u2 diagonalisable u diagonalisable Que ***devient*** ce résultat lorsque le corps est R ? 5.1.6 Soit A Mn ( R ) telle que A3 3 A 2 In 0n

**44275**: 5.1.5 Soit E un C - espace vectoriel de dimension finie non nulle et u L ( E ) , donner une condition nécessaire et suffisante pour que : u2 diagonalisable u diagonalisable Que devient ce résultat lorsque le corps ***est*** R ? 5.1.6 Soit A Mn ( R ) telle que A3 3 A 2 In 0n

**44318**: A est-elle diagonalisable ? Calculer Ap pour p Z. Topologie sur les endomorphismes Soit ( E , k k ) un espace vectoriel normé ***a*** , on peut munir Lc ( E ) ( l' espace des endomorphismes continus de E ) de la norme subordonnée donnée par : On a alors : De plus , est une norme d' algèbre : Si E est de dimension finie , tous les endomorphismes sont continues : Lc ( E ) L ( E ) ( c' est faux en dimension infinie )

**44321**: A est-elle diagonalisable ? Calculer Ap pour p Z. Topologie sur les endomorphismes Soit ( E , k k ) un espace vectoriel normé a , on ***peut*** munir Lc ( E ) ( l' espace des endomorphismes continus de E ) de la norme subordonnée donnée par : On a alors : De plus , est une norme d' algèbre : Si E est de dimension finie , tous les endomorphismes sont continues : Lc ( E ) L ( E ) ( c' est faux en dimension infinie )

**44344**: A est-elle diagonalisable ? Calculer Ap pour p Z. Topologie sur les endomorphismes Soit ( E , k k ) un espace vectoriel normé a , on peut munir Lc ( E ) ( l' espace des endomorphismes continus de E ) de la norme subordonnée donnée par : On ***a*** alors : De plus , est une norme d' algèbre : Si E est de dimension finie , tous les endomorphismes sont continues : Lc ( E ) L ( E ) ( c' est faux en dimension infinie )

**44350**: A est-elle diagonalisable ? Calculer Ap pour p Z. Topologie sur les endomorphismes Soit ( E , k k ) un espace vectoriel normé a , on peut munir Lc ( E ) ( l' espace des endomorphismes continus de E ) de la norme subordonnée donnée par : On a alors : De plus , ***est*** une norme d' algèbre : Si E est de dimension finie , tous les endomorphismes sont continues : Lc ( E ) L ( E ) ( c' est faux en dimension infinie )

**44358**: A est-elle diagonalisable ? Calculer Ap pour p Z. Topologie sur les endomorphismes Soit ( E , k k ) un espace vectoriel normé a , on peut munir Lc ( E ) ( l' espace des endomorphismes continus de E ) de la norme subordonnée donnée par : On a alors : De plus , est une norme d' algèbre : Si E ***est*** de dimension finie , tous les endomorphismes sont continues : Lc ( E ) L ( E ) ( c' est faux en dimension infinie )

**44366**: A est-elle diagonalisable ? Calculer Ap pour p Z. Topologie sur les endomorphismes Soit ( E , k k ) un espace vectoriel normé a , on peut munir Lc ( E ) ( l' espace des endomorphismes continus de E ) de la norme subordonnée donnée par : On a alors : De plus , est une norme d' algèbre : Si E est de dimension finie , tous les endomorphismes ***sont*** continues : Lc ( E ) L ( E ) ( c' est faux en dimension infinie )

**44379**: A est-elle diagonalisable ? Calculer Ap pour p Z. Topologie sur les endomorphismes Soit ( E , k k ) un espace vectoriel normé a , on peut munir Lc ( E ) ( l' espace des endomorphismes continus de E ) de la norme subordonnée donnée par : On a alors : De plus , est une norme d' algèbre : Si E est de dimension finie , tous les endomorphismes sont continues : Lc ( E ) L ( E ) ( c' ***est*** faux en dimension infinie )

**44396**: ***a.*** Dans cette partie , quand on parle d' un espace vectoriel normé ( E , k k ) , on se place toujours dans le corps K R ou C. Proposition 5.2 Densité des endomorphismes inversibles et diagonalisables Soit ( E , k k ) un espace vectoriel normé de dimension finie non nulle

**44403**: a. Dans cette partie , quand on ***parle*** d' un espace vectoriel normé ( E , k k ) , on se place toujours dans le corps K R ou C. Proposition 5.2 Densité des endomorphismes inversibles et diagonalisables Soit ( E , k k ) un espace vectoriel normé de dimension finie non nulle

**44418**: a. Dans cette partie , quand on parle d' un espace vectoriel normé ( E , k k ) , on se ***place*** toujours dans le corps K R ou C. Proposition 5.2 Densité des endomorphismes inversibles et diagonalisables Soit ( E , k k ) un espace vectoriel normé de dimension finie non nulle

**44460**: GL ( E ) ***est*** un ouvert dense dans L ( E ) 2

**44482**: si K C , l' ensemble des endomorphismes de E diagonalisables ***est*** dense dans L ( E )

**44497**: GL ( E ) ***est*** un ouvert de L ( E ) car GL ( E ) det1 ( K ) avec det : L ( E ) K continue et K ouvert de K. Soit u L ( E )

**44522**: GL ( E ) est un ouvert de L ( E ) car GL ( E ) det1 ( K ) avec det : L ( E ) K ***continue*** et K ouvert de K. Soit u L ( E )

**44540**: Son polynôme caractéristique u ne ***a*** qu' un nombre fini de racines ( car deg u dim E 1 ) donc il existe r 0 tel que u ne s' annule pas sur B(0 , r ) 0

**44557**: Son polynôme caractéristique u ne a qu' un nombre fini de racines ( car deg u dim E 1 ) donc il ***existe*** r 0 tel que u ne s' annule pas sur B(0 , r ) 0

**44565**: Son polynôme caractéristique u ne a qu' un nombre fini de racines ( car deg u dim E 1 ) donc il existe r 0 tel que u ne s' ***annule*** pas sur B(0 , r ) 0

**44575**: Autrement ***dit*** , pour tout B(0 , r ) 0 , u

**44602**: idE u quand 0 , u ***est*** bien la limite d' une suite d' endomorphismes inversibles , donc GL ( E ) est dense dans L ( E )

**44618**: idE u quand 0 , u est bien la limite d' une suite d' endomorphismes inversibles , donc GL ( E ) ***est*** dense dans L ( E )

**44633**: Puisque K C , il ***existe*** une base E dans laquelle T MatE ( u ) est triangulaire supérieure ( n dim E ) : si les valeurs propres i sont toutes égales Alors , pour tout p N , p 2 , Tp est diagonalisable car elle a n valeurs propres distinctes par construction et Tp T quand p , donc u est bien la limite d' une suite d' endomorphismes diagonalisables de E ( les up correspondant aux matrices Tp dans la base E de E )

**44644**: Puisque K C , il existe une base E dans laquelle T MatE ( u ) ***est*** triangulaire supérieure ( n dim E ) : si les valeurs propres i sont toutes égales Alors , pour tout p N , p 2 , Tp est diagonalisable car elle a n valeurs propres distinctes par construction et Tp T quand p , donc u est bien la limite d' une suite d' endomorphismes diagonalisables de E ( les up correspondant aux matrices Tp dans la base E de E )

**44658**: Puisque K C , il existe une base E dans laquelle T MatE ( u ) est triangulaire supérieure ( n dim E ) : si les valeurs propres i ***sont*** toutes égales Alors , pour tout p N , p 2 , Tp est diagonalisable car elle a n valeurs propres distinctes par construction et Tp T quand p , donc u est bien la limite d' une suite d' endomorphismes diagonalisables de E ( les up correspondant aux matrices Tp dans la base E de E )

**44672**: Puisque K C , il existe une base E dans laquelle T MatE ( u ) est triangulaire supérieure ( n dim E ) : si les valeurs propres i sont toutes égales Alors , pour tout p N , p 2 , Tp ***est*** diagonalisable car elle a n valeurs propres distinctes par construction et Tp T quand p , donc u est bien la limite d' une suite d' endomorphismes diagonalisables de E ( les up correspondant aux matrices Tp dans la base E de E )

**44676**: Puisque K C , il existe une base E dans laquelle T MatE ( u ) est triangulaire supérieure ( n dim E ) : si les valeurs propres i sont toutes égales Alors , pour tout p N , p 2 , Tp est diagonalisable car elle ***a*** n valeurs propres distinctes par construction et Tp T quand p , donc u est bien la limite d' une suite d' endomorphismes diagonalisables de E ( les up correspondant aux matrices Tp dans la base E de E )

**44691**: Puisque K C , il existe une base E dans laquelle T MatE ( u ) est triangulaire supérieure ( n dim E ) : si les valeurs propres i sont toutes égales Alors , pour tout p N , p 2 , Tp est diagonalisable car elle a n valeurs propres distinctes par construction et Tp T quand p , donc u ***est*** bien la limite d' une suite d' endomorphismes diagonalisables de E ( les up correspondant aux matrices Tp dans la base E de E )

**44724**: Tp T Diag Le deuxième résultat ***est*** faux sur R , par exemple la matrice : ne est pas la limite d' une suite de matrices de M2 ( R ) diagonalisables

**44735**: Tp T Diag Le deuxième résultat est faux sur R , par exemple la matrice : ne ***est*** pas la limite d' une suite de matrices de M2 ( R ) diagonalisables

**44759**: Cependant , la démonstration ci - dessus ne ***utilise*** que le fait que u est trigonalisable donc on a démontré que tout endomorphisme trigonalisable est limite d' une suite d' endomorphismes diagonalisables

**44765**: Cependant , la démonstration ci - dessus ne utilise que le fait que u ***est*** trigonalisable donc on a démontré que tout endomorphisme trigonalisable est limite d' une suite d' endomorphismes diagonalisables

**44775**: Cependant , la démonstration ci - dessus ne utilise que le fait que u est trigonalisable donc on a démontré que tout endomorphisme trigonalisable ***est*** limite d' une suite d' endomorphismes diagonalisables

**44788**: Ces propriétés de densité ***sont*** utiles pour démontrer des résultats sur L ( E ) : on démontre le résultat pour les endomorphismes inversibles ou digonalisables , puis on étend le résultat par densité ( voir les exercices 5.2 , page 244 )

**44801**: Ces propriétés de densité sont utiles pour démontrer des résultats sur L ( E ) : on ***démontre*** le résultat pour les endomorphismes inversibles ou digonalisables , puis on étend le résultat par densité ( voir les exercices 5.2 , page 244 )

**44813**: Ces propriétés de densité sont utiles pour démontrer des résultats sur L ( E ) : on démontre le résultat pour les endomorphismes inversibles ou digonalisables , puis on ***étend*** le résultat par densité ( voir les exercices 5.2 , page 244 )

**44858**: Alors : pour toute valeur propre de u , u Démonstration Si ***est*** une valeur propre de u , en notant x E 0E un vecteur propre associé , on a ku(x)k u kxk ku(x)k k.xk kxk d' où u en divisant par kxk 6 0E

**44876**: Alors : pour toute valeur propre de u , u Démonstration Si est une valeur propre de u , en notant x E 0E un vecteur propre associé , on ***a*** ku(x)k u kxk ku(x)k k.xk kxk d' où u en divisant par kxk 6 0E

**44905**: Le rayon spectral de A ***est*** défini par : D' après la propriété précédente , pour ne importe quelle norme d' algèbre sur Mn ( C ) , on a : Soit A Mn ( C ) et soit 0

**44917**: Le rayon spectral de A est défini par : D' après la propriété précédente , pour ne ***importe*** quelle norme d' algèbre sur Mn ( C ) , on a : Soit A Mn ( C ) et soit 0

**44929**: Le rayon spectral de A est défini par : D' après la propriété précédente , pour ne importe quelle norme d' algèbre sur Mn ( C ) , on ***a*** : Soit A Mn ( C ) et soit 0

**44942**: Il ***existe*** une norme subordonnée A , sur Mn ( C ) telle que Démonstration On trigonalise A P T P 1 avec T ti , j ( i , j)1,n2 Mn ( C ) triangulaire supérieure et P GLn ( C )

**44988**: On Un calcul ***démontre*** que : donc en particulier : Posons alors : D' après ce qui précède : Par conséquent , pour tout 0 , si 0 est assez petit : Il reste alors à vérifier que A , est une norme subordonnée ( ce qui donne alors l' inégalité AA , ( A ) d' après la remarque 5.10 , de la présente page )

**44995**: On Un calcul démontre que : donc en particulier : ***Posons*** alors : D' après ce qui précède : Par conséquent , pour tout 0 , si 0 est assez petit : Il reste alors à vérifier que A , est une norme subordonnée ( ce qui donne alors l' inégalité AA , ( A ) d' après la remarque 5.10 , de la présente page )

**45002**: On Un calcul démontre que : donc en particulier : Posons alors : D' après ce qui ***précède*** : Par conséquent , pour tout 0 , si 0 est assez petit : Il reste alors à vérifier que A , est une norme subordonnée ( ce qui donne alors l' inégalité AA , ( A ) d' après la remarque 5.10 , de la présente page )

**45013**: On Un calcul démontre que : donc en particulier : Posons alors : D' après ce qui précède : Par conséquent , pour tout 0 , si 0 ***est*** assez petit : Il reste alors à vérifier que A , est une norme subordonnée ( ce qui donne alors l' inégalité AA , ( A ) d' après la remarque 5.10 , de la présente page )

**45018**: On Un calcul démontre que : donc en particulier : Posons alors : D' après ce qui précède : Par conséquent , pour tout 0 , si 0 est assez petit : Il ***reste*** alors à vérifier que A , est une norme subordonnée ( ce qui donne alors l' inégalité AA , ( A ) d' après la remarque 5.10 , de la présente page )

**45025**: On Un calcul démontre que : donc en particulier : Posons alors : D' après ce qui précède : Par conséquent , pour tout 0 , si 0 est assez petit : Il reste alors à vérifier que A , ***est*** une norme subordonnée ( ce qui donne alors l' inégalité AA , ( A ) d' après la remarque 5.10 , de la présente page )

**45032**: On Un calcul démontre que : donc en particulier : Posons alors : D' après ce qui précède : Par conséquent , pour tout 0 , si 0 est assez petit : Il reste alors à vérifier que A , est une norme subordonnée ( ce qui ***donne*** alors l' inégalité AA , ( A ) d' après la remarque 5.10 , de la présente page )

**45057**: Pour cela , on ***démontre*** que X 7 k(P D ) 1 Xk est une norme sur Mn,1 ( C ) et que N est sa norme subordonnée sur Mn ( C ) ( en exercice )

**45066**: Pour cela , on démontre que X 7 k(P D ) 1 Xk ***est*** une norme sur Mn,1 ( C ) et que N est sa norme subordonnée sur Mn ( C ) ( en exercice )

**45077**: Pour cela , on démontre que X 7 k(P D ) 1 Xk est une norme sur Mn,1 ( C ) et que N ***est*** sa norme subordonnée sur Mn ( C ) ( en exercice )

**45098**: Dans beaucoup d' applications , on s' ***intéresse*** à des suites récurrentes de la forme : par exemple pour résoudre de manière approchée des équations différentielles ou des équations aux dérivées partielles

**45125**: Il ***est*** souvent crucial que la suite ( Uk ) kN soit bornée ( par exemple pour garantir la stabilité du schéma numérique considéré )

**45153**: Comme : il ***faut*** que ( Ak ) kN soit borné

**45165**: La propriété précédente ***implique*** que : ( Ak ) kN est bornée , alors ( A ) 1 et si ( A ) 1 alors ( Ak ) kN est bornée Ainsi , le calcul du rayon spectral de A ( souvent calculé de manière approchée dans les applications ) nous permet de démontrer la stabilité d' une méthode numérique

**45172**: La propriété précédente implique que : ( Ak ) kN ***est*** bornée , alors ( A ) 1 et si ( A ) 1 alors ( Ak ) kN est bornée Ainsi , le calcul du rayon spectral de A ( souvent calculé de manière approchée dans les applications ) nous permet de démontrer la stabilité d' une méthode numérique

**45191**: La propriété précédente implique que : ( Ak ) kN est bornée , alors ( A ) 1 et si ( A ) 1 alors ( Ak ) kN ***est*** bornée Ainsi , le calcul du rayon spectral de A ( souvent calculé de manière approchée dans les applications ) nous permet de démontrer la stabilité d' une méthode numérique

**45213**: La propriété précédente implique que : ( Ak ) kN est bornée , alors ( A ) 1 et si ( A ) 1 alors ( Ak ) kN est bornée Ainsi , le calcul du rayon spectral de A ( souvent calculé de manière approchée dans les applications ) nous ***permet*** de démontrer la stabilité d' une méthode numérique

**45270**: Pour tout u L ( E ) telle que u R , la série k an uk ***est*** convergente et sa somme définit un endomorphisme de E. De plus , l' application est continue sur u L ( E ) , u R. Démonstration La série converge absolument car , pour tout k N , ak .uk ak uk et que k ak uk converge ( on est dans le disque de convergence D(0 , R ) )

**45275**: Pour tout u L ( E ) telle que u R , la série k an uk est convergente et sa somme ***définit*** un endomorphisme de E. De plus , l' application est continue sur u L ( E ) , u R. Démonstration La série converge absolument car , pour tout k N , ak .uk ak uk et que k ak uk converge ( on est dans le disque de convergence D(0 , R ) )

**45285**: Pour tout u L ( E ) telle que u R , la série k an uk est convergente et sa somme définit un endomorphisme de E. De plus , l' application ***est*** continue sur u L ( E ) , u R. Démonstration La série converge absolument car , pour tout k N , ak .uk ak uk et que k ak uk converge ( on est dans le disque de convergence D(0 , R ) )

**45286**: Pour tout u L ( E ) telle que u R , la série k an uk est convergente et sa somme définit un endomorphisme de E. De plus , l' application est ***continue*** sur u L ( E ) , u R. Démonstration La série converge absolument car , pour tout k N , ak .uk ak uk et que k ak uk converge ( on est dans le disque de convergence D(0 , R ) )

**45320**: Pour tout u L ( E ) telle que u R , la série k an uk est convergente et sa somme définit un endomorphisme de E. De plus , l' application est continue sur u L ( E ) , u R. Démonstration La série converge absolument car , pour tout k N , ak .uk ak uk et que k ak uk converge ( on ***est*** dans le disque de convergence D(0 , R ) )

**45337**: Puisque L ( E ) ***est*** fermé ( car de dimension finie ) , la somme définit bien un élément de L ( E )

**45348**: Puisque L ( E ) est fermé ( car de dimension finie ) , la somme ***définit*** bien un élément de L ( E )

**45359**: k ***est*** normale sur tous les disques u L' argument ci - dessus démontre que la convergence de L ( E ) , u r avec r 0 , R. Pour tout k N , u 7 ak .uk est continue , donc u 7 k0 ak .u est continu sur u L ( E ) , u r pour tout r 0 , R , donc sur u L ( E ) , u R. On peut ainsi parler d' exponentielle , de sinus , de logarithmes , etc. d' endomorphismes ou de matrices

**45371**: k est normale sur tous les disques u L' argument ci - dessus ***démontre*** que la convergence de L ( E ) , u r avec r 0 , R. Pour tout k N , u 7 ak .uk est continue , donc u 7 k0 ak .u est continu sur u L ( E ) , u r pour tout r 0 , R , donc sur u L ( E ) , u R. On peut ainsi parler d' exponentielle , de sinus , de logarithmes , etc. d' endomorphismes ou de matrices

**45397**: k est normale sur tous les disques u L' argument ci - dessus démontre que la convergence de L ( E ) , u r avec r 0 , R. Pour tout k N , u 7 ak .uk ***est*** continue , donc u 7 k0 ak .u est continu sur u L ( E ) , u r pour tout r 0 , R , donc sur u L ( E ) , u R. On peut ainsi parler d' exponentielle , de sinus , de logarithmes , etc. d' endomorphismes ou de matrices

**45398**: k est normale sur tous les disques u L' argument ci - dessus démontre que la convergence de L ( E ) , u r avec r 0 , R. Pour tout k N , u 7 ak .uk est ***continue*** , donc u 7 k0 ak .u est continu sur u L ( E ) , u r pour tout r 0 , R , donc sur u L ( E ) , u R. On peut ainsi parler d' exponentielle , de sinus , de logarithmes , etc. d' endomorphismes ou de matrices

**45406**: k est normale sur tous les disques u L' argument ci - dessus démontre que la convergence de L ( E ) , u r avec r 0 , R. Pour tout k N , u 7 ak .uk est continue , donc u 7 k0 ak .u ***est*** continu sur u L ( E ) , u r pour tout r 0 , R , donc sur u L ( E ) , u R. On peut ainsi parler d' exponentielle , de sinus , de logarithmes , etc. d' endomorphismes ou de matrices

**45435**: k est normale sur tous les disques u L' argument ci - dessus démontre que la convergence de L ( E ) , u r avec r 0 , R. Pour tout k N , u 7 ak .uk est continue , donc u 7 k0 ak .u est continu sur u L ( E ) , u r pour tout r 0 , R , donc sur u L ( E ) , u R. On ***peut*** ainsi parler d' exponentielle , de sinus , de logarithmes , etc. d' endomorphismes ou de matrices

**45456**: partielles qui ***sont*** des k0 ak .u est un polynôme en u car c' est la limite de la suite des sommes polynômes en u et Ku est fermé car de dimension finie

**45461**: partielles qui sont des k0 ak .u ***est*** un polynôme en u car c' est la limite de la suite des sommes polynômes en u et Ku est fermé car de dimension finie

**45468**: partielles qui sont des k0 ak .u est un polynôme en u car c' ***est*** la limite de la suite des sommes polynômes en u et Ku est fermé car de dimension finie

**45481**: partielles qui sont des k0 ak .u est un polynôme en u car c' est la limite de la suite des sommes polynômes en u et Ku ***est*** fermé car de dimension finie

**45494**: En particulier , k0 ak .uk ***commute*** avec Soit u L ( E ) tel que u 1

**45510**: Alors idE u ***est*** inversible et En effet , cette somme existe d' après la propriété ci - dessus et : et de même : uk ( idE u ) idE Soit ( E , k k ) un espace vectoriel normé de dimension finie et soit u L ( E )

**45518**: Alors idE u est inversible et En effet , cette somme ***existe*** d' après la propriété ci - dessus et : et de même : uk ( idE u ) idE Soit ( E , k k ) un espace vectoriel normé de dimension finie et soit u L ( E )

**45564**: L' exponentielle de u ***est*** défini exp(u ) eu Soit ( E , k k ) un espace vectoriel normé de dimension finie et soit ( u , v ) L ( E)2

**45621**: pour tout p GL ( E ) , exp(p1 u p ) p1 exp(u ) p ( en particulier , deux matrices semblables ***ont*** des exponentielles semblables ) Démonstration 1

**45639**: Puisque ***est*** une norme d' algèbre : uk exp(u ) 3

**45651**: C' ***est*** un produit de Cauchy

**45667**: Soit N N , posons : Comme u et v ***commutent*** : d' où le résultat puisque N exp(u ) exp(v ) exp(u v ) quand N

**45691**: D' après ce qui ***précède*** , comme u et u commutent : d' où le résultat

**45697**: D' après ce qui précède , comme u et u ***commutent*** : d' où le résultat

**45719**: L' exponentielle de matrices ***sert*** notamment pour étudier les systèmes linéaires différentiels à coefficients constants ( voir le chapitre 4 ) : La solution vérifiant la condition initiale X(t0 ) X0 est donnée par : exp(s

**45746**: L' exponentielle de matrices sert notamment pour étudier les systèmes linéaires différentiels à coefficients constants ( voir le chapitre 4 ) : La solution vérifiant la condition initiale X(t0 ) X0 ***est*** donnée par : exp(s

**45758**: A ) B(s ) ds Nous ***sommes*** donc ramenés au calcul de exp(t

**45775**: A ) on ***peut*** : 1

**45790**: Calculer An puis sommer la série n t n!.A ( c' ***est*** rarement une méthode efficace )

**45801**: Si la matrice A ***est*** diagonalisable : A P Diag(1 ,

**45818**: , p ) P 1 , on ***a*** : ( voir le code ci-dessous )

**45835**: Dans le cas général , on ***peut*** utiliser la décomposition de Dunford ( voir la partie suivante )

**45855**: I Cependant , si la matrice ne ***est*** pas diagonalisable , il est souvent plus efficace pour résoudre un système linéaire différentielle de trigonaliser la matrice A et de résoudre un système différentiel triangulaire ( voir le chapitre 4 )

**45860**: I Cependant , si la matrice ne est pas diagonalisable , il ***est*** souvent plus efficace pour résoudre un système linéaire différentielle de trigonaliser la matrice A et de résoudre un système différentiel triangulaire ( voir le chapitre 4 )

**45908**: ( ***a*** ) Démontrer que u 7 u est continue de L ( E ) dans Kn X. ( b ) En déduire par un argument de densité que : ( c ) Démontrer que le résultat est encore vrai dans ne importe quel corps K et ne importe quel K - espace vectoriel E ( par un argument algébrique )

**45915**: ( a ) Démontrer que u 7 u ***est*** continue de L ( E ) dans Kn X. ( b ) En déduire par un argument de densité que : ( c ) Démontrer que le résultat est encore vrai dans ne importe quel corps K et ne importe quel K - espace vectoriel E ( par un argument algébrique )

**45916**: ( a ) Démontrer que u 7 u est ***continue*** de L ( E ) dans Kn X. ( b ) En déduire par un argument de densité que : ( c ) Démontrer que le résultat est encore vrai dans ne importe quel corps K et ne importe quel K - espace vectoriel E ( par un argument algébrique )

**45944**: ( a ) Démontrer que u 7 u est continue de L ( E ) dans Kn X. ( b ) En déduire par un argument de densité que : ( c ) Démontrer que le résultat ***est*** encore vrai dans ne importe quel corps K et ne importe quel K - espace vectoriel E ( par un argument algébrique )

**45949**: ( a ) Démontrer que u 7 u est continue de L ( E ) dans Kn X. ( b ) En déduire par un argument de densité que : ( c ) Démontrer que le résultat est encore vrai dans ne ***importe*** quel corps K et ne importe quel K - espace vectoriel E ( par un argument algébrique )

**45955**: ( a ) Démontrer que u 7 u est continue de L ( E ) dans Kn X. ( b ) En déduire par un argument de densité que : ( c ) Démontrer que le résultat est encore vrai dans ne importe quel corps K et ne ***importe*** quel K - espace vectoriel E ( par un argument algébrique )

**46005**: ( ***a*** ) Démontrer que l' intérieur de l' ensemble des endomorphismes de E diagonalisables est l' ensemble des endomorphismes de E diagonalisables avec n valeurs propres distinctes

**46019**: ( a ) Démontrer que l' intérieur de l' ensemble des endomorphismes de E diagonalisables ***est*** l' ensemble des endomorphismes de E diagonalisables avec n valeurs propres distinctes

**46045**: ( b ) Démontrer que l' adhérence des endomorphismes de E diagonalisables ***est*** l' ensemble des endomorphismes de E trigonalisables

**46055**: ( ***a*** ) Démontrer que p A Mn ( R ) , rang A p est fermé dans Mn ( R )

**46069**: ( a ) Démontrer que p A Mn ( R ) , rang A p ***est*** fermé dans Mn ( R )

**46157**: Démontrer que : 5.2.6 Démontrer que : A Mn ( C ) , det(exp(A ) ) exp(trace(A ) ) Décomposition de Dunford Soit E un K - espace vectoriel de dimension finie non nulle et u L ( E ) nilpotent ***a***

**46168**: Alors : u ( 1)dim E X dim E ***a.*** C' est - à - dire qu' il existe p N tel que up 0L ( E )

**46177**: Alors : u ( 1)dim E X dim E a. C' est - à - dire qu' il ***existe*** p N tel que up 0L ( E )

**46202**: Si n 1 , c' ***est*** évident

**46209**: Soit N , on ***suppose*** que le résultat est vrai au rang n. Soit u L ( E ) nilpotent ( avec dim E n 1 ) , soit p N tel que up 0L ( E )

**46213**: Soit N , on suppose que le résultat ***est*** vrai au rang n. Soit u L ( E ) nilpotent ( avec dim E n 1 ) , soit p N tel que up 0L ( E )

**46245**: On ***a*** det(u)p det(up ) 0 donc det(u ) 0

**46264**: En particulier , Ker(u ) 6 0E donc il ***existe*** e1 Ker(u )

**46287**: , en1 ) de E. On ***a*** Mais u est nilpotent : il existe p N tel que up 0L ( E ) donc donc C p 0n

**46290**: , en1 ) de E. On a Mais u ***est*** nilpotent : il existe p N tel que up 0L ( E ) donc donc C p 0n

**46294**: , en1 ) de E. On a Mais u est nilpotent : il ***existe*** p N tel que up 0L ( E ) donc donc C p 0n

**46314**: L' hypothèse de récurrence ***démontre*** que C ( 1)n X n d' où donc le résultat est vrai au rang n 1

**46326**: L' hypothèse de récurrence démontre que C ( 1)n X n d' où donc le résultat ***est*** vrai au rang n 1

**46340**: Par principe de récurrence , le résultat ***est*** vrai pour tout N

**46372**: Si le polynôme caractéristique u de u ***est*** scindé : où les k sont distincts , on appelle espace caractéristique associé à la valeur propre k : Soit E un K - espace vectoriel de dimension finie non nulle et u L ( E ) tel que u soit scindé : où les k sont distincts 1

**46378**: Si le polynôme caractéristique u de u est scindé : où les k ***sont*** distincts , on appelle espace caractéristique associé à la valeur propre k : Soit E un K - espace vectoriel de dimension finie non nulle et u L ( E ) tel que u soit scindé : où les k sont distincts 1

**46382**: Si le polynôme caractéristique u de u est scindé : où les k sont distincts , on ***appelle*** espace caractéristique associé à la valeur propre k : Soit E un K - espace vectoriel de dimension finie non nulle et u L ( E ) tel que u soit scindé : où les k sont distincts 1

**46419**: Si le polynôme caractéristique u de u est scindé : où les k sont distincts , on appelle espace caractéristique associé à la valeur propre k : Soit E un K - espace vectoriel de dimension finie non nulle et u L ( E ) tel que u soit scindé : où les k ***sont*** distincts 1

**46434**: pour tout k 1 , p , Fu ( k ) ***est*** stable par u 4

**46464**: C' ***est*** une conséquence immédiate du lemme des noyaux car les ( X k ) k sont premiers deux à deux et du fait que Ker u ( u ) E ( théorème de Cayley - Hamilton )

**46479**: C' est une conséquence immédiate du lemme des noyaux car les ( X k ) k ***sont*** premiers deux à deux et du fait que Ker u ( u ) E ( théorème de Cayley - Hamilton )

**46509**: Soit k 1 , p. ***Notons*** uk la restriction de u à Fu ( k )

**46522**: On ***a*** donc uk k

**46544**: idFu ( k ) ***est*** nilpotent

**46550**: Son polynôme caractéristique ***est*** ( X)dim Fu ( k ) ( propriété 5.9 , page précédente )

**46567**: Mais uk ***divise*** u , donc dim Fu ( k ) i

**46584**: D' après le point 2 on ***a*** : d' où dim Fu ( k ) k

**46598**: Alors les Qk ***sont*** premiers deux à deux , donc d' après l' identité de Bézout , il existe des polynômes Posons , pour tout k 1 , p , Pk Uk Qk et k Pk ( u )

**46613**: Alors les Qk sont premiers deux à deux , donc d' après l' identité de Bézout , il ***existe*** des polynômes Posons , pour tout k 1 , p , Pk Uk Qk et k Pk ( u )

**46636**: On ***a*** : Pour tout ( i , j ) 1 , p , i 6 j , Qi Qj divise u donc ( Qi Qj ) ( u ) 0L ( E ) ( théorème de CayleyHamilton )

**46655**: On a : Pour tout ( i , j ) 1 , p , i 6 j , Qi Qj ***divise*** u donc ( Qi Qj ) ( u ) 0L ( E ) ( théorème de CayleyHamilton )

**46676**: On ***a*** donc : donc i est un projecteur

**46681**: On a donc : donc i ***est*** un projecteur

**46686**: On ***conclut*** en démontrant que Im i Fu ( i ) et Ker i ( en exercice )

**46706**: Si u ***est*** diagonalisable , alors pour tout k 1 , p , Fu ( k ) Eu ( k )

**46735**: En particulier , les projecteurs sur les espaces propres ***sont*** des polynômes en u. Il est toujours possible de se ramener au cas où u est scindé en se plaçant dans le corps des racines de u ( si K R , on se place dans K C )

**46741**: En particulier , les projecteurs sur les espaces propres sont des polynômes en u. Il ***est*** toujours possible de se ramener au cas où u est scindé en se plaçant dans le corps des racines de u ( si K R , on se place dans K C )

**46751**: En particulier , les projecteurs sur les espaces propres sont des polynômes en u. Il est toujours possible de se ramener au cas où u ***est*** scindé en se plaçant dans le corps des racines de u ( si K R , on se place dans K C )

**46770**: En particulier , les projecteurs sur les espaces propres sont des polynômes en u. Il est toujours possible de se ramener au cas où u est scindé en se plaçant dans le corps des racines de u ( si K R , on se ***place*** dans K C )

**46779**: Le point 2 ***démontre*** que si u est scindé , alors dans une base E de E adaptée à la décomposition E Fu ( 1 ) Fu ( p ) , la matrice de u est diagonale par blocs : Théorème 5.5 Décomposition de Dunford Soit E un K - espace vectoriel de dimension finie non nulle et u L ( E ) tel que u soit scindé

**46783**: Le point 2 démontre que si u ***est*** scindé , alors dans une base E de E adaptée à la décomposition E Fu ( 1 ) Fu ( p ) , la matrice de u est diagonale par blocs : Théorème 5.5 Décomposition de Dunford Soit E un K - espace vectoriel de dimension finie non nulle et u L ( E ) tel que u soit scindé

**46811**: Le point 2 démontre que si u est scindé , alors dans une base E de E adaptée à la décomposition E Fu ( 1 ) Fu ( p ) , la matrice de u ***est*** diagonale par blocs : Théorème 5.5 Décomposition de Dunford Soit E un K - espace vectoriel de dimension finie non nulle et u L ( E ) tel que u soit scindé

**46846**: Il ***existe*** un unique couple ( d , n ) L ( E)2 tel que : et qui commutent ( d n n d )

**46863**: Il existe un unique couple ( d , n ) L ( E)2 tel que : et qui ***commutent*** ( d n n d )

**46879**: avec d diagonalisable et n nilpotent Démonstration On ***reprend*** les notations de la propriété 5.10 , page 245

**46899**: D' après le point 2 , il ***suffit*** de définir d et n sur chacun des Fu ( k ) : Par construction d est diagonalisable

**46916**: D' après le point 2 , il suffit de définir d et n sur chacun des Fu ( k ) : Par construction d ***est*** diagonalisable

**46947**: Pour tout k 1 , p , ( nFu ( k ) ) k 0L ( Fu ( k ) donc n De plus , n et d ***commutent*** sur chaque Fu ( k ) , donc sur E. Unicité

**46966**: On ***peut*** démontrer que d et n sont des polynômes en u , puisque ( en reprenant les notations de la En particulier , le couple ( d , n ) est unique

**46972**: On peut démontrer que d et n ***sont*** des polynômes en u , puisque ( en reprenant les notations de la En particulier , le couple ( d , n ) est unique

**46996**: On peut démontrer que d et n sont des polynômes en u , puisque ( en reprenant les notations de la En particulier , le couple ( d , n ) ***est*** unique

**47005**: Si ( d0 , n0 ) ***est*** un autre couple convenant , alors d0 et n0 ( qui sont des polynômes en u ) commutent avec u donc avec d et n. Ainsi , d et d0 sont co-diagonalisables ( voir le théorème 4.3 , page 205 du chapitre 4 ) donc d d0 n n0 est diagonalisable

**47017**: Si ( d0 , n0 ) est un autre couple convenant , alors d0 et n0 ( qui ***sont*** des polynômes en u ) commutent avec u donc avec d et n. Ainsi , d et d0 sont co-diagonalisables ( voir le théorème 4.3 , page 205 du chapitre 4 ) donc d d0 n n0 est diagonalisable

**47023**: Si ( d0 , n0 ) est un autre couple convenant , alors d0 et n0 ( qui sont des polynômes en u ) ***commutent*** avec u donc avec d et n. Ainsi , d et d0 sont co-diagonalisables ( voir le théorème 4.3 , page 205 du chapitre 4 ) donc d d0 n n0 est diagonalisable

**47036**: Si ( d0 , n0 ) est un autre couple convenant , alors d0 et n0 ( qui sont des polynômes en u ) commutent avec u donc avec d et n. Ainsi , d et d0 ***sont*** co-diagonalisables ( voir le théorème 4.3 , page 205 du chapitre 4 ) donc d d0 n n0 est diagonalisable

**47055**: Si ( d0 , n0 ) est un autre couple convenant , alors d0 et n0 ( qui sont des polynômes en u ) commutent avec u donc avec d et n. Ainsi , d et d0 sont co-diagonalisables ( voir le théorème 4.3 , page 205 du chapitre 4 ) donc d d0 n n0 ***est*** diagonalisable

**47068**: Or le seul endomorphisme à la fois nilpotent et diagonalisable ***est*** 0L ( E ) , d' où l' unicité

**47092**: La décomposition de Dunford : u 7 ( d , n ) ne ***est*** pas continue ( K C )

**47094**: La décomposition de Dunford : u 7 ( d , n ) ne est pas ***continue*** ( K C )

**47104**: En effet , on ***a*** ( u ) ( u , 0L ( E ) ) pour tout u L ( E ) diagonalisable donc si était continue , par densité des endomorphismes diagonalisables ( proposition 5.2 , page 238 ) , on aurait ( u ) ( u , 0L ( E ) ) pour tout u L ( E ) , c' est - à - dire que tout endomorphisme serait diagonalisable

**47127**: En effet , on a ( u ) ( u , 0L ( E ) ) pour tout u L ( E ) diagonalisable donc si était ***continue*** , par densité des endomorphismes diagonalisables ( proposition 5.2 , page 238 ) , on aurait ( u ) ( u , 0L ( E ) ) pour tout u L ( E ) , c' est - à - dire que tout endomorphisme serait diagonalisable

**47180**: La décomposition de Dunford ne ***est*** donc pas un outil adapté au calcul numérique approché

**47194**: Attention : ne ***est*** pas la décomposition de Dunford de A car A est diagonalisable ( elle a deux valeurs propres distinctes 1 et 2 )

**47204**: Attention : ne est pas la décomposition de Dunford de A car A ***est*** diagonalisable ( elle a deux valeurs propres distinctes 1 et 2 )

**47208**: Attention : ne est pas la décomposition de Dunford de A car A est diagonalisable ( elle ***a*** deux valeurs propres distinctes 1 et 2 )

**47222**: Sa décomposition de Dunford ***est*** elle-même : A A 02

**47230**: On ***peut*** se servir de la décomposition de Dunford pour calculer les puissances An ou l' exponentielle exp(A ) d' une matrice carrée A. Soit A D N avec D diagonalisable et N nilpotent la décomposition de Dunford de A. On note q l' indice de nilpotence de N ( le plus petit entier q N tel que N q 0n )

**47270**: On peut se servir de la décomposition de Dunford pour calculer les puissances An ou l' exponentielle exp(A ) d' une matrice carrée A. Soit A D N avec D diagonalisable et N nilpotent la décomposition de Dunford de A. On ***note*** q l' indice de nilpotence de N ( le plus petit entier q N tel que N q 0n )

**47296**: Puisque D et N ***commutent*** , on a d' après la formule du binôme de Newton : Puisque D et N commutent , on a : exp(A ) exp(D ) exp(N ) On a déjà vu comment calculer l' exponentielle d' une matrice diagonalisable

**47299**: Puisque D et N commutent , on ***a*** d' après la formule du binôme de Newton : Puisque D et N commutent , on a : exp(A ) exp(D ) exp(N ) On a déjà vu comment calculer l' exponentielle d' une matrice diagonalisable

**47313**: Puisque D et N commutent , on a d' après la formule du binôme de Newton : Puisque D et N ***commutent*** , on a : exp(A ) exp(D ) exp(N ) On a déjà vu comment calculer l' exponentielle d' une matrice diagonalisable

**47316**: Puisque D et N commutent , on a d' après la formule du binôme de Newton : Puisque D et N commutent , on ***a*** : exp(A ) exp(D ) exp(N ) On a déjà vu comment calculer l' exponentielle d' une matrice diagonalisable

**47341**: Pour N , on ***a*** simplement : En pratique : 1

**47350**: on ***prend*** une base E de E adaptée à la décomposition de E en espaces caractéristiques , pour obtenir la matrice A de u dans cette base sous la forme d' une matrice diagonale par blocs : 2

**47406**: on trigonalise Bk de manière à la mettre sous la forme : Bk k .Ink Nk où Nk ***est*** triangulaire supérieure nilpotente

**47415**: I Mais ces méthodes ***sont*** fastidieuses ( et peu adaptées au calcul numérique approchée , voir la remarque 5.15 , page précédente )

**47436**: On ***préfère*** donc passer par la trigonalisation de A et résoudre des systèmes triangulaires

**47473**: Le commutant de u ***est*** défini par : 1

**47483**: C ( u ) ***est*** un sous-espace vectoriel de L(E )

**47515**: Les sous-espaces propres de u ***sont*** stables par les endomorphismes de C ( u )

**47552**: Alors : C ( u ) ***est*** isomorphe à En particulier : u a n valeurs propres distinctes si , et seulement si , Ku C ( u ) si , et seulement si , dim C ( u ) n Démonstration Considérons l' application linéaire de C ( u ) dans pk1 L Eu ( k ) définie par : On vérifie alors que est bien définie et bijective ( car les espaces propres Eu ( k ) sont stables par les éléments de C ( u ) et que E Eu ( 1 ) Eu ( p ) ) d' où le résultat

**47559**: Alors : C ( u ) est isomorphe à En particulier : u ***a*** n valeurs propres distinctes si , et seulement si , Ku C ( u ) si , et seulement si , dim C ( u ) n Démonstration Considérons l' application linéaire de C ( u ) dans pk1 L Eu ( k ) définie par : On vérifie alors que est bien définie et bijective ( car les espaces propres Eu ( k ) sont stables par les éléments de C ( u ) et que E Eu ( 1 ) Eu ( p ) ) d' où le résultat

**47608**: Alors : C ( u ) est isomorphe à En particulier : u a n valeurs propres distinctes si , et seulement si , Ku C ( u ) si , et seulement si , dim C ( u ) n Démonstration Considérons l' application linéaire de C ( u ) dans pk1 L Eu ( k ) définie par : On ***vérifie*** alors que est bien définie et bijective ( car les espaces propres Eu ( k ) sont stables par les éléments de C ( u ) et que E Eu ( 1 ) Eu ( p ) ) d' où le résultat

**47611**: Alors : C ( u ) est isomorphe à En particulier : u a n valeurs propres distinctes si , et seulement si , Ku C ( u ) si , et seulement si , dim C ( u ) n Démonstration Considérons l' application linéaire de C ( u ) dans pk1 L Eu ( k ) définie par : On vérifie alors que ***est*** bien définie et bijective ( car les espaces propres Eu ( k ) sont stables par les éléments de C ( u ) et que E Eu ( 1 ) Eu ( p ) ) d' où le résultat

**47625**: Alors : C ( u ) est isomorphe à En particulier : u a n valeurs propres distinctes si , et seulement si , Ku C ( u ) si , et seulement si , dim C ( u ) n Démonstration Considérons l' application linéaire de C ( u ) dans pk1 L Eu ( k ) définie par : On vérifie alors que est bien définie et bijective ( car les espaces propres Eu ( k ) ***sont*** stables par les éléments de C ( u ) et que E Eu ( 1 ) Eu ( p ) ) d' où le résultat

**47656**: En particulier , on ***a*** : dim Ku p dim Eu ( k ) Alors u a n valeurs propres distinctes si , et seulement si , dim Ku n si , et seulement si , pour tout k 1 , p , dim Eu ( k ) 1 ( donc l' inégalité ci - dessus est une égalité ) , d' où le résultat ( on a déjà l' inclusion Ku C ( u ) )

**47668**: En particulier , on a : dim Ku p dim Eu ( k ) Alors u ***a*** n valeurs propres distinctes si , et seulement si , dim Ku n si , et seulement si , pour tout k 1 , p , dim Eu ( k ) 1 ( donc l' inégalité ci - dessus est une égalité ) , d' où le résultat ( on a déjà l' inclusion Ku C ( u ) )

**47708**: En particulier , on a : dim Ku p dim Eu ( k ) Alors u a n valeurs propres distinctes si , et seulement si , dim Ku n si , et seulement si , pour tout k 1 , p , dim Eu ( k ) 1 ( donc l' inégalité ci - dessus ***est*** une égalité ) , d' où le résultat ( on a déjà l' inclusion Ku C ( u ) )

**47719**: En particulier , on a : dim Ku p dim Eu ( k ) Alors u a n valeurs propres distinctes si , et seulement si , dim Ku n si , et seulement si , pour tout k 1 , p , dim Eu ( k ) 1 ( donc l' inégalité ci - dessus est une égalité ) , d' où le résultat ( on ***a*** déjà l' inclusion Ku C ( u ) )

**47732**: Si on ***veut*** déterminer C ( u ) , on peut utiliser la décomposition de Dunford : u d n , d diagonalisable , n nilpotent , et qui commutent ce qui revient à trouver C ( d ) ( on utilise ce qui précède ) et C ( n )

**47740**: Si on veut déterminer C ( u ) , on ***peut*** utiliser la décomposition de Dunford : u d n , d diagonalisable , n nilpotent , et qui commutent ce qui revient à trouver C ( d ) ( on utilise ce qui précède ) et C ( n )

**47759**: Si on veut déterminer C ( u ) , on peut utiliser la décomposition de Dunford : u d n , d diagonalisable , n nilpotent , et qui ***commutent*** ce qui revient à trouver C ( d ) ( on utilise ce qui précède ) et C ( n )

**47762**: Si on veut déterminer C ( u ) , on peut utiliser la décomposition de Dunford : u d n , d diagonalisable , n nilpotent , et qui commutent ce qui ***revient*** à trouver C ( d ) ( on utilise ce qui précède ) et C ( n )

**47771**: Si on veut déterminer C ( u ) , on peut utiliser la décomposition de Dunford : u d n , d diagonalisable , n nilpotent , et qui commutent ce qui revient à trouver C ( d ) ( on ***utilise*** ce qui précède ) et C ( n )

**47774**: Si on veut déterminer C ( u ) , on peut utiliser la décomposition de Dunford : u d n , d diagonalisable , n nilpotent , et qui commutent ce qui revient à trouver C ( d ) ( on utilise ce qui ***précède*** ) et C ( n )

**47783**: On ***doit*** donc maintenant étudier ce qui se passe pour un endomorphisme nilpotent

**47790**: On doit donc maintenant étudier ce qui se ***passe*** pour un endomorphisme nilpotent

**47828**: Alors il ***existe*** une base E de E pour laquelle : où les blocs Bk , appelés blocs de Jordan , sont soit nuls , soit de la forme : Démonstration On passe par les invariants de similitude ( voir la dernière partie ) : si ( P1 ,

**47847**: Alors il existe une base E de E pour laquelle : où les blocs Bk , appelés blocs de Jordan , ***sont*** soit nuls , soit de la forme : Démonstration On passe par les invariants de similitude ( voir la dernière partie ) : si ( P1 ,

**47858**: Alors il existe une base E de E pour laquelle : où les blocs Bk , appelés blocs de Jordan , sont soit nuls , soit de la forme : Démonstration On ***passe*** par les invariants de similitude ( voir la dernière partie ) : si ( P1 ,

**47881**: , Pr ) ***est*** la suite des invariants de similitude de u alors le produit P1 Pr est égal au signe près à u , qui vaut ( 1)n X n ( propriété 5.9 , page 245 )

**47895**: , Pr ) est la suite des invariants de similitude de u alors le produit P1 Pr ***est*** égal au signe près à u , qui vaut ( 1)n X n ( propriété 5.9 , page 245 )

**47904**: , Pr ) est la suite des invariants de similitude de u alors le produit P1 Pr est égal au signe près à u , qui ***vaut*** ( 1)n X n ( propriété 5.9 , page 245 )

**47919**: Les Pi ***sont*** donc de la forme X ni

**47929**: On en ***déduit*** que les matrices compagnons C(Pi ) des Pi sont des transposées de blocs de Jordan

**47938**: On en déduit que les matrices compagnons C(Pi ) des Pi ***sont*** des transposées de blocs de Jordan

**47947**: On ***conclut*** alors en appliquant la réduction de Frobenius à u. On peut aussi le démontrer directement ( voir l' exercice 5.3.11 , page 251 )

**47958**: On conclut alors en appliquant la réduction de Frobenius à u. On ***peut*** aussi le démontrer directement ( voir l' exercice 5.3.11 , page 251 )

**48008**: Alors il ***existe*** une base E de E pour laquelle : où les blocs Bk , appelés blocs de Jordan , sont soit diagonales , soit de la forme : Démonstration On reprend les notations de la définition 5.8 , page 245

**48027**: Alors il existe une base E de E pour laquelle : où les blocs Bk , appelés blocs de Jordan , ***sont*** soit diagonales , soit de la forme : Démonstration On reprend les notations de la définition 5.8 , page 245

**48038**: Alors il existe une base E de E pour laquelle : où les blocs Bk , appelés blocs de Jordan , sont soit diagonales , soit de la forme : Démonstration On ***reprend*** les notations de la définition 5.8 , page 245

**48059**: Sur chaque espace caractéristique Fu ( k ) , on ***décompose*** uFu ( k ) en la somme d' un endomorphisme nilpotent et d' un endomorphisme diagonalisable

**48078**: On ***utilise*** alors la réduction de Jordan pour les endomorphismes nilpotents et le fait que E est la somme directe des espaces caractéristiques de u. 1

**48093**: On utilise alors la réduction de Jordan pour les endomorphismes nilpotents et le fait que E ***est*** la somme directe des espaces caractéristiques de u. 1

**48108**: Lorsque les valeurs propres ***sont*** séparées , tout se passe bien : 2

**48113**: Lorsque les valeurs propres sont séparées , tout se ***passe*** bien : 2

**48122**: Lorsque les valeurs propres ***sont*** confondues , il y a de mauvaises surprises , ainsi : 5.3.1 Trouver le commutant de la matrice : 5.3.2 Soit E un C - espace vectoriel de dimension finie n 1 et soit u L(E )

**48127**: Lorsque les valeurs propres sont confondues , il y ***a*** de mauvaises surprises , ainsi : 5.3.1 Trouver le commutant de la matrice : 5.3.2 Soit E un C - espace vectoriel de dimension finie n 1 et soit u L(E )

**48164**: Démontrer qu' il ***existe*** des entiers n1 ,

**48211**: fn des endomorphismes nilpotents de E qui ***commutent*** deux à deux

**48261**: ( b ) Démontrer que l' ensemble des endomorphismes qui ***commutent*** avec f est un sous-espace vectoriel de L ( E ) de base 5.3.5 Soit E un K - espace vectoriel de dimension 2 et u L ( E ) qui ne est pas une homothétie

**48264**: ( b ) Démontrer que l' ensemble des endomorphismes qui commutent avec f ***est*** un sous-espace vectoriel de L ( E ) de base 5.3.5 Soit E un K - espace vectoriel de dimension 2 et u L ( E ) qui ne est pas une homothétie

**48294**: ( b ) Démontrer que l' ensemble des endomorphismes qui commutent avec f est un sous-espace vectoriel de L ( E ) de base 5.3.5 Soit E un K - espace vectoriel de dimension 2 et u L ( E ) qui ne ***est*** pas une homothétie

**48309**: Déterminer la dimension du commutant de u. 5.3.6 Soit ( ***a*** , b , c ) K3

**48335**: Démontrer qu' elle ***commute*** avec sa transposée si , et seulement si , elle est diagonale

**48346**: Démontrer qu' elle commute avec sa transposée si , et seulement si , elle ***est*** diagonale

**48368**: ( ***a*** ) Démontrer que est de dimension au moins n. ( b ) Démontrer que si A Mn ( C ) alors son commutant est de dimension au moins n. ( c ) Que dire d' une matrice A de Mn ( C ) dont le commutant est égal à l' ensemble CA ? 5.3.9 Pour quels entiers n le groupe GLn ( R ) admet -il un sous-groupe isomorphe à Z4 Z ? ( a ) Démontrer que A est semblable à : ( b ) Trouver les matrices réelles qui commutent avec A. 5.3.11 Soit E un K - espace vectoriel et u L ( E ) un endomorphisme nilpotent de E. Soit q l' indice de nilpotence de u : Soit x E tel que : ( a ) Démontrer que la famille x , u(x ) ,

**48372**: ( a ) Démontrer que ***est*** de dimension au moins n. ( b ) Démontrer que si A Mn ( C ) alors son commutant est de dimension au moins n. ( c ) Que dire d' une matrice A de Mn ( C ) dont le commutant est égal à l' ensemble CA ? 5.3.9 Pour quels entiers n le groupe GLn ( R ) admet -il un sous-groupe isomorphe à Z4 Z ? ( a ) Démontrer que A est semblable à : ( b ) Trouver les matrices réelles qui commutent avec A. 5.3.11 Soit E un K - espace vectoriel et u L ( E ) un endomorphisme nilpotent de E. Soit q l' indice de nilpotence de u : Soit x E tel que : ( a ) Démontrer que la famille x , u(x ) ,

**48392**: ( a ) Démontrer que est de dimension au moins n. ( b ) Démontrer que si A Mn ( C ) alors son commutant ***est*** de dimension au moins n. ( c ) Que dire d' une matrice A de Mn ( C ) dont le commutant est égal à l' ensemble CA ? 5.3.9 Pour quels entiers n le groupe GLn ( R ) admet -il un sous-groupe isomorphe à Z4 Z ? ( a ) Démontrer que A est semblable à : ( b ) Trouver les matrices réelles qui commutent avec A. 5.3.11 Soit E un K - espace vectoriel et u L ( E ) un endomorphisme nilpotent de E. Soit q l' indice de nilpotence de u : Soit x E tel que : ( a ) Démontrer que la famille x , u(x ) ,

**48415**: ( a ) Démontrer que est de dimension au moins n. ( b ) Démontrer que si A Mn ( C ) alors son commutant est de dimension au moins n. ( c ) Que dire d' une matrice A de Mn ( C ) dont le commutant ***est*** égal à l' ensemble CA ? 5.3.9 Pour quels entiers n le groupe GLn ( R ) admet -il un sous-groupe isomorphe à Z4 Z ? ( a ) Démontrer que A est semblable à : ( b ) Trouver les matrices réelles qui commutent avec A. 5.3.11 Soit E un K - espace vectoriel et u L ( E ) un endomorphisme nilpotent de E. Soit q l' indice de nilpotence de u : Soit x E tel que : ( a ) Démontrer que la famille x , u(x ) ,

**48433**: ( a ) Démontrer que est de dimension au moins n. ( b ) Démontrer que si A Mn ( C ) alors son commutant est de dimension au moins n. ( c ) Que dire d' une matrice A de Mn ( C ) dont le commutant est égal à l' ensemble CA ? 5.3.9 Pour quels entiers n le groupe GLn ( R ) ***admet*** -il un sous-groupe isomorphe à Z4 Z ? ( a ) Démontrer que A est semblable à : ( b ) Trouver les matrices réelles qui commutent avec A. 5.3.11 Soit E un K - espace vectoriel et u L ( E ) un endomorphisme nilpotent de E. Soit q l' indice de nilpotence de u : Soit x E tel que : ( a ) Démontrer que la famille x , u(x ) ,

**48443**: ( a ) Démontrer que est de dimension au moins n. ( b ) Démontrer que si A Mn ( C ) alors son commutant est de dimension au moins n. ( c ) Que dire d' une matrice A de Mn ( C ) dont le commutant est égal à l' ensemble CA ? 5.3.9 Pour quels entiers n le groupe GLn ( R ) admet -il un sous-groupe isomorphe à Z4 Z ? ( ***a*** ) Démontrer que A est semblable à : ( b ) Trouver les matrices réelles qui commutent avec A. 5.3.11 Soit E un K - espace vectoriel et u L ( E ) un endomorphisme nilpotent de E. Soit q l' indice de nilpotence de u : Soit x E tel que : ( a ) Démontrer que la famille x , u(x ) ,

**48448**: ( a ) Démontrer que est de dimension au moins n. ( b ) Démontrer que si A Mn ( C ) alors son commutant est de dimension au moins n. ( c ) Que dire d' une matrice A de Mn ( C ) dont le commutant est égal à l' ensemble CA ? 5.3.9 Pour quels entiers n le groupe GLn ( R ) admet -il un sous-groupe isomorphe à Z4 Z ? ( a ) Démontrer que A ***est*** semblable à : ( b ) Trouver les matrices réelles qui commutent avec A. 5.3.11 Soit E un K - espace vectoriel et u L ( E ) un endomorphisme nilpotent de E. Soit q l' indice de nilpotence de u : Soit x E tel que : ( a ) Démontrer que la famille x , u(x ) ,

**48460**: ( a ) Démontrer que est de dimension au moins n. ( b ) Démontrer que si A Mn ( C ) alors son commutant est de dimension au moins n. ( c ) Que dire d' une matrice A de Mn ( C ) dont le commutant est égal à l' ensemble CA ? 5.3.9 Pour quels entiers n le groupe GLn ( R ) admet -il un sous-groupe isomorphe à Z4 Z ? ( a ) Démontrer que A est semblable à : ( b ) Trouver les matrices réelles qui ***commutent*** avec A. 5.3.11 Soit E un K - espace vectoriel et u L ( E ) un endomorphisme nilpotent de E. Soit q l' indice de nilpotence de u : Soit x E tel que : ( a ) Démontrer que la famille x , u(x ) ,

**48498**: ( a ) Démontrer que est de dimension au moins n. ( b ) Démontrer que si A Mn ( C ) alors son commutant est de dimension au moins n. ( c ) Que dire d' une matrice A de Mn ( C ) dont le commutant est égal à l' ensemble CA ? 5.3.9 Pour quels entiers n le groupe GLn ( R ) admet -il un sous-groupe isomorphe à Z4 Z ? ( a ) Démontrer que A est semblable à : ( b ) Trouver les matrices réelles qui commutent avec A. 5.3.11 Soit E un K - espace vectoriel et u L ( E ) un endomorphisme nilpotent de E. Soit q l' indice de nilpotence de u : Soit x E tel que : ( ***a*** ) Démontrer que la famille x , u(x ) ,

**48517**: , uq1 ( x ) ***est*** libre

**48528**: ( b ) En déduire que si E ***est*** de dimension finie alors dim E q. I On pose E1 Vect x , u(x ) ,

**48538**: ( b ) En déduire que si E est de dimension finie alors dim E q. I On ***pose*** E1 Vect x , u(x ) ,

**48556**: , uq1 ( x ) et on ***suppose*** que E est de dimension finie n p. I Par ailleurs , soit E ? une forme linéaire telle que : On pose alors : I Finalement , on pose : ( c ) Justifier l' existence de

**48559**: , uq1 ( x ) et on suppose que E ***est*** de dimension finie n p. I Par ailleurs , soit E ? une forme linéaire telle que : On pose alors : I Finalement , on pose : ( c ) Justifier l' existence de

**48579**: , uq1 ( x ) et on suppose que E est de dimension finie n p. I Par ailleurs , soit E ? une forme linéaire telle que : On ***pose*** alors : I Finalement , on pose : ( c ) Justifier l' existence de

**48586**: , uq1 ( x ) et on suppose que E est de dimension finie n p. I Par ailleurs , soit E ? une forme linéaire telle que : On pose alors : I Finalement , on ***pose*** : ( c ) Justifier l' existence de

**48600**: ( d ) Quelle ***est*** la dimension de ? ( Justifier )

**48613**: ( e ) Quelle ***est*** la dimension de G ? ( Justifier )

**48651**: ( f ) Démontrer que : ( g ) Démontrer que : ( h ) En déduire la proposition 5.3 , page 249 Résolution d' équations matricielles Nous ***allons*** nous intéresser à deux types d' équations : 1

**48676**: Extraction de racine : soit A Mn ( K ) et p N , ***existe*** -t -il des matrices B telles que B p A ? Et si c' est le cas , comment les trouver toutes ? 2

**48691**: Extraction de racine : soit A Mn ( K ) et p N , existe -t -il des matrices B telles que B p A ? Et si c' ***est*** le cas , comment les trouver toutes ? 2

**48717**: Logarithme : soit A Mn ( K ) ( K R ou C ) , ***existe*** -t -il des matrices B telles que B p A ? Et si c' est le cas , comment les trouver toutes ? L' extraction de racine possède deux propriétés immédiates : 1

**48732**: Logarithme : soit A Mn ( K ) ( K R ou C ) , existe -t -il des matrices B telles que B p A ? Et si c' ***est*** le cas , comment les trouver toutes ? L' extraction de racine possède deux propriétés immédiates : 1

**48745**: Logarithme : soit A Mn ( K ) ( K R ou C ) , existe -t -il des matrices B telles que B p A ? Et si c' est le cas , comment les trouver toutes ? L' extraction de racine ***possède*** deux propriétés immédiates : 1

**48754**: Si B ***existe*** , alors det(B)p det(A ) donc une condition nécessaire d' existence est que det(A ) admette une racine p - ième dans le corps K. 2

**48766**: Si B existe , alors det(B)p det(A ) donc une condition nécessaire d' existence ***est*** que det(A ) admette une racine p - ième dans le corps K. 2

**48784**: Si B ***existe*** , alors B commute avec A ( A B B A ) , donc B est nécessairement un élément du commutant C ( A ) de A. C' est facile lorsque A est diagonalisable

**48788**: Si B existe , alors B ***commute*** avec A ( A B B A ) , donc B est nécessairement un élément du commutant C ( A ) de A. C' est facile lorsque A est diagonalisable

**48800**: Si B existe , alors B commute avec A ( A B B A ) , donc B ***est*** nécessairement un élément du commutant C ( A ) de A. C' est facile lorsque A est diagonalisable

**48813**: Si B existe , alors B commute avec A ( A B B A ) , donc B est nécessairement un élément du commutant C ( A ) de A. C' ***est*** facile lorsque A est diagonalisable

**48817**: Si B existe , alors B commute avec A ( A B B A ) , donc B est nécessairement un élément du commutant C ( A ) de A. C' est facile lorsque A ***est*** diagonalisable

**48821**: On ***est*** alors ramené à la résolution de B p .In , ce qui est aisée

**48834**: On est alors ramené à la résolution de B p .In , ce qui ***est*** aisée

**48840**: Lorsque A ne ***est*** plus diagonalisable , il ne y a plus de méthode générale

**48847**: Lorsque A ne est plus diagonalisable , il ne y ***a*** plus de méthode générale

**48857**: Soit la matrice : ***résolvons*** B A , pour p 2 , 3 , d' inconnue B M3 ( R )

**48878**: On ***a*** det(A ) 2 , donc il ne y a pas de solution pour p 2 ( car on est dans le cas K R , mais il en existe dans C )

**48887**: On a det(A ) 2 , donc il ne y ***a*** pas de solution pour p 2 ( car on est dans le cas K R , mais il en existe dans C )

**48897**: On a det(A ) 2 , donc il ne y a pas de solution pour p 2 ( car on ***est*** dans le cas K R , mais il en existe dans C )

**48907**: On a det(A ) 2 , donc il ne y a pas de solution pour p 2 ( car on est dans le cas K R , mais il en ***existe*** dans C )

**48919**: Pour p 3 , on ***peut*** diagonaliser A : il existe P GL3 ( R ) telle que : Les matrices qui commutent avec cette dernière matrice sont de la forme Diag(B1 ) avec B1 M2 ( R )

**48924**: Pour p 3 , on peut diagonaliser A : il ***existe*** P GL3 ( R ) telle que : Les matrices qui commutent avec cette dernière matrice sont de la forme Diag(B1 ) avec B1 M2 ( R )

**48936**: Pour p 3 , on peut diagonaliser A : il existe P GL3 ( R ) telle que : Les matrices qui ***commutent*** avec cette dernière matrice sont de la forme Diag(B1 ) avec B1 M2 ( R )

**48941**: Pour p 3 , on peut diagonaliser A : il existe P GL3 ( R ) telle que : Les matrices qui commutent avec cette dernière matrice ***sont*** de la forme Diag(B1 ) avec B1 M2 ( R )

**48955**: On ***doit*** donc résoudre B13 I2 et 3 2

**48966**: On en ***déduit*** que B1 est diagonalisable dans C , égale à I2 ou semblable à Diag(j , j 2 ) avec j exp(2 i 3 ) , ce qui ramené dans R nous donne : Finalement , les B qui conviennent vérifient : Le calcul brutal ne est pas efficace ! Même si l' on pense à montrer que B C ( A )

**48969**: On en déduit que B1 ***est*** diagonalisable dans C , égale à I2 ou semblable à Diag(j , j 2 ) avec j exp(2 i 3 ) , ce qui ramené dans R nous donne : Finalement , les B qui conviennent vérifient : Le calcul brutal ne est pas efficace ! Même si l' on pense à montrer que B C ( A )

**48998**: On en déduit que B1 est diagonalisable dans C , égale à I2 ou semblable à Diag(j , j 2 ) avec j exp(2 i 3 ) , ce qui ramené dans R nous ***donne*** : Finalement , les B qui conviennent vérifient : Le calcul brutal ne est pas efficace ! Même si l' on pense à montrer que B C ( A )

**49005**: On en déduit que B1 est diagonalisable dans C , égale à I2 ou semblable à Diag(j , j 2 ) avec j exp(2 i 3 ) , ce qui ramené dans R nous donne : Finalement , les B qui ***conviennent*** vérifient : Le calcul brutal ne est pas efficace ! Même si l' on pense à montrer que B C ( A )

**49006**: On en déduit que B1 est diagonalisable dans C , égale à I2 ou semblable à Diag(j , j 2 ) avec j exp(2 i 3 ) , ce qui ramené dans R nous donne : Finalement , les B qui conviennent ***vérifient*** : Le calcul brutal ne est pas efficace ! Même si l' on pense à montrer que B C ( A )

**49012**: On en déduit que B1 est diagonalisable dans C , égale à I2 ou semblable à Diag(j , j 2 ) avec j exp(2 i 3 ) , ce qui ramené dans R nous donne : Finalement , les B qui conviennent vérifient : Le calcul brutal ne ***est*** pas efficace ! Même si l' on pense à montrer que B C ( A )

**49020**: On en déduit que B1 est diagonalisable dans C , égale à I2 ou semblable à Diag(j , j 2 ) avec j exp(2 i 3 ) , ce qui ramené dans R nous donne : Finalement , les B qui conviennent vérifient : Le calcul brutal ne est pas efficace ! Même si l' on ***pense*** à montrer que B C ( A )

**49034**: Si la matrice ne ***est*** plus diagonalisable ( mais est inversible ) , on considère la décomposition de Dunford de A : A D N avec D diagonalisable et N nilpotente , et on cherche B également sous la forme B D0 N 0 avec D0 diagonalisable et N 0 nilpotente

**49039**: Si la matrice ne est plus diagonalisable ( mais ***est*** inversible ) , on considère la décomposition de Dunford de A : A D N avec D diagonalisable et N nilpotente , et on cherche B également sous la forme B D0 N 0 avec D0 diagonalisable et N 0 nilpotente

**49044**: Si la matrice ne est plus diagonalisable ( mais est inversible ) , on ***considère*** la décomposition de Dunford de A : A D N avec D diagonalisable et N nilpotente , et on cherche B également sous la forme B D0 N 0 avec D0 diagonalisable et N 0 nilpotente

**49064**: Si la matrice ne est plus diagonalisable ( mais est inversible ) , on considère la décomposition de Dunford de A : A D N avec D diagonalisable et N nilpotente , et on ***cherche*** B également sous la forme B D0 N 0 avec D0 diagonalisable et N 0 nilpotente

**49083**: On ***obtient*** : Il vient alors : que l' on peut résoudre en utilisant le développement en série entière de x 7 ( 1 x)1p et le fait que la matrice D0 N 0 est nilpotente

**49086**: On obtient : Il ***vient*** alors : que l' on peut résoudre en utilisant le développement en série entière de x 7 ( 1 x)1p et le fait que la matrice D0 N 0 est nilpotente

**49092**: On obtient : Il vient alors : que l' on ***peut*** résoudre en utilisant le développement en série entière de x 7 ( 1 x)1p et le fait que la matrice D0 N 0 est nilpotente

**49116**: On obtient : Il vient alors : que l' on peut résoudre en utilisant le développement en série entière de x 7 ( 1 x)1p et le fait que la matrice D0 N 0 ***est*** nilpotente

**49125**: Soit A la matrice : Elle ***est*** de déterminant 2 0

**49136**: Donc , par exemple : ***vérifie*** exp(B ) A ( ce ne est pas la seule car plusieurs matrices B 0 vérifient exp(B 0 ) I2 )

**49143**: Donc , par exemple : vérifie exp(B ) A ( ce ne ***est*** pas la seule car plusieurs matrices B 0 vérifient exp(B 0 ) I2 )

**49152**: Donc , par exemple : vérifie exp(B ) A ( ce ne est pas la seule car plusieurs matrices B 0 ***vérifient*** exp(B 0 ) I2 )

**49165**: Si exp(B ) A et si ***considère*** les décompositions de Dunford A D N et B D0 N 0 alors exp(D0 ) exp(N 0 ) D N donc exp(D0 ) D et exp(N 0 ) In D1 N

**49207**: En utilisant la série entière du logarithme , on ***a*** donc : où p est l' indice de nilpotence de N , c' est - à - dire le plus petit entier p N tel que N p 0n

**49212**: En utilisant la série entière du logarithme , on a donc : où p ***est*** l' indice de nilpotence de N , c' est - à - dire le plus petit entier p N tel que N p 0n

**49245**: Avec Wxmaxima : Soit n N , ***existe*** -t -il h L ( RX ) tel que hn T ? Si oui , déterminer h. 5.4.2 Quelles sont les A GL2 ( R ) telles qu' il existe X GL2 ( R ) telle que A X 3 ? 5.4.3 Démontrer que exp Mn ( C ) GLn ( C ) 5.4.4 ( a ) Démontrer que si A Mn ( C ) vérifie exp(A ) In , alors A est diagonalisable

**49265**: Avec Wxmaxima : Soit n N , existe -t -il h L ( RX ) tel que hn T ? Si oui , déterminer h. 5.4.2 Quelles ***sont*** les A GL2 ( R ) telles qu' il existe X GL2 ( R ) telle que A X 3 ? 5.4.3 Démontrer que exp Mn ( C ) GLn ( C ) 5.4.4 ( a ) Démontrer que si A Mn ( C ) vérifie exp(A ) In , alors A est diagonalisable

**49275**: Avec Wxmaxima : Soit n N , existe -t -il h L ( RX ) tel que hn T ? Si oui , déterminer h. 5.4.2 Quelles sont les A GL2 ( R ) telles qu' il ***existe*** X GL2 ( R ) telle que A X 3 ? 5.4.3 Démontrer que exp Mn ( C ) GLn ( C ) 5.4.4 ( a ) Démontrer que si A Mn ( C ) vérifie exp(A ) In , alors A est diagonalisable

**49301**: Avec Wxmaxima : Soit n N , existe -t -il h L ( RX ) tel que hn T ? Si oui , déterminer h. 5.4.2 Quelles sont les A GL2 ( R ) telles qu' il existe X GL2 ( R ) telle que A X 3 ? 5.4.3 Démontrer que exp Mn ( C ) GLn ( C ) 5.4.4 ( ***a*** ) Démontrer que si A Mn ( C ) vérifie exp(A ) In , alors A est diagonalisable

**49311**: Avec Wxmaxima : Soit n N , existe -t -il h L ( RX ) tel que hn T ? Si oui , déterminer h. 5.4.2 Quelles sont les A GL2 ( R ) telles qu' il existe X GL2 ( R ) telle que A X 3 ? 5.4.3 Démontrer que exp Mn ( C ) GLn ( C ) 5.4.4 ( a ) Démontrer que si A Mn ( C ) ***vérifie*** exp(A ) In , alors A est diagonalisable

**49318**: Avec Wxmaxima : Soit n N , existe -t -il h L ( RX ) tel que hn T ? Si oui , déterminer h. 5.4.2 Quelles sont les A GL2 ( R ) telles qu' il existe X GL2 ( R ) telle que A X 3 ? 5.4.3 Démontrer que exp Mn ( C ) GLn ( C ) 5.4.4 ( a ) Démontrer que si A Mn ( C ) vérifie exp(A ) In , alors A ***est*** diagonalisable

**49428**: Trouver toutes les solutions dans M3 ( R ) de l' équation : Invariants de similitudes Quand on ***a*** une relation d' équivalence , il est naturel d' essayer de caractériser ses classes d' équivalence

**49435**: Trouver toutes les solutions dans M3 ( R ) de l' équation : Invariants de similitudes Quand on a une relation d' équivalence , il ***est*** naturel d' essayer de caractériser ses classes d' équivalence

**49448**: Nous nous ***intéressons*** ici à la relation de similitude

**49467**: Soit ( A , B ) Mn ( K)2 tel que ***A*** B. Alors : 1

**49491**: rang(A ) rang(B ) , trace(A ) trace(B ) et det(A ) det(B ) Malheureusement , ce ne ***sont*** que des conditions nécessaires

**49499**: Les matrices ***ont*** le même rang ( 4 ) , la même trace ( 0 ) , le même déterminant ( 0 ) , le même polynôme caractéristique ( ( X)7 ) , le même polynôme minimal ( X 3 ) , le même spectre ( 0 ) , la même dimension des espaces propres ( 3 ) et la même dimension des espaces caractéristiques ( 7 )

**49569**: Mais elles ne ***sont*** pas semblables car A2 et B 2 ne ont pas même rang ! Cependant , si A et B sont diagonalisables , alors : 1

**49578**: Mais elles ne sont pas semblables car A2 et B 2 ne ***ont*** pas même rang ! Cependant , si A et B sont diagonalisables , alors : 1

**49589**: Mais elles ne sont pas semblables car A2 et B 2 ne ont pas même rang ! Cependant , si A et B ***sont*** diagonalisables , alors : 1

**49606**: Si K C et n 2 les classes de similitude ***sont*** : ( a ) les homothéties de rapport C ( avec ( X ) 2 et X ) ( c ) les matrices non diagonalisables semblables à 2

**49609**: Si K C et n 2 les classes de similitude sont : ( ***a*** ) les homothéties de rapport C ( avec ( X ) 2 et X ) ( c ) les matrices non diagonalisables semblables à 2

**49647**: Si K C et n 3 , les classes de similitude ***sont*** : ( a ) les homothéties ( avec ( X ) 3 et X ) ( b ) les matrices semblables à ( c ) les matrices semblables à ( d ) les matrices semblables à ( e ) les matrices semblables à ( f ) les matrices semblables à La méthode générale pour montrer que A B est de ramener A et B à la même forme réduite : diagonale , Dunford , Frobenius ( voir plus loin ) , etc. Cela revient souvent à résoudre des systèmes linéaires

**49650**: Si K C et n 3 , les classes de similitude sont : ( ***a*** ) les homothéties ( avec ( X ) 3 et X ) ( b ) les matrices semblables à ( c ) les matrices semblables à ( d ) les matrices semblables à ( e ) les matrices semblables à ( f ) les matrices semblables à La méthode générale pour montrer que A B est de ramener A et B à la même forme réduite : diagonale , Dunford , Frobenius ( voir plus loin ) , etc. Cela revient souvent à résoudre des systèmes linéaires

**49704**: Si K C et n 3 , les classes de similitude sont : ( a ) les homothéties ( avec ( X ) 3 et X ) ( b ) les matrices semblables à ( c ) les matrices semblables à ( d ) les matrices semblables à ( e ) les matrices semblables à ( f ) les matrices semblables à La méthode générale pour montrer que ***A*** B est de ramener A et B à la même forme réduite : diagonale , Dunford , Frobenius ( voir plus loin ) , etc. Cela revient souvent à résoudre des systèmes linéaires

**49706**: Si K C et n 3 , les classes de similitude sont : ( a ) les homothéties ( avec ( X ) 3 et X ) ( b ) les matrices semblables à ( c ) les matrices semblables à ( d ) les matrices semblables à ( e ) les matrices semblables à ( f ) les matrices semblables à La méthode générale pour montrer que A B ***est*** de ramener A et B à la même forme réduite : diagonale , Dunford , Frobenius ( voir plus loin ) , etc. Cela revient souvent à résoudre des systèmes linéaires

**49731**: Si K C et n 3 , les classes de similitude sont : ( a ) les homothéties ( avec ( X ) 3 et X ) ( b ) les matrices semblables à ( c ) les matrices semblables à ( d ) les matrices semblables à ( e ) les matrices semblables à ( f ) les matrices semblables à La méthode générale pour montrer que A B est de ramener A et B à la même forme réduite : diagonale , Dunford , Frobenius ( voir plus loin ) , etc. Cela ***revient*** souvent à résoudre des systèmes linéaires

**49742**: Les matrices suivantes ***sont*** semblables : Soit E un K - espace vectoriel de dimension finie non nulle , u L ( E ) et x E. On définit : Ix l' idéal des polynômes annulateurs x , c' est - à - dire le noyau de : KX E définie par ( P ) x le polynôme minimal de x ( le polynôme unitaire engendrant l' idéal Ix )

**49767**: Les matrices suivantes sont semblables : Soit E un K - espace vectoriel de dimension finie non nulle , u L ( E ) et x E. On ***définit*** : Ix l' idéal des polynômes annulateurs x , c' est - à - dire le noyau de : KX E définie par ( P ) x le polynôme minimal de x ( le polynôme unitaire engendrant l' idéal Ix )

**49816**: Le sous-espace vectoriel hxi de E ***est*** stable par u et : dim(hxi ) deg x par factorisation de

**49870**: , uq1 ( x ) ) ***est*** la matrice compagnon de x ( voir la définition 5.5 , page 232 ) : Il existe x E tel que x u ( voir l' exercice 4 de la série d' exercices 5.1 , page 236 )

**49887**: , uq1 ( x ) ) est la matrice compagnon de x ( voir la définition 5.5 , page 232 ) : Il ***existe*** x E tel que x u ( voir l' exercice 4 de la série d' exercices 5.1 , page 236 )

**49936**: Alors il ***existe*** r N et hxi i et , pour tout i 1 , r 1 , xi xi1 et xr u Démonstration Soit x E tel que x u ( voir la remarque précédente )

**49994**: En notant k deg u , une base de hxi ***est*** donnée par : On complète en une base ( e1 ,

**50024**: , e?n ) ***est*** la base duale associée , posons : Alors G est un sous-espace vectoriel de E stable par u. Démontrons que E hxi G. ce qui contredit y G. On a donc y 0E d' où hx G 0E

**50034**: , e?n ) est la base duale associée , posons : Alors G ***est*** un sous-espace vectoriel de E stable par u. Démontrons que E hxi G. ce qui contredit y G. On a donc y 0E d' où hx G 0E

**50043**: , e?n ) est la base duale associée , posons : Alors G est un sous-espace vectoriel de E stable par u. ***Démontrons*** que E hxi G. ce qui contredit y G. On a donc y 0E d' où hx G 0E

**50050**: , e?n ) est la base duale associée , posons : Alors G est un sous-espace vectoriel de E stable par u. Démontrons que E hxi G. ce qui ***contredit*** y G. On a donc y 0E d' où hx G 0E

**50054**: , e?n ) est la base duale associée , posons : Alors G est un sous-espace vectoriel de E stable par u. Démontrons que E hxi G. ce qui contredit y G. On ***a*** donc y 0E d' où hx G 0E

**50079**: Par dualité , pour montrer que dim G n dim(hxi ) n k , il ***suffit*** de montrer que Considérons l' application de Ku dans Vect qui a P ( u ) Ku associe e?k P ( u )

**50083**: Par dualité , pour montrer que dim G n dim(hxi ) n k , il suffit de montrer que ***Considérons*** l' application de Ku dans Vect qui a P ( u ) Ku associe e?k P ( u )

**50091**: Par dualité , pour montrer que dim G n dim(hxi ) n k , il suffit de montrer que Considérons l' application de Ku dans Vect qui ***a*** P ( u ) Ku associe e?k P ( u )

**50097**: Par dualité , pour montrer que dim G n dim(hxi ) n k , il suffit de montrer que Considérons l' application de Ku dans Vect qui a P ( u ) Ku ***associe*** e?k P ( u )

**50108**: Par définition de , ***est*** surjective

**50112**: Elle ***est*** de plus injective car si e?k P ( u ) 0L ( E ) avec P ( u ) Ku 0L ( E ) , ce qui est une contradiction , donc P ( u ) 0L ( E ) donc est injective

**50140**: Elle est de plus injective car si e?k P ( u ) 0L ( E ) avec P ( u ) Ku 0L ( E ) , ce qui ***est*** une contradiction , donc P ( u ) 0L ( E ) donc est injective

**50154**: Elle est de plus injective car si e?k P ( u ) 0L ( E ) avec P ( u ) Ku 0L ( E ) , ce qui est une contradiction , donc P ( u ) 0L ( E ) donc ***est*** injective

**50164**: Finalement , Ku et Vect ( ) ***sont*** isomorphes donc la même dimension : dim Vect ( ) dim Ku deg u k. On a donc bien E hxi G avec G stable par u. Le polynôme minimal de uhxi est x u donc divise le polynôme minimal de uG On recommence alors la démonstration avec G à la place de E et uG G à la place de u. En un nombre fini r d' étapes , on obtient la décomposition voulue

**50181**: Finalement , Ku et Vect ( ) sont isomorphes donc la même dimension : dim Vect ( ) dim Ku deg u k. On ***a*** donc bien E hxi G avec G stable par u. Le polynôme minimal de uhxi est x u donc divise le polynôme minimal de uG On recommence alors la démonstration avec G à la place de E et uG G à la place de u. En un nombre fini r d' étapes , on obtient la décomposition voulue

**50197**: Finalement , Ku et Vect ( ) sont isomorphes donc la même dimension : dim Vect ( ) dim Ku deg u k. On a donc bien E hxi G avec G stable par u. Le polynôme minimal de uhxi ***est*** x u donc divise le polynôme minimal de uG On recommence alors la démonstration avec G à la place de E et uG G à la place de u. En un nombre fini r d' étapes , on obtient la décomposition voulue

**50201**: Finalement , Ku et Vect ( ) sont isomorphes donc la même dimension : dim Vect ( ) dim Ku deg u k. On a donc bien E hxi G avec G stable par u. Le polynôme minimal de uhxi est x u donc ***divise*** le polynôme minimal de uG On recommence alors la démonstration avec G à la place de E et uG G à la place de u. En un nombre fini r d' étapes , on obtient la décomposition voulue

**50208**: Finalement , Ku et Vect ( ) sont isomorphes donc la même dimension : dim Vect ( ) dim Ku deg u k. On a donc bien E hxi G avec G stable par u. Le polynôme minimal de uhxi est x u donc divise le polynôme minimal de uG On ***recommence*** alors la démonstration avec G à la place de E et uG G à la place de u. En un nombre fini r d' étapes , on obtient la décomposition voulue

**50236**: Finalement , Ku et Vect ( ) sont isomorphes donc la même dimension : dim Vect ( ) dim Ku deg u k. On a donc bien E hxi G avec G stable par u. Le polynôme minimal de uhxi est x u donc divise le polynôme minimal de uG On recommence alors la démonstration avec G à la place de E et uG G à la place de u. En un nombre fini r d' étapes , on ***obtient*** la décomposition voulue

**50243**: Cette décomposition ***est*** unique , dans le sens où si ( y1 ,

**50271**: , 0E ) ***vérifie*** : hyi i et , pour tout i 1 , r 1 , yi yi1 et ys u alors s r et pour tout i 1 , s , yi xi

**50352**: , Pr où , pour tout i 1 , r , Pi xi donné par le théorème 5.7 , page précédente , ***sont*** appelées les invariants de similitude de u. La matrice de u dans la base E de E adaptée à la décomposition de E du théorème 5.7 , page précédente : est appelée forme de Frobenius

**50383**: , Pr où , pour tout i 1 , r , Pi xi donné par le théorème 5.7 , page précédente , sont appelées les invariants de similitude de u. La matrice de u dans la base E de E adaptée à la décomposition de E du théorème 5.7 , page précédente : ***est*** appelée forme de Frobenius

**50406**: Théorème 5.8 Caractérisation des classes de similitudes Deux matrices A et B de Mn ( K ) ***sont*** semblables si , et seulement si , elles sont les mêmes invariants de similitude

**50415**: Théorème 5.8 Caractérisation des classes de similitudes Deux matrices A et B de Mn ( K ) sont semblables si , et seulement si , elles ***sont*** les mêmes invariants de similitude

**50424**: Démonstration C' ***est*** une application directe du théorème de Frobenius ( théorème 5.7 , page précédente )

**50453**: Les théorèmes 5.7 , page précédente et 5.8 , de la présente page ***permettent*** de montrer de nombreux résultats théoriques , par exemple : la réduction de Jordan des endomorphismes nilpotents ( voir la proposition 5.3 , page 249 ) le fait que toute matrice est semblable à sa transposée si deux matrices A et B de Mn ( K ) sont semblables en tant que matrices de Mn ( L ) , où L est un surcorps de K , alors elles sont semblables sur Mn ( K ) ( en effet , par unicité , les invariants de similitude dans Mn ( K ) sont les invariants de similitudes dans Mn ( L ) )

**50485**: Les théorèmes 5.7 , page précédente et 5.8 , de la présente page permettent de montrer de nombreux résultats théoriques , par exemple : la réduction de Jordan des endomorphismes nilpotents ( voir la proposition 5.3 , page 249 ) le fait que toute matrice ***est*** semblable à sa transposée si deux matrices A et B de Mn ( K ) sont semblables en tant que matrices de Mn ( L ) , où L est un surcorps de K , alors elles sont semblables sur Mn ( K ) ( en effet , par unicité , les invariants de similitude dans Mn ( K ) sont les invariants de similitudes dans Mn ( L ) )

**50501**: Les théorèmes 5.7 , page précédente et 5.8 , de la présente page permettent de montrer de nombreux résultats théoriques , par exemple : la réduction de Jordan des endomorphismes nilpotents ( voir la proposition 5.3 , page 249 ) le fait que toute matrice est semblable à sa transposée si deux matrices A et B de Mn ( K ) ***sont*** semblables en tant que matrices de Mn ( L ) , où L est un surcorps de K , alors elles sont semblables sur Mn ( K ) ( en effet , par unicité , les invariants de similitude dans Mn ( K ) sont les invariants de similitudes dans Mn ( L ) )

**50515**: Les théorèmes 5.7 , page précédente et 5.8 , de la présente page permettent de montrer de nombreux résultats théoriques , par exemple : la réduction de Jordan des endomorphismes nilpotents ( voir la proposition 5.3 , page 249 ) le fait que toute matrice est semblable à sa transposée si deux matrices A et B de Mn ( K ) sont semblables en tant que matrices de Mn ( L ) , où L ***est*** un surcorps de K , alors elles sont semblables sur Mn ( K ) ( en effet , par unicité , les invariants de similitude dans Mn ( K ) sont les invariants de similitudes dans Mn ( L ) )

**50523**: Les théorèmes 5.7 , page précédente et 5.8 , de la présente page permettent de montrer de nombreux résultats théoriques , par exemple : la réduction de Jordan des endomorphismes nilpotents ( voir la proposition 5.3 , page 249 ) le fait que toute matrice est semblable à sa transposée si deux matrices A et B de Mn ( K ) sont semblables en tant que matrices de Mn ( L ) , où L est un surcorps de K , alors elles ***sont*** semblables sur Mn ( K ) ( en effet , par unicité , les invariants de similitude dans Mn ( K ) sont les invariants de similitudes dans Mn ( L ) )

**50546**: Les théorèmes 5.7 , page précédente et 5.8 , de la présente page permettent de montrer de nombreux résultats théoriques , par exemple : la réduction de Jordan des endomorphismes nilpotents ( voir la proposition 5.3 , page 249 ) le fait que toute matrice est semblable à sa transposée si deux matrices A et B de Mn ( K ) sont semblables en tant que matrices de Mn ( L ) , où L est un surcorps de K , alors elles sont semblables sur Mn ( K ) ( en effet , par unicité , les invariants de similitude dans Mn ( K ) ***sont*** les invariants de similitudes dans Mn ( L ) )

**50561**: Si C(P ) ***est*** une matrice compagnon , sa forme de Jordan ne fait apparaître qu' un et un seul bloc pour chaque valeur propre ( est une racine de P et la taille du bloc est son ordre de multiplicité )

**50571**: Si C(P ) est une matrice compagnon , sa forme de Jordan ne ***fait*** apparaître qu' un et un seul bloc pour chaque valeur propre ( est une racine de P et la taille du bloc est son ordre de multiplicité )

**50584**: Si C(P ) est une matrice compagnon , sa forme de Jordan ne fait apparaître qu' un et un seul bloc pour chaque valeur propre ( ***est*** une racine de P et la taille du bloc est son ordre de multiplicité )

**50594**: Si C(P ) est une matrice compagnon , sa forme de Jordan ne fait apparaître qu' un et un seul bloc pour chaque valeur propre ( est une racine de P et la taille du bloc ***est*** son ordre de multiplicité )

**50602**: On ***peut*** ainsi obtenir la forme de Jordan d' une matrice à partir de sa forme de Frobenius

**50621**: On ***peut*** montrer que si une matrice A Mn ( K a pour invariants de similitude P1 ,

**50631**: On peut montrer que si une matrice A Mn ( K ***a*** pour invariants de similitude P1 ,

**50647**: , Pr , alors : On ***parle*** alors de forme de Smith

**50655**: On ***a*** un algorithme simple ( à base de pivots , voir plus bas ) qui permet de calculer cette forme et ainsi trouver les invariants de similitude de A et sa forme de Frobenius

**50670**: On a un algorithme simple ( à base de pivots , voir plus bas ) qui ***permet*** de calculer cette forme et ainsi trouver les invariants de similitude de A et sa forme de Frobenius

**50691**: Cela ***permet*** notamment de répondre à la question : Les matrices A et B sont - elles semblables ? De plus , dans cet algorithme , il ne y a pas besoin de factoriser les polynômes , cela fonctionne dans ne importe quel corps

**50704**: Cela permet notamment de répondre à la question : Les matrices A et B ***sont*** - elles semblables ? De plus , dans cet algorithme , il ne y a pas besoin de factoriser les polynômes , cela fonctionne dans ne importe quel corps

**50719**: Cela permet notamment de répondre à la question : Les matrices A et B sont - elles semblables ? De plus , dans cet algorithme , il ne y ***a*** pas besoin de factoriser les polynômes , cela fonctionne dans ne importe quel corps

**50728**: Cela permet notamment de répondre à la question : Les matrices A et B sont - elles semblables ? De plus , dans cet algorithme , il ne y a pas besoin de factoriser les polynômes , cela ***fonctionne*** dans ne importe quel corps

**50731**: Cela permet notamment de répondre à la question : Les matrices A et B sont - elles semblables ? De plus , dans cet algorithme , il ne y a pas besoin de factoriser les polynômes , cela fonctionne dans ne ***importe*** quel corps

**50742**: Les matrices P et Q du théorème ***sont*** dans GLn ( KX ) , leurs déterminants sont donc inversibles dans KX , ces déterminants sont donc des polynômes constants non nuls ! On note M A X.In mi , j ( i , j)1,n2 et Étape 1 Par permutations de lignes et de colonnes , on se ramène au cas où deg m1,1 ( M )

**50751**: Les matrices P et Q du théorème sont dans GLn ( KX ) , leurs déterminants ***sont*** donc inversibles dans KX , ces déterminants sont donc des polynômes constants non nuls ! On note M A X.In mi , j ( i , j)1,n2 et Étape 1 Par permutations de lignes et de colonnes , on se ramène au cas où deg m1,1 ( M )

**50759**: Les matrices P et Q du théorème sont dans GLn ( KX ) , leurs déterminants sont donc inversibles dans KX , ces déterminants ***sont*** donc des polynômes constants non nuls ! On note M A X.In mi , j ( i , j)1,n2 et Étape 1 Par permutations de lignes et de colonnes , on se ramène au cas où deg m1,1 ( M )

**50792**: Les matrices P et Q du théorème sont dans GLn ( KX ) , leurs déterminants sont donc inversibles dans KX , ces déterminants sont donc des polynômes constants non nuls ! On note M A X.In mi , j ( i , j)1,n2 et Étape 1 Par permutations de lignes et de colonnes , on se ***ramène*** au cas où deg m1,1 ( M )

**50806**: Étape 2 Si il ***existe*** sur la première ligne ( ou la première colonne ) un élément m1,k ( ou mk,1 ) non multiple de m1,1 , on le remplace ( en utilisant une transvection ) par le reste de la division euclidienne de m1,k ( ou mk,1 ) par m1,1 et on revient à l' étape 1 en permutant les colonnes ( ou les lignes ) 1 et k. Etape 3 L' étape 2 a pour effet de diminuer strictement ( M ) , elle ne peut donc se réaliser qu' un nombre fini de fois

**50855**: Étape 2 Si il existe sur la première ligne ( ou la première colonne ) un élément m1,k ( ou mk,1 ) non multiple de m1,1 , on le remplace ( en utilisant une transvection ) par le reste de la division euclidienne de m1,k ( ou mk,1 ) par m1,1 et on ***revient*** à l' étape 1 en permutant les colonnes ( ou les lignes ) 1 et k. Etape 3 L' étape 2 a pour effet de diminuer strictement ( M ) , elle ne peut donc se réaliser qu' un nombre fini de fois

**50877**: Étape 2 Si il existe sur la première ligne ( ou la première colonne ) un élément m1,k ( ou mk,1 ) non multiple de m1,1 , on le remplace ( en utilisant une transvection ) par le reste de la division euclidienne de m1,k ( ou mk,1 ) par m1,1 et on revient à l' étape 1 en permutant les colonnes ( ou les lignes ) 1 et k. Etape 3 L' étape 2 ***a*** pour effet de diminuer strictement ( M ) , elle ne peut donc se réaliser qu' un nombre fini de fois

**50889**: Étape 2 Si il existe sur la première ligne ( ou la première colonne ) un élément m1,k ( ou mk,1 ) non multiple de m1,1 , on le remplace ( en utilisant une transvection ) par le reste de la division euclidienne de m1,k ( ou mk,1 ) par m1,1 et on revient à l' étape 1 en permutant les colonnes ( ou les lignes ) 1 et k. Etape 3 L' étape 2 a pour effet de diminuer strictement ( M ) , elle ne ***peut*** donc se réaliser qu' un nombre fini de fois

**50904**: Ainsi , on se ***ramène*** au cas où tous les éléments de la première ligne et de la première colonne sont multiples de m1,1

**50920**: Ainsi , on se ramène au cas où tous les éléments de la première ligne et de la première colonne ***sont*** multiples de m1,1

**50926**: On ***utilise*** alors m1,1 comme pivot pour les annuler

**50938**: Étape 4 On ***obtient*** une matrice de la forme et , si m1,1 ne divise pas un des éléments de M1 , par exemple mi , j , on fait apparaître ( par des transvections ) le reste de la division euclidienne de mi , j par m1,1 puis par des permutations on l' échange avec m1,1 et on recommence l' étape 1

**50949**: Étape 4 On obtient une matrice de la forme et , si m1,1 ne ***divise*** pas un des éléments de M1 , par exemple mi , j , on fait apparaître ( par des transvections ) le reste de la division euclidienne de mi , j par m1,1 puis par des permutations on l' échange avec m1,1 et on recommence l' étape 1

**50964**: Étape 4 On obtient une matrice de la forme et , si m1,1 ne divise pas un des éléments de M1 , par exemple mi , j , on ***fait*** apparaître ( par des transvections ) le reste de la division euclidienne de mi , j par m1,1 puis par des permutations on l' échange avec m1,1 et on recommence l' étape 1

**50994**: Étape 4 On obtient une matrice de la forme et , si m1,1 ne divise pas un des éléments de M1 , par exemple mi , j , on fait apparaître ( par des transvections ) le reste de la division euclidienne de mi , j par m1,1 puis par des permutations on l' échange avec m1,1 et on ***recommence*** l' étape 1

**51011**: Étape 5 Au bout d' un nombre fini d' étapes , on ***obtient*** une matrice de la forme où m1,1 divise tous les éléments de M1 et on recommence l' étape 1 à partir de la matrice M1 qui est de taille strictement plus petite

**51019**: Étape 5 Au bout d' un nombre fini d' étapes , on obtient une matrice de la forme où m1,1 ***divise*** tous les éléments de M1 et on recommence l' étape 1 à partir de la matrice M1 qui est de taille strictement plus petite

**51027**: Étape 5 Au bout d' un nombre fini d' étapes , on obtient une matrice de la forme où m1,1 divise tous les éléments de M1 et on ***recommence*** l' étape 1 à partir de la matrice M1 qui est de taille strictement plus petite

**51038**: Étape 5 Au bout d' un nombre fini d' étapes , on obtient une matrice de la forme où m1,1 divise tous les éléments de M1 et on recommence l' étape 1 à partir de la matrice M1 qui ***est*** de taille strictement plus petite

**51050**: La forme de Smith correspondante ***est*** : donc les invariants de similitude de A sont ( A ( X 2)3 ) ( un seul élément )

**51059**: La forme de Smith correspondante est : donc les invariants de similitude de A ***sont*** ( A ( X 2)3 ) ( un seul élément )

**51074**: On en ***déduit*** que sa forme de Frobenius est : La forme de Smith correspondante est donc les invariants de similitude de A sont ( X 1 , A ( X 1)2 )

**51080**: On en déduit que sa forme de Frobenius ***est*** : La forme de Smith correspondante est donc les invariants de similitude de A sont ( X 1 , A ( X 1)2 )

**51087**: On en déduit que sa forme de Frobenius est : La forme de Smith correspondante ***est*** donc les invariants de similitude de A sont ( X 1 , A ( X 1)2 )

**51095**: On en déduit que sa forme de Frobenius est : La forme de Smith correspondante est donc les invariants de similitude de A ***sont*** ( X 1 , A ( X 1)2 )

**51108**: On en ***déduit*** que sa forme de Frobenius 5.5.1 Déterminer la forme de Frobenius de la matrice : 5.5.2 Démontrer que : sont semblables

**51128**: On en déduit que sa forme de Frobenius 5.5.1 Déterminer la forme de Frobenius de la matrice : 5.5.2 Démontrer que : ***sont*** semblables

**51147**: 5.5.3 Démontrer que si n 2 , 3 , deux matrices de Mn ( K ) ***sont*** semblables si , et seulement si , elles ont le même polynôme caractéristique et le même polynôme minimal

**51156**: 5.5.3 Démontrer que si n 2 , 3 , deux matrices de Mn ( K ) sont semblables si , et seulement si , elles ***ont*** le même polynôme caractéristique et le même polynôme minimal

**51171**: Démontrer que ce résultat ***est*** faux 5.5.5 Démontrer que : est un ouvert dense de Mn ( C )

**51177**: Démontrer que ce résultat est faux 5.5.5 Démontrer que : ***est*** un ouvert dense de Mn ( C )

**51194**: 5.5.6 Démontrer que l' adhérence de : ***est*** l' ensemble des matrices A telles que Sp(A ) U z C , z 1

**51212**: ( ***a*** ) On suppose que P est scindé dans K , à quelle condition C(P ) est-elle diagonalisable ? ( b ) On suppose que P est irréductible , montrer qu' en ce cas KC(P ) est un corps P admet au moins une racine ( laquelle ? )

**51215**: ( a ) On ***suppose*** que P est scindé dans K , à quelle condition C(P ) est-elle diagonalisable ? ( b ) On suppose que P est irréductible , montrer qu' en ce cas KC(P ) est un corps P admet au moins une racine ( laquelle ? )

**51218**: ( a ) On suppose que P ***est*** scindé dans K , à quelle condition C(P ) est-elle diagonalisable ? ( b ) On suppose que P est irréductible , montrer qu' en ce cas KC(P ) est un corps P admet au moins une racine ( laquelle ? )

**51235**: ( a ) On suppose que P est scindé dans K , à quelle condition C(P ) est-elle diagonalisable ? ( b ) On ***suppose*** que P est irréductible , montrer qu' en ce cas KC(P ) est un corps P admet au moins une racine ( laquelle ? )

**51238**: ( a ) On suppose que P est scindé dans K , à quelle condition C(P ) est-elle diagonalisable ? ( b ) On suppose que P ***est*** irréductible , montrer qu' en ce cas KC(P ) est un corps P admet au moins une racine ( laquelle ? )

**51248**: ( a ) On suppose que P est scindé dans K , à quelle condition C(P ) est-elle diagonalisable ? ( b ) On suppose que P est irréductible , montrer qu' en ce cas KC(P ) ***est*** un corps P admet au moins une racine ( laquelle ? )

**51252**: ( a ) On suppose que P est scindé dans K , à quelle condition C(P ) est-elle diagonalisable ? ( b ) On suppose que P est irréductible , montrer qu' en ce cas KC(P ) est un corps P ***admet*** au moins une racine ( laquelle ? )

**51347**: Cet exercice a été posé à un oral de concours dans les années 70 ... Soit ( ***a*** , b , u0 , u1 ) Z2 , on construit successivement les suites récurrentes : Nous allons démontrer que 1

**51358**: Cet exercice a été posé à un oral de concours dans les années 70 ... Soit ( a , b , u0 , u1 ) Z2 , on ***construit*** successivement les suites récurrentes : Nous allons démontrer que 1

**51376**: La suite ( vn ) nN ***est*** périodique à partir d' un certain rang

**51390**: Et sa période ***est*** dans l' ensemble des diviseurs stricts de 120

**51412**: Démonstration de la périodicité La suite ( vn , vn1 ) nN ***est*** à valeurs dans 0 , 92 qui est fini

**51420**: Démonstration de la périodicité La suite ( vn , vn1 ) nN est à valeurs dans 0 , 92 qui ***est*** fini

**51424**: Il ***existe*** donc deux entiers distincts ( p , q ) N2 , p q tels que Il est alors facile de démontrer par récurrence sur n que : la suite est donc périodique à partir du rang p , et sa période est un diviseur de q p. Démonstration des valeurs possibles de la période On peut considérer qu' on travaille en fait dans l' anneau Z10 Z qui , par théorème chinois , est isomorphe à Z2 Z Z5 Z. Nous pouvons donc travailler dans des corps

**51441**: Il existe donc deux entiers distincts ( p , q ) N2 , p q tels que Il ***est*** alors facile de démontrer par récurrence sur n que : la suite est donc périodique à partir du rang p , et sa période est un diviseur de q p. Démonstration des valeurs possibles de la période On peut considérer qu' on travaille en fait dans l' anneau Z10 Z qui , par théorème chinois , est isomorphe à Z2 Z Z5 Z. Nous pouvons donc travailler dans des corps

**51454**: Il existe donc deux entiers distincts ( p , q ) N2 , p q tels que Il est alors facile de démontrer par récurrence sur n que : la suite ***est*** donc périodique à partir du rang p , et sa période est un diviseur de q p. Démonstration des valeurs possibles de la période On peut considérer qu' on travaille en fait dans l' anneau Z10 Z qui , par théorème chinois , est isomorphe à Z2 Z Z5 Z. Nous pouvons donc travailler dans des corps

**51466**: Il existe donc deux entiers distincts ( p , q ) N2 , p q tels que Il est alors facile de démontrer par récurrence sur n que : la suite est donc périodique à partir du rang p , et sa période ***est*** un diviseur de q p. Démonstration des valeurs possibles de la période On peut considérer qu' on travaille en fait dans l' anneau Z10 Z qui , par théorème chinois , est isomorphe à Z2 Z Z5 Z. Nous pouvons donc travailler dans des corps

**51480**: Il existe donc deux entiers distincts ( p , q ) N2 , p q tels que Il est alors facile de démontrer par récurrence sur n que : la suite est donc périodique à partir du rang p , et sa période est un diviseur de q p. Démonstration des valeurs possibles de la période On ***peut*** considérer qu' on travaille en fait dans l' anneau Z10 Z qui , par théorème chinois , est isomorphe à Z2 Z Z5 Z. Nous pouvons donc travailler dans des corps

**51484**: Il existe donc deux entiers distincts ( p , q ) N2 , p q tels que Il est alors facile de démontrer par récurrence sur n que : la suite est donc périodique à partir du rang p , et sa période est un diviseur de q p. Démonstration des valeurs possibles de la période On peut considérer qu' on ***travaille*** en fait dans l' anneau Z10 Z qui , par théorème chinois , est isomorphe à Z2 Z Z5 Z. Nous pouvons donc travailler dans des corps

**51498**: Il existe donc deux entiers distincts ( p , q ) N2 , p q tels que Il est alors facile de démontrer par récurrence sur n que : la suite est donc périodique à partir du rang p , et sa période est un diviseur de q p. Démonstration des valeurs possibles de la période On peut considérer qu' on travaille en fait dans l' anneau Z10 Z qui , par théorème chinois , ***est*** isomorphe à Z2 Z Z5 Z. Nous pouvons donc travailler dans des corps

**51506**: Il existe donc deux entiers distincts ( p , q ) N2 , p q tels que Il est alors facile de démontrer par récurrence sur n que : la suite est donc périodique à partir du rang p , et sa période est un diviseur de q p. Démonstration des valeurs possibles de la période On peut considérer qu' on travaille en fait dans l' anneau Z10 Z qui , par théorème chinois , est isomorphe à Z2 Z Z5 Z. Nous ***pouvons*** donc travailler dans des corps

**51526**: Dans Z2 Z La vectorialisation de la suite ( en notant ***a*** la classe de a dans Z2 Z ) donne On obtient alors que A est semblable à une matrice parmi ( Z2 Z ) impossible de période 1 ou 2 de période 1 si son polynôme caractéristique est scindé

**51530**: Dans Z2 Z La vectorialisation de la suite ( en notant a la classe de ***a*** dans Z2 Z ) donne On obtient alors que A est semblable à une matrice parmi ( Z2 Z ) impossible de période 1 ou 2 de période 1 si son polynôme caractéristique est scindé

**51535**: Dans Z2 Z La vectorialisation de la suite ( en notant a la classe de a dans Z2 Z ) ***donne*** On obtient alors que A est semblable à une matrice parmi ( Z2 Z ) impossible de période 1 ou 2 de période 1 si son polynôme caractéristique est scindé

**51537**: Dans Z2 Z La vectorialisation de la suite ( en notant a la classe de a dans Z2 Z ) donne On ***obtient*** alors que A est semblable à une matrice parmi ( Z2 Z ) impossible de période 1 ou 2 de période 1 si son polynôme caractéristique est scindé

**51541**: Dans Z2 Z La vectorialisation de la suite ( en notant a la classe de a dans Z2 Z ) donne On obtient alors que A ***est*** semblable à une matrice parmi ( Z2 Z ) impossible de période 1 ou 2 de période 1 si son polynôme caractéristique est scindé

**51564**: Dans Z2 Z La vectorialisation de la suite ( en notant a la classe de a dans Z2 Z ) donne On obtient alors que A est semblable à une matrice parmi ( Z2 Z ) impossible de période 1 ou 2 de période 1 si son polynôme caractéristique ***est*** scindé

**51572**: Lorsque son polynôme caractéristique ne ***est*** pas scindé , de degré 2 , il est alors irréductible ( c' est X 2 X 1 ) donc , en posant C(A ) , et en se plaçant dans M2 ( Z2 Z ) , les racines de A sont alors et I2 , distinctes

**51581**: Lorsque son polynôme caractéristique ne est pas scindé , de degré 2 , il ***est*** alors irréductible ( c' est X 2 X 1 ) donc , en posant C(A ) , et en se plaçant dans M2 ( Z2 Z ) , les racines de A sont alors et I2 , distinctes

**51586**: Lorsque son polynôme caractéristique ne est pas scindé , de degré 2 , il est alors irréductible ( c' ***est*** X 2 X 1 ) donc , en posant C(A ) , et en se plaçant dans M2 ( Z2 Z ) , les racines de A sont alors et I2 , distinctes

**51614**: Lorsque son polynôme caractéristique ne est pas scindé , de degré 2 , il est alors irréductible ( c' est X 2 X 1 ) donc , en posant C(A ) , et en se plaçant dans M2 ( Z2 Z ) , les racines de A ***sont*** alors et I2 , distinctes

**51622**: Donc ***A*** est semblable à Diag ( , I2 )

**51623**: Donc A ***est*** semblable à Diag ( , I2 )

**51635**: Or la période ***est*** de 3

**51651**: Dans Z2 Z les périodes de la suite ( un ) nN ***sont*** , à partir d' un certain rang 1 , 2 ou 3

**51679**: Dans Z5 Z ( Nous noterons toujours les classes sous la forme ***a*** ) Le même raisonnement nous conduit , lorsque le polynôme caractéristique est scindé aux matrices ( ( , ) Z5 Z2 ) impossible de période 1 , 5 ou 20 de période 1 , 2 ou 4 En effet , on sait d' après le petit théorème de Fermat que ce qui justifie la période 1 , 2 ou 4 pour la troisième forme de matrice

**51685**: Dans Z5 Z ( Nous noterons toujours les classes sous la forme a ) Le même raisonnement nous ***conduit*** , lorsque le polynôme caractéristique est scindé aux matrices ( ( , ) Z5 Z2 ) impossible de période 1 , 5 ou 20 de période 1 , 2 ou 4 En effet , on sait d' après le petit théorème de Fermat que ce qui justifie la période 1 , 2 ou 4 pour la troisième forme de matrice

**51691**: Dans Z5 Z ( Nous noterons toujours les classes sous la forme a ) Le même raisonnement nous conduit , lorsque le polynôme caractéristique ***est*** scindé aux matrices ( ( , ) Z5 Z2 ) impossible de période 1 , 5 ou 20 de période 1 , 2 ou 4 En effet , on sait d' après le petit théorème de Fermat que ce qui justifie la période 1 , 2 ou 4 pour la troisième forme de matrice

**51721**: Dans Z5 Z ( Nous noterons toujours les classes sous la forme a ) Le même raisonnement nous conduit , lorsque le polynôme caractéristique est scindé aux matrices ( ( , ) Z5 Z2 ) impossible de période 1 , 5 ou 20 de période 1 , 2 ou 4 En effet , on ***sait*** d' après le petit théorème de Fermat que ce qui justifie la période 1 , 2 ou 4 pour la troisième forme de matrice

**51732**: Dans Z5 Z ( Nous noterons toujours les classes sous la forme a ) Le même raisonnement nous conduit , lorsque le polynôme caractéristique est scindé aux matrices ( ( , ) Z5 Z2 ) impossible de période 1 , 5 ou 20 de période 1 , 2 ou 4 En effet , on sait d' après le petit théorème de Fermat que ce qui ***justifie*** la période 1 , 2 ou 4 pour la troisième forme de matrice

**51766**: De plus , si n N , à l' aide de la formule du binôme de Newton , on ***a*** si 6 0 ( cas évident où la période est 1 , à partir du second rang ) et 6 1 ( cas simple ou la périodicité est 5 , la première condition ci-dessous étant inutile ) , la matrice est égale à I2 si ce qui nous donne une période de 10 ou 20

**51776**: De plus , si n N , à l' aide de la formule du binôme de Newton , on a si 6 0 ( cas évident où la période ***est*** 1 , à partir du second rang ) et 6 1 ( cas simple ou la périodicité est 5 , la première condition ci-dessous étant inutile ) , la matrice est égale à I2 si ce qui nous donne une période de 10 ou 20

**51794**: De plus , si n N , à l' aide de la formule du binôme de Newton , on a si 6 0 ( cas évident où la période est 1 , à partir du second rang ) et 6 1 ( cas simple ou la périodicité ***est*** 5 , la première condition ci-dessous étant inutile ) , la matrice est égale à I2 si ce qui nous donne une période de 10 ou 20

**51807**: De plus , si n N , à l' aide de la formule du binôme de Newton , on a si 6 0 ( cas évident où la période est 1 , à partir du second rang ) et 6 1 ( cas simple ou la périodicité est 5 , la première condition ci-dessous étant inutile ) , la matrice ***est*** égale à I2 si ce qui nous donne une période de 10 ou 20

**51815**: De plus , si n N , à l' aide de la formule du binôme de Newton , on a si 6 0 ( cas évident où la période est 1 , à partir du second rang ) et 6 1 ( cas simple ou la périodicité est 5 , la première condition ci-dessous étant inutile ) , la matrice est égale à I2 si ce qui nous ***donne*** une période de 10 ou 20

**51832**: mod 5 et n Lorsque le polynôme caractéristique de ***A*** ne est plus scindé , il est irréductible ( car de degré 2 ) , en posant C(A ) , et en travaillant dans le corps K Z5 Z qui est de cardinal 25 , on a ( même démonstration que Fermat ) on a donc de période 3 , 6 , 8 , 12 ou 24

**51834**: mod 5 et n Lorsque le polynôme caractéristique de A ne ***est*** plus scindé , il est irréductible ( car de degré 2 ) , en posant C(A ) , et en travaillant dans le corps K Z5 Z qui est de cardinal 25 , on a ( même démonstration que Fermat ) on a donc de période 3 , 6 , 8 , 12 ou 24

**51839**: mod 5 et n Lorsque le polynôme caractéristique de A ne est plus scindé , il ***est*** irréductible ( car de degré 2 ) , en posant C(A ) , et en travaillant dans le corps K Z5 Z qui est de cardinal 25 , on a ( même démonstration que Fermat ) on a donc de période 3 , 6 , 8 , 12 ou 24

**51863**: mod 5 et n Lorsque le polynôme caractéristique de A ne est plus scindé , il est irréductible ( car de degré 2 ) , en posant C(A ) , et en travaillant dans le corps K Z5 Z qui ***est*** de cardinal 25 , on a ( même démonstration que Fermat ) on a donc de période 3 , 6 , 8 , 12 ou 24

**51869**: mod 5 et n Lorsque le polynôme caractéristique de A ne est plus scindé , il est irréductible ( car de degré 2 ) , en posant C(A ) , et en travaillant dans le corps K Z5 Z qui est de cardinal 25 , on ***a*** ( même démonstration que Fermat ) on a donc de période 3 , 6 , 8 , 12 ou 24

**51877**: mod 5 et n Lorsque le polynôme caractéristique de A ne est plus scindé , il est irréductible ( car de degré 2 ) , en posant C(A ) , et en travaillant dans le corps K Z5 Z qui est de cardinal 25 , on a ( même démonstration que Fermat ) on ***a*** donc de période 3 , 6 , 8 , 12 ou 24

**51896**: En enlevant celle qu' on ***connaît*** ( 1 , 2 et 4 correspondant aux matrices précédemment vues ) Dans Z5 Z les périodes de la suite ( un ) nN sont , à partir d' un certain rang dans Finalement les périodes possibles sont , d' après le théorème chinois ( on fait le PPCM des périodes dans Z2 Z et dans Z5 Z ) Juste pour le plaisir , construisons une situation où la période est 60

**51921**: En enlevant celle qu' on connaît ( 1 , 2 et 4 correspondant aux matrices précédemment vues ) Dans Z5 Z les périodes de la suite ( un ) nN ***sont*** , à partir d' un certain rang dans Finalement les périodes possibles sont , d' après le théorème chinois ( on fait le PPCM des périodes dans Z2 Z et dans Z5 Z ) Juste pour le plaisir , construisons une situation où la période est 60

**51934**: En enlevant celle qu' on connaît ( 1 , 2 et 4 correspondant aux matrices précédemment vues ) Dans Z5 Z les périodes de la suite ( un ) nN sont , à partir d' un certain rang dans Finalement les périodes possibles ***sont*** , d' après le théorème chinois ( on fait le PPCM des périodes dans Z2 Z et dans Z5 Z ) Juste pour le plaisir , construisons une situation où la période est 60

**51943**: En enlevant celle qu' on connaît ( 1 , 2 et 4 correspondant aux matrices précédemment vues ) Dans Z5 Z les périodes de la suite ( un ) nN sont , à partir d' un certain rang dans Finalement les périodes possibles sont , d' après le théorème chinois ( on ***fait*** le PPCM des périodes dans Z2 Z et dans Z5 Z ) Juste pour le plaisir , construisons une situation où la période est 60

**51961**: En enlevant celle qu' on connaît ( 1 , 2 et 4 correspondant aux matrices précédemment vues ) Dans Z5 Z les périodes de la suite ( un ) nN sont , à partir d' un certain rang dans Finalement les périodes possibles sont , d' après le théorème chinois ( on fait le PPCM des périodes dans Z2 Z et dans Z5 Z ) Juste pour le plaisir , ***construisons*** une situation où la période est 60

**51967**: En enlevant celle qu' on connaît ( 1 , 2 et 4 correspondant aux matrices précédemment vues ) Dans Z5 Z les périodes de la suite ( un ) nN sont , à partir d' un certain rang dans Finalement les périodes possibles sont , d' après le théorème chinois ( on fait le PPCM des périodes dans Z2 Z et dans Z5 Z ) Juste pour le plaisir , construisons une situation où la période ***est*** 60

**51976**: Le polynôme caractéristique dans Z Z ***doit*** être X 2 X 1 ( pour obtenir le 3 ) , et être de la forme ( X ) 2 dans Z5 Z , ( matrice étant non-diagonalisable ( pour obtenir le 20 )

**52013**: ***Prenons*** , par exemple 2

**52020**: On ***trouve*** donc donc , par exemple , a 9 et b 1

**52027**: On trouve donc donc , par exemple , ***a*** 9 et b 1

**52038**: La vérification par le calcul ***donne*** , pour les 63 premiers termes ( avec u0 u1 1 ) Application linéaire , 46 Automorphisme , 46 adaptée à une somme directe , 43 ante - duale , 70 d' un espace vectoriel , 35 Codimension , 72 Coefficients d' une matrice , 90 Cofacteur d' une matrice carrée , 170 Comatrice , 170 Combinaison linéaire , 21 Commutant d' un endomorphisme , 248 Condition initiale ( d' un système linéaire ) , 207 Décomposition LU , 180 Dimension d' un espace vectoriel , 39 Drapeau stable , 199 Droite vectorielle , 39 Dual d' un espace vectoriel , 68 Décomposition de Dunford , 247 Décomposition en somme directe , 32 Déterminant d' un endomorphisme , 162 d' une famille de vecteurs , 158 d' une matrice carrée , 160 de Vandermonde , 173 Éléments propres , 184 Endomorphisme , 46 diagonalisable , 196 trigonalisable , 199 Équation d' un hyperplan , 71 Espace caractéristique , 245 Espace propre d' un endomorphisme , 183 d' une matrice , 183 Espace vectoriel , 19 de dimension finie , 38 Exponentielle d' endomorphisme , 240 Famille duale , 69 Fonctions spline , 85 Forme p - linéaire , 154 anti-symétrique , 155 symétrique , 155 Forme de Smith , 273 Forme linéaire , 46 , 68 Groupe symétrique , 151 Hyperplan d' un espace vectoriel , 71 Idéal annulateur , 230 d' une application linéaire , 49 d' une matrice , 114 Interpolation de Lagrange , 80 Isomorphisme , 46 Matrice , 89 anti-symétrique , 106 augmentée , 140 circulante , 198 compagnon , 232 d' un vecteur , 90 d' une application linéaire , 90 d' une famille de vecteurs , 109 de dilatation , 121 de passage , 110 de permutation , 121 de transvection , 121 diagonale , 106 diagonalisable , 196 identité , 100 inversible , 108 symétrique , 106 triangulaire , 106 trigonalisable , 200 semblables , 119 équivalentes , 119 Mineur d' une matrice carrée , 170 Multiplicité d' une valeur propre , 195 d' une application linéaire , 49 d' une matrice , 114 Orthogonal ( direct ) d' une partie , 77 ( indirect ) d' une partie , 77 Partie ou famille génératrice , 25 Partition d' un ensemble , 116 Permutation , 151 Plan vectoriel , 39 caractéristique , 193 d' endomorphisme , 227 de matrice , 227 minimal , 231 d' espaces vectoriels , 22 de deux matrices , 98 de Kronecker , 146 Projecteur , 53 Projection , 52 Trace d' une matrice , 107 Transposition , 152 Transposée d' une matrice , 103 Valeur propre d' un endomorphisme , 183 d' une matrice , 183 Vecteur propre d' un endomorphisme , 183 d' une matrice , 183 Vecteurs , 20 indépendants , 34 linéairement dépendants , 34 d' une application linéaire , 55 d' une matrice , 114 Rayon spectral , 239 d' équivalence , 116 réflexive , 116 symétrique , 116 transitive , 116 Scalaires , 20 Second membre ( d' un système linéaire ) , 207 Signature d' une permutation , 152 de sous-espaces vectoriels , 28 d' une famille d' espaces vectoriels , 31 de deux sous-espaces vectoriels , 30 Sous-espace vectoriel , 23 engendré par une partie , 25 stable , 52 d' un endomorphisme , 183 d' une matrice , 183 Supplémentaire d' un sous-espace vectoriel , 32 Support d' une famille , 26 Symbole de Kronecker , 58 Symétrie , 54 associé , 207 Système linéaire , 78 de Cramer , 140 différentiel , 215 homogène , 78 associé , 78 récurrent , 207 Caractérisation des applications linéaires injectives , 67 surjectives , 67 des automorphismes en dimension finie , 57 des classes de similitude , 270 des endomorphismes diagonalisables , 197 , 229 trigonalisables , 200 , 234 des hyperplans , 72 des matrices équivalentes , 120 Changement de base pour les applications linéaires , 111 pour les vecteurs , 111 Correspondance matrices - blocs et décomposition en somme directe , 144 de co-diagonalisation , 205 de co-trigonalisation , 205 Développement d' un déterminant , 171 Forme de Jordan d' un endomorphisme nilpotent , 249 trigonalisable , 249 Formule de Grassman , 44 Invariants de similitude , 270 Lemme des noyaux , 228 Mise en équation des sous-espaces de codimensions finies , 75 Pivot de Gauss , 127 Rang d' une composée , 56 d' extension linéaire , 66 de Cayley - Hamilton , 233 de factorisation des applications linéaires , 62 de Frobenius , 269 de l' échange , 38 de la base incomplète , 40 des faisceaux d' hyperplans , 73 du rang , 57 du relèvement linéaire , 65 Commandes Wxmaxima binomial , 208 charpoly , 234 columnswap , 279 eigenvalues , 184 , 190 identfor , 175 cspline , 86 length , 136 load(solverec ) , 211 polytocompanion , 270 , 271 randompermutation , 137 setify , 26 solverec , 211 kroneckerproduct , 147 Commandes Python matplotlib , 26 binomial , 208 Dictionnaire , 113 .factorlist , 235 BlockDiagMatrix , 272 BlockMatrix , 148 expand , 186 Function , 192 initsession , 26 interpolate , 82 , 85 interpolatingspline , 86 .charpoly , 235 .cofactormatrix , 176 .extract , 170 .allcoeffs , 271 rsolve , 213 integer , 192 Figure 5.1 Similitude Trajectoires du système :